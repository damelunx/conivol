<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Bayesian estimates for conic intrinsic volumes • conivol</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/yeti/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="Bayesian estimates for conic intrinsic volumes">
<meta property="og:description" content="">
<meta property="og:image" content="http://damelunx.github.io/conivol/logo.png">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">conivol</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li>
  <a href="../reference/conivol.html">Overview</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/conic-intrinsic-volumes.html">Conic intrinsic volumes and (bivariate) chi-bar-squared distribution</a>
    </li>
    <li>
      <a href="../articles/estim-conic-intrinsic-volumes-with-EM.html">Estimating conic intrinsic volumes from bivariate chi-bar-squared data</a>
    </li>
    <li>
      <a href="../articles/bayesian.html">Bayesian estimates for conic intrinsic volumes</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="../reference/index.html">
    <span class="fa fa-file-code-o"></span>
     
    functions
  </a>
</li>
<li>
  <a href="https://github.com/damelunx/conivol">
    <span class="fa fa-github fa-lg"></span>
     
    github
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Bayesian estimates for conic intrinsic volumes</h1>
                        <h4 class="author">Dennis Amelunxen</h4>
            
            <h4 class="date">2018-04-12</h4>
          </div>

    
    
<div class="contents">
<p>This note describes how to derive Bayesian estimates of the conic intrinsic volumes, given sample data either from the intrinsic volumes distribution or from the bivariate chi-bar-squared distribution. The simplest case of direct samples from the intrinsic volumes distribution, and without enforcing any properties for the intrinsic volumes, can be solved analytically; enforcing the log-concavity inequalities already prohibits an analytical solution. The case of reconstructing the intrinsic volumes based on bivariate chi-bar-squared data is even more challenging. In these more complicated cases the posterior distribution will be sampled through Monte-Carlo sampling. The functions in <code>conivol</code> mostly use the sampler <a href="http://mc-stan.org/">Stan</a> (<a href="https://en.wikipedia.org/wiki/Stan_(software)">wikipedia</a>), although <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> (<a href="https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler">wikipedia</a>) is also partially supported. See below for more details.</p>
<p>We assume familiarity with the</p>
<p><strong>Other vignettes:</strong></p>
<ul>
<li>
<a href="conic-intrinsic-volumes.html">Conic intrinsic volumes and (bivariate) chi-bar-squared distribution</a>: introduces conic intrinsic volumes and (bivariate) chi-bar-squared distributions, as well as the computations involving polyhedral cones,</li>
<li>
<a href="estim-conic-intrinsic-volumes-with-EM.html">Estimating conic intrinsic volumes from bivariate chi-bar-squared data</a>: describes the details of the algorithm for finding the intrinsic volumes of closed convex cones from samples of the associated bivariate chi-bar-squared distribution.</li>
</ul>
<div id="setup" class="section level2">
<h2 class="hasAnchor">
<a href="#setup" class="anchor"></a>Setup and notation</h2>
<p>As in the <a href="estim-conic-intrinsic-volumes-with-EM.html#setup">previous vignette</a>, we use the following notation:</p>
<ul>
<li>
<span class="math inline">\(C\subseteq\text{R}^d\)</span> denotes a closed convex cone, <span class="math inline">\(C^\circ=\{y\in\text{R}^d\mid \forall x\in C: x^Ty\leq 0\}\)</span> the polar cone, and <span class="math inline">\(\Pi_C\colon\text{R}^d\to C\)</span> denotes the orthogonal projection map, <span class="math display">\[ \Pi_C(z) = \text{argmin}\{\|x-z\|\mid x\in C\} . \]</span> We will assume in the following that <span class="math inline">\(C\)</span> (and thus <span class="math inline">\(C^\circ\)</span>) is not a linear subspace so that the intrinsic volumes with even and with odd indices each add up to <span class="math inline">\(\frac12\)</span>.</li>
<li>
<span class="math inline">\(v = v(C) = (v_0(C),\ldots,v_d(C))\)</span> denotes the vector of intrinsic volumes.</li>
<li>We work with the two main random variables <span class="math display">\[ X=\|\Pi_C(g)\|^2 ,\quad Y=\|\Pi_{C^\circ}(g)\|^2, \]</span> where <span class="math inline">\(g\sim N(0,I_d)\)</span>. So <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are chi-bar-squared distributed with reference cones <span class="math inline">\(C\)</span> and <span class="math inline">\(C^\circ\)</span>, respectively, and the pair <span class="math inline">\((X,Y)\)</span> is distributed according to the bivariate chi-bar-squared distribution with reference cone <span class="math inline">\(C\)</span>.</li>
<li>We also define the “latent” variable <span class="math inline">\(Z\in\{0,1,\ldots,d\}\)</span>, <span class="math inline">\(\text{Prob}\{Z=k\}=v_k\)</span>. In the case of a polyhedral cone we may indeed have direct samples from the variable <span class="math inline">\(Z\)</span>, and also in the case of bivariate chi-bar-squared data, the variable <span class="math inline">\(Z\)</span> is <a href="estim-conic-intrinsic-volumes-with-EM.html#mod_bivchibarsq">not entirely latent</a>; we will explain this subtlety again <a href="#biv-chibsq">below</a>.</li>
</ul>
<p><strong>Example computations:</strong></p>
<p>We study in this vignette the intrinsic volumes of the following randomly defined polyhedral cone, <span class="math inline">\(C=\{Ax\mid x\geq0\}\)</span> with <span class="math inline">\(A\in\text{R}^d\)</span> given by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="dv">17</span>
num_gen &lt;-<span class="st"> </span><span class="dv">50</span>
<span class="kw">set.seed</span>(<span class="dv">1324</span>)
A &lt;-<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,num_gen), <span class="kw">rnorm</span>((d<span class="dv">-1</span>)*num_gen)), d, num_gen, <span class="dt">byrow =</span> <span class="ot">TRUE</span> )
out &lt;-<span class="st"> </span><span class="kw"><a href="../reference/polyh_reduce.html">polyh_reduce_gen</a></span>(A)
<span class="kw">str</span>(out)
<span class="co">#&gt; List of 5</span>
<span class="co">#&gt;  $ dimC     : int 17</span>
<span class="co">#&gt;  $ linC     : int 0</span>
<span class="co">#&gt;  $ QL       : logi NA</span>
<span class="co">#&gt;  $ QC       : num [1:17, 1:17] 1 0 0 0 0 0 0 0 0 0 ...</span>
<span class="co">#&gt;  $ A_reduced: num [1:17, 1:50] 1 -1.451 -1.105 -0.221 0.758 ...</span>
dimC &lt;-<span class="st"> </span>out$dimC
linC &lt;-<span class="st"> </span>out$linC
A_red &lt;-<span class="st"> </span>out$A_reduced</code></pre></div>
<p>In other words, the generators of the cone are iid Gaussian vectors chosen in an affine hyperplane of height one. We choose two batches of samples both from the intrinsic volumes distribution and from the bivariate chi-bar-squared distribution to illustrate the different approaches for deriving posterior distributions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samp_iv_sm &lt;-<span class="st"> </span><span class="kw"><a href="../reference/polyh_rivols.html">polyh_rivols_gen</a></span>(<span class="fl">1e2</span>, A_red, <span class="dt">reduce=</span><span class="ot">FALSE</span>)$multsamp
samp_iv_la &lt;-<span class="st"> </span><span class="kw"><a href="../reference/polyh_rivols.html">polyh_rivols_gen</a></span>(<span class="fl">2e3</span>, A_red, <span class="dt">reduce=</span><span class="ot">FALSE</span>)$multsamp
samp_bcb_sm &lt;-<span class="st"> </span><span class="kw"><a href="../reference/polyh_rbichibarsq.html">polyh_rbichibarsq_gen</a></span>(<span class="fl">1e2</span>, A_red, <span class="dt">reduce=</span><span class="ot">FALSE</span>)
samp_bcb_la &lt;-<span class="st"> </span><span class="kw"><a href="../reference/polyh_rbichibarsq.html">polyh_rbichibarsq_gen</a></span>(<span class="fl">2e3</span>, A_red, <span class="dt">reduce=</span><span class="ot">FALSE</span>)</code></pre></div>
<p>The prior distributions can take initial guesses for the intrinsic volumes into account. To illustrate this we will only use the initial estimate that is based on the statistical dimension and the normal approximation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v0_iv_sm &lt;-<span class="st"> </span><span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>( dimC, <span class="kw">sum</span>(<span class="dv">0</span>:dimC *<span class="st"> </span>samp_iv_sm/<span class="kw">sum</span>(samp_iv_sm)), <span class="dt">init_mode=</span><span class="dv">1</span> )
v0_iv_la &lt;-<span class="st"> </span><span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>( dimC, <span class="kw">sum</span>(<span class="dv">0</span>:dimC *<span class="st"> </span>samp_iv_la/<span class="kw">sum</span>(samp_iv_la)), <span class="dt">init_mode=</span><span class="dv">1</span> )
v0_bcb_sm &lt;-<span class="st"> </span><span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>( dimC, <span class="kw"><a href="../reference/estim_statdim_var.html">estim_statdim_var</a></span>(dimC,samp_bcb_sm)$delta, <span class="dt">init_mode=</span><span class="dv">1</span>)
v0_bcb_la &lt;-<span class="st"> </span><span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>( dimC, <span class="kw"><a href="../reference/estim_statdim_var.html">estim_statdim_var</a></span>(dimC,samp_bcb_la)$delta, <span class="dt">init_mode=</span><span class="dv">1</span>)</code></pre></div>
<p>The different initial starting points (their closeness is explained by the fact that the starting point depends on a single parameter (the statistical dimension), which is easily estimated): <img src="bayesian_figures/start-V-disp-1.png" width="672" style="display: block; margin: auto;"></p>
</div>
<div id="prior_dist" class="section level2">
<h2 class="hasAnchor">
<a href="#prior_dist" class="anchor"></a>Prior distribution</h2>
<p>In a Bayesian approach we do not consider the intrinsic volumes as fixed parameters but rather as random themselves. Hence, we introduce the random variable <span class="math inline">\(V=(V_0,\ldots,V_d)\)</span> that takes values in the probability simplex <span class="math display">\[ \Delta^d = \big\{x\in\text{R}^{d+1}\mid 0\leq x_k\leq 1 \text{ for all }k,\; x_0+\dots+x_d=1\big\} . \]</span></p>
<p>For taking the parity equations <span class="math display">\[ V_0+V_2+V_4+\dots=V_1+V_3+V_5+\dots=\tfrac{1}{2} \]</span> into account, we further introduce the two random variables <span class="math inline">\(V^e=2(V_0,V_2,V_4,\ldots)^T\)</span> and <span class="math inline">\(V^o=2(V_1,V_3,V_5,\ldots)^T\)</span>. A random model for the intrinsic volumes that respects the parity equation thus can be modeled through <span class="math inline">\(V^e\)</span> and <span class="math inline">\(V^o\)</span> taking values in the corresponding probability simplices: <span class="math inline">\(V^e\in \Delta^{d^e}\)</span> and <span class="math inline">\(V^o\in \Delta^{d^o}\)</span> with <span class="math inline">\(d^e=\lfloor \frac{d}{2}\rfloor\)</span> and <span class="math inline">\(d^o=\lceil \frac{d}{2}\rceil-1\)</span>.</p>
<p>For taking the <a href="conic-intrinsic-volumes.html#log-conc">log-concavity inequalities</a> into account we further introduce the variables <span class="math display">\[ U=(U_0,\ldots,U_d) ,\qquad U_k=-\log(V_k) , \]</span> as well as the transformed vector <span class="math display">\[ S=(S_0,\ldots,S_d)=TU ,\qquad S_k=\begin{cases}
    U_k-2U_{k+1}+U_{k+2} &amp; \text{if } 0\leq k\leq d-2
    \\[1mm] U_0+U_2+U_4+\dots &amp; \text{if } k=d-1
    \\[1mm] U_1+U_3+U_5+\dots &amp; \text{if } k=d \text{ and $d$ odd}
    \\[1mm] U_0+U_1+U_3+U_5+\dots &amp; \text{if } k=d \text{ and $d$ even} ,
    \end{cases} \]</span> with <span class="math inline">\(T\in\text{R}^{(d+1)\times(d+1)}\)</span>. The log-concavity inequalities <span class="math inline">\(V_k^2\geq V_{k-1}V_{k+1}\)</span>, <span class="math inline">\(k=1,\ldots,d-1\)</span>, are thus equivalent to the inequalities <span class="math inline">\(S_k\geq0\)</span>, <span class="math inline">\(k=0,\ldots,d-2\)</span>.</p>
<p>Note that it would be possible to reconstruct <span class="math inline">\((V_0,\ldots,V_d)\)</span> entirely from <span class="math inline">\((S_0,\ldots,S_{d-2})\)</span> by using the parity equations, which translate into <span class="math display">\[ \exp(-U_0)+\exp(-U_2)+\exp(-U_4)+\dots
    =\exp(-U_1)+\exp(-U_3)+\exp(-U_5)+\dots=\tfrac{1}{2} . \]</span> But following this approach eventually leads to the problem of sampling the posterior distribution on a nonlinear manifold, which is at the moment not supported in <a href="http://mc-stan.org/">Stan</a>. The additional components <span class="math inline">\(S_{d-1}\)</span> and <span class="math inline">\(S_d\)</span> are used for pragmatic reasons, and should ideally be avoided. We reconstruct the values of <span class="math inline">\(V\)</span> through the equation <span class="math display">\[ V = \frac{\text{exp}(-T^{-1}S)}{\|\text{exp}(-T^{-1}S)\|} . \]</span> Log-concavity will be enforced by assuming a prior distribution for <span class="math inline">\(S\)</span>, whose support lies in the positive orthant. One might additionally think of discarding those samples for <span class="math inline">\(V\)</span> from the posterior distribution where the parity equation is too much violated, or one might project these points on the corresponding linear subspace. But this step is not supported in <code>conivol</code> as it is unclear which approach will yield the best results.</p>
<div id="no_enf_par_logc" class="section level3">
<h3 class="hasAnchor">
<a href="#no_enf_par_logc" class="anchor"></a>No enforced parity or log-concavity</h3>
The natural choice for the prior distribution of <span class="math inline">\(V\)</span> as a random element in the probability simplex <span class="math inline">\(\Delta^d\)</span> is the Dirichlet distribution, <span class="math inline">\(V\sim \text{Dirichlet}(\alpha)\)</span> with <span class="math inline">\(\alpha=(\alpha_0,\ldots,\alpha_d)\)</span>, <span class="math inline">\(\alpha_k&gt;0\)</span> for all <span class="math inline">\(k\)</span>; that is,
<span class="math display">\[\begin{align*}
   p(V=v) &amp; \propto v_0^{a_0-1}\dots v_d^{a_d-1}
\end{align*}\]</span>
for <span class="math inline">\(v\in\Delta^d\)</span>, and zero else. The expectation of <span class="math inline">\(V\)</span> and the marginal variances are given by
<span class="math display">\[\begin{align*}
    \text{E}[V] &amp; = \frac{(\alpha_0,\ldots,\alpha_d)}{\sum_j\alpha_j} ,
    &amp; \text{var}[V_k] &amp; = \frac{1}{1+\sum_j\alpha_j}
    \frac{\alpha_k}{\sum_j\alpha_j}
    \Big( 1-\frac{\alpha_k}{\sum_j\alpha_j} \Big) .
\end{align*}\]</span>
<p>The sum of the parameters, <span class="math inline">\(\sum_{j=0}^d\alpha_j\)</span>, is the <em>prior sample size</em>, cf. <span class="citation">(Gelman et al. <a href="#ref-GCSDVR14">2014</a>)</span>, and its effect is clearly seen in the above formulas: scaling the parameters <span class="math inline">\(\alpha_j\)</span> by a constant <span class="math inline">\(c\)</span> keeps the expected values fixed, but changes the variance by a factor of <span class="math inline">\((1+\sum_j\alpha_j)/(1+c\sum_j\alpha_j)\)</span>.</p>
<p>We thus arrive at a natural choice for the parameters of the prior distribution, using the starting point of the EM algorithm <span class="math inline">\(v^{(0)}\)</span> (see the <a href="estim-conic-intrinsic-volumes-with-EM.html#start_EM">previous vignette</a>): <span class="math display">\[ \alpha_k = c\,v^{(0)}_k \;,\quad \text{for $k=0,\ldots,d$} . \]</span> The expectation is given by <span class="math inline">\(\text{E}[V]=v^{(0)}\)</span>, and the prior sample size is given by <span class="math inline">\(\sum_j c v^{(0)}_j=c\)</span>. Choosing a prior sample size <span class="math inline">\(c=1\)</span> will yield a noninformative prior; setting this parameter to a higher value will make the prior more informative.</p>
</div>
<div id="enf_par" class="section level3">
<h3 class="hasAnchor">
<a href="#enf_par" class="anchor"></a>Enforced parity equation</h3>
<p>The parity equations are enforced simply by using the two random probability vectors <span class="math inline">\(V^e\)</span> and <span class="math inline">\(V^o\)</span>. We use the same reasoning as above to find priors for these variables: <span class="math display">\[ V^e\sim \text{Dirichlet}(\alpha^e) ,\quad V^o\sim \text{Dirichlet}(\alpha^o) , \]</span> with the constants <span class="math inline">\(\alpha^e=(\alpha^e_0,\alpha^e_2,\alpha^e_4,\ldots)\)</span> and <span class="math inline">\(\alpha^o=(\alpha^o_1,\alpha^o_3,\alpha^o_5,\ldots)\)</span> chosen with respect to a sample size <span class="math inline">\(c\)</span>: <span class="math display">\[ \alpha^e_j = 2v^{(0)}_j \;,\quad \alpha^o_k = 2v^{(0)}_k \;,\quad
    \text{for $j$ even and $k$ odd} . \]</span></p>
<p><strong>Example computations:</strong></p>
<p>We illustrate the prior distributions (noninformative and informative) using the initial estimate for the small sample from the intrinsic volumes distribution by sampling ten elements. (Note that the prior can be made “completely noninformative” by choosing the uniform distribution for the initial estimate <span class="math inline">\(v^{(0)}\)</span>, which is the default in the function <code>polyh_bayes</code>.)</p>
<p><strong>Noninformative (prior sample size <span class="math inline">\(=1\)</span>):</strong> <img src="bayesian_figures/prior-iv-noninf-1.png" width="672" style="display: block; margin: auto;"></p>
<p><strong>Informative (prior sample size <span class="math inline">\(=d\)</span>):</strong> <img src="bayesian_figures/prior-iv-inf-1.png" width="672" style="display: block; margin: auto;"></p>
</div>
<div id="enf_logc" class="section level3">
<h3 class="hasAnchor">
<a href="#enf_logc" class="anchor"></a>Enforced log-concavity</h3>
The log-concavity inequalities are enforced by using the transformed vector <span class="math inline">\(S=T\log(V)\)</span> and by choosing a prior distribution whose support lies in the positive orthant. We use the gamma distribution, <span class="math display">\[ S_k \sim \text{Gamma}(\alpha_k,\theta_k) ,\quad k=0,\ldots,d, \]</span> where <span class="math inline">\(\alpha_k&gt;0\)</span> and <span class="math inline">\(\theta_k&gt;0\)</span> denote the shape and scale, that is, <span class="math display">\[ p(S_k=s) \propto \frac{s^{\alpha_k-1}}{\exp(s/\theta_k)} . \]</span> Expectation and variance are given by
<span class="math display">\[\begin{align*}
    \text{E}[S_k] &amp; = \alpha_k\theta_k ,
    &amp; \text{var}(S_k) &amp; = \alpha_k\theta_k^2 .
\end{align*}\]</span>
<p>We choose these constants again through the initial guess for the intrinsic volumes <span class="math inline">\(v^{(0)}\)</span>, which yields an initial guess for the transformed variable, <span class="math inline">\(s^{(0)}=-T\log(v^{(0)})\)</span>, which in turn can be used to find the shape and inverse scale of the gamma distribution:</p>
<ol style="list-style-type: decimal">
<li>
<em>noninformative:</em> <span class="math inline">\(\alpha_k = 2\)</span>, <span class="math inline">\(\theta_k = s^{(0)}_k/2\)</span>,</li>
<li>
<em>informative:</em> <span class="math inline">\(\alpha_k = s^{(0)}_k\)</span>, <span class="math inline">\(\theta_k = 1\)</span>,</li>
</ol>
<p>for <span class="math inline">\(k=0,\ldots,d\)</span>.</p>
<p><strong>Example computations:</strong></p>
<p>We use the same initial estimate for <span class="math inline">\(v^{(0)}\)</span> as above, transform this to an initial estimate <span class="math inline">\(s^{(0)}=T\log(v^{(0)})\)</span>, set the prior for <span class="math inline">\(S\)</span> and sample ten elements from this distribution, and transform those back to prior samples for <span class="math inline">\(V\)</span>.</p>
<p><strong>Noninformative:</strong> <img src="bayesian_figures/prior-iv-noninf-logc-1.png" width="672" style="display: block; margin: auto;"></p>
<p><strong>Informative:</strong> <img src="bayesian_figures/prior-iv-inf-logc-1.png" width="672" style="display: block; margin: auto;"></p>
</div>
</div>
<div id="post_ivol" class="section level2">
<h2 class="hasAnchor">
<a href="#post_ivol" class="anchor"></a>Posterior for intrinsic volumes data</h2>
<p>If the underlying cone is polyhedral, then we can obtain direct samples from the intrinsic volumes distribution (via <code>polyh_rivols_gen</code> and <code>polyh_rivols_ineq</code>). This case is the best case scenario, where estimates for the intrinsic volumes are immediate and errors can be bound easily. We describe this case to prepare for the more difficult case of reconstructing the intrinsic volumes from bivariate chi-bar-squared data, and for illustrating the use of the functions in <code>conivol</code>.</p>
<p>Let <span class="math inline">\(z_1,\ldots,z_n\in\{0,1,\ldots,d\}\)</span> be independent samples of the latent variable <span class="math inline">\(Z\)</span>. Identifying the data with the counting weight, <span class="math inline">\(w=(w_0,\ldots,w_d)\)</span>, <span class="math inline">\(w_k=|\{i\mid z_i=k\}|\)</span>, we see that <span class="math inline">\(w\)</span> is a sample of the random variable <span class="math display">\[ W\sim\text{Multinom}(n;v_0,\ldots,v_d) . \]</span> The corresponding graphical model thus has a very simple form:<br><img src="bayes_diagrams/bayes_direct.png" width="121" style="display: block; margin: auto;"></p>
<p>The Dirichlet distribution is a conjugate prior for the multinomial distribution, and the posterior distribution is obtained by adding the weight vector to the parameter vector: <span class="math display">\[ \text{pre($V$): } \text{Dirichlet}(\alpha) \quad\Rightarrow\quad
    \text{post($V$): } \text{Dirichlet}(\alpha+w) . \]</span> In the following we describe the posterior distribution for the case of enforced parity equation and for the case of enforced log-concavity (including some sample computations). In the latter case the posterior distribution can not be derived analytically; instead we will use the MCMC samplers <a href="http://mc-stan.org/">Stan</a> and <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> to sample from the posterior distribution.</p>
<div id="post_enf_par" class="section level3">
<h3 class="hasAnchor">
<a href="#post_enf_par" class="anchor"></a>Enforced parity equation</h3>
<p>As described above, the only thing we need to do to enforce the parity equation is to decompose the vector into even and odd parts. The corresponding graphical model thus still has a very simple form:<br><img src="bayes_diagrams/bayes_direct_par.png" width="297" style="display: block; margin: auto;"></p>
Decomposing the sample data accordingly,
<span class="math display">\[\begin{align*}
    w^e &amp; = (w_0,w_2,w_4,\ldots) ,
    &amp; w^o &amp; = (w_1,w_3,w_5,\ldots) ,
\end{align*}\]</span>
<p>we obtain the posterior distributions for <span class="math inline">\(V^e\)</span> and <span class="math inline">\(V^o\)</span> as <span class="math display">\[ \text{post($V^e$): } \text{Dirichlet}(\alpha^e+w^e) \;,\quad
    \text{post($V^o$): } \text{Dirichlet}(\alpha^o+w^o) . \]</span> The function <code>polyh_bayes</code> computes the weights and generates functions for sampling and for computing marginal quantiles of the posterior distribution.</p>
<p><strong>Example computations:</strong></p>
<p>We sample again ten elements each from the posterior distributions using the small and large samples, and using a noninformative and an informative prior, as described above.</p>
<p><strong>Small sample, noninformative prior:</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">post_iv &lt;-<span class="st"> </span><span class="kw"><a href="../reference/polyh_bayes.html">polyh_bayes</a></span>(samp_iv_sm,dimC,linC,<span class="dt">v_prior=</span>v0_iv_sm,<span class="dt">prior_sample_size=</span><span class="dv">1</span>)
<span class="kw">str</span>(post_iv)</code></pre></div>
<p><img src="bayesian_figures/dir-enf-par-sm-noninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r dir-enf-par-sm-noninf, fig.width=7, fig.align="center", echo=FALSE} --><!-- tib_plot <- t(post_iv$post_samp(N)) %>% --><!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% --><!--     gather(key,value,2:(N+1)) --><!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + --><!--     geom_line() + theme_bw() + --><!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) --><!-- ``` --></p>
<p><strong>Small sample, informative prior:</strong> <img src="bayesian_figures/dir-enf-par-sm-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r dir-enf-par-sm-inf, fig.width=7, fig.align="center", echo=FALSE} --><!-- post_iv <- polyh_bayes(samp_iv_sm,dimC,linC,v_prior=v0_iv_sm,prior_sample_size=d) --><!-- tib_plot <- t(post_iv$post_samp(N)) %>% --><!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% --><!--     gather(key,value,2:(N+1)) --><!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + --><!--     geom_line() + theme_bw() + --><!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) --><!-- ``` --></p>
<p><strong>Large sample, noninformative prior:</strong> <img src="bayesian_figures/dir-enf-par-la-noninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r dir-enf-par-la-noninf, fig.width=7, fig.align="center", echo=FALSE} --><!-- post_iv <- polyh_bayes(samp_iv_la,dimC,linC,v_prior=v0_iv_la) --><!-- tib_plot <- t(post_iv$post_samp(N)) %>% --><!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% --><!--     gather(key,value,2:(N+1)) --><!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + --><!--     geom_line() + theme_bw() + --><!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) --><!-- ``` --></p>
<p><strong>Large sample, informative prior:</strong> <img src="bayesian_figures/dir-enf-par-la-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r dir-enf-par-la-inf, fig.width=7, fig.align="center", echo=FALSE} --><!-- post_iv <- polyh_bayes(samp_iv_la,dimC,linC,v_prior=v0_iv_la,prior_sample_size=d) --><!-- tib_plot <- t(post_iv$post_samp(N)) %>% --><!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% --><!--     gather(key,value,2:(N+1)) --><!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + --><!--     geom_line() + theme_bw() + --><!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) --><!-- ``` --></p>
</div>
<div id="post_enf_logc" class="section level3">
<h3 class="hasAnchor">
<a href="#post_enf_logc" class="anchor"></a>Enforced log-concavity</h3>
<p>In order to enforce log-concavity we use the prior on the transformed parameters <span class="math inline">\(S_k=-T\log(V_k)\)</span>, as described above. We arrive at the following graphical model:<br><img src="bayes_diagrams/bayes_direct_logconc.png" width="137" style="display: block; margin: auto;"></p>
<p>The posterior distribution for <span class="math inline">\(V\)</span> can not be solved analytically. We can sample from the posterior distribution using a Markov chain Monte Carlo approach. The function <code>polyh_stan</code> creates the input for the sampler <a href="http://mc-stan.org/">Stan</a>, which can be used in R via the <code>rstan</code> package.</p>
<p><strong>Example computations:</strong></p>
<p>We use the function <code>polyh_stan</code> to create the input for Stan, which we pass through an input file using the <code>rstan</code> package. We sample one thousand elements from the posterior distribution (after a burn-in phase) and then display every one-hundredth element so that in the end we see again ten elements from the posterior distribution. We do this for the small and large sample and using a noninformative and an informative prior distribution.</p>
<p><strong>Small sample, noninformative prior:</strong><br>
Define Stan model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">filename &lt;-<span class="st"> "ex_stan_model_iv.stan"</span>
staninp &lt;-<span class="st"> </span><span class="kw"><a href="../reference/polyh_stan.html">polyh_stan</a></span>(samp_iv_sm, dimC, linC,
                      <span class="dt">v_prior=</span>v0_iv_sm, <span class="dt">prior=</span><span class="st">"noninformative"</span>, <span class="dt">filename=</span>filename)</code></pre></div>
<p>Run Stan model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stanfit &lt;-<span class="st"> </span><span class="kw">stan</span>( <span class="dt">file =</span> filename, <span class="dt">data =</span> staninp$data, <span class="dt">chains =</span> <span class="dv">1</span>,
                 <span class="dt">warmup =</span> <span class="dv">1000</span>, <span class="dt">iter =</span> <span class="dv">2000</span>, <span class="dt">cores =</span> <span class="dv">2</span>, <span class="dt">refresh =</span> <span class="dv">1000</span> )
post_iv_logc &lt;-<span class="st"> </span>rstan::<span class="kw">extract</span>(stanfit)$V[<span class="dv">100</span>*(<span class="dv">1</span>:<span class="dv">10</span>),]</code></pre></div>
<p><img src="bayesian_figures/dir-enf-logc-sm-noninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r dir-enf-logc-sm-noninf, fig.width=7, fig.align="center", echo=FALSE} --><!-- tib_plot <- t(post_iv_logc) %>% --><!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% --><!--     gather(key,value,2:(N+1)) --><!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + --><!--     geom_line() + theme_bw() + --><!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) --><!-- ``` --></p>
<p><strong>Small sample, informative prior:</strong> <img src="bayesian_figures/dir-enf-logc-sm-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r dir-enf-logc-sm-inf, fig.width=7, fig.align="center", echo=FALSE, warning=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- polyh_stan(samp_iv_sm, dimC, linC, --><!--                       v_prior=v0_iv_sm, prior="informative") --><!-- # run Stan model --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --></p>
<!-- post_iv_logc <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_iv_logc) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>Large sample, noninformative prior:</strong> <img src="bayesian_figures/dir-enf-logc-la-noninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r dir-enf-logc-la-noninf, fig.width=7, fig.align="center", echo=FALSE, warning=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- polyh_stan(samp_iv_la, dimC, linC, --><!--                       v_prior=v0_iv_la, prior="noninformative") --><!-- # run Stan model --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --></p>
<!-- post_iv_logc <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_iv_logc) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>Large sample, informative prior:</strong> <img src="bayesian_figures/dir-enf-logc-la-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r dir-enf-logc-la-inf, fig.width=7, fig.align="center", echo=FALSE, warning=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- polyh_stan(samp_iv_la, dimC, linC, --><!--                       v_prior=v0_iv_la, prior="informative") --><!-- # run Stan model --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --><!-- # remove Stan input file --><!-- file.remove(filename) --></p>
<!-- post_iv_logc <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_iv_logc) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
</div>
</div>
<div id="biv-chibsq" class="section level2">
<h2 class="hasAnchor">
<a href="#biv-chibsq" class="anchor"></a>Posterior for bivariate chi-bar-squared data</h2>
<p>If the given data are samples from the bivariate chi-bar-squared distribution, we work with the variable <span class="math inline">\(Z\)</span> as a latent variable. Recall that this variable is not entirely latent, as we have the equivalences <span class="math display">\[ Z=d\iff g\in \text{int}(C) \stackrel{\text{a.s.}}{\iff} Y=0 ,\qquad
    Z=0\iff g\in \text{int}(C^\circ) \stackrel{\text{a.s.}}{\iff} X=0 . \]</span> We will thus only regard the data points <span class="math inline">\((X_i,Y_i)\)</span> such that both components are nonzero. These on the other hand follow independent chi-squared distributions with degrees of freedom given by the value of the latent variable. The graphical model looks as follows:<br><img src="bayes_diagrams/bayes_indirect.png" width="393" style="display: block; margin: auto;"></p>
<div id="bcb_enf_par" class="section level3">
<h3 class="hasAnchor">
<a href="#bcb_enf_par" class="anchor"></a>Enforced parity</h3>
<p>In order to enforce log-concavity we split up the intrinsic volumes variables according to even and odd indices, analogously to the case of sample data from the intrinsic volumes distribution. According to the parity of <span class="math inline">\(d\)</span> we arrive at the following graphical models.</p>
<p><span class="math inline">\(d\)</span> even:<br><img src="bayes_diagrams/bayes_indirect_par_even.png" width="474" style="display: block; margin: auto;"></p>
<p><span class="math inline">\(d\)</span> odd:<br><img src="bayes_diagrams/bayes_indirect_par_odd.png" width="403" style="display: block; margin: auto;"></p>
<p>Again, the posterior distribution for <span class="math inline">\(V\)</span> can not be solved analytically, but we can sample the posterior distribution through an MCMC approach. The functions <code>estim_jags</code> and <code>estim_stan</code> create inputs for the samplers <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> and <a href="http://mc-stan.org/">Stan</a>, respectively; these programs can be used in R via the packages <code>rjags</code> and <code>rstan</code>.</p>
<p><strong>Example computations:</strong></p>
<p>We redo the computations from the case of intrinsic volumes data, now using the bivariate chi-bar-squared data; and we do these computations twice, first using JAGS, then using Stan.</p>
<p><strong>JAGS | Small sample, noninformative prior:</strong><br><img src="bayesian_figures/indir-JAGS-enf-par-sm-ninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-JAGS-enf-par-sm-ninf, warning=FALSE, fig.width=7, fig.align="center"} --><!-- # obtain input data for JAGS model --><!-- in_jags <- estim_jags(samp_bcb_sm, d, v_prior=v0_bcb_sm) --></p>
<!-- # create JAGS model -->
<!-- model_connection <- textConnection(in_jags$model) -->
<!-- mod <- jags.model(model_connection , -->
<!--                   data = in_jags$data , -->
<!--                   n.chains = 1 , -->
<!--                   n.adapt = 1000) -->
<!-- close(model_connection) -->
<!-- # sample posterior distribution, take every thousandth sample -->
<!-- mod_sim <- coda.samples(model=mod, variable.names=in_jags$variable.names, n.iter=1e4) -->
<!-- mod_csim <- as.mcmc(do.call(rbind, mod_sim)) -->
<!-- post_bcb_pa <- mod_csim[1e3*(1:10),] -->
<!-- tib_plot <- t(post_bcb_pa) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>JAGS | Small sample, informative prior:</strong><br><img src="bayesian_figures/indir-JAGS-enf-par-sm-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-JAGS-enf-par-sm-inf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE, warning=FALSE, results=FALSE} --><!-- # obtain input data for JAGS model --><!-- in_jags <- estim_jags(samp_bcb_sm, d, v_prior=v0_bcb_sm, prior_sample_size=d) --></p>
<!-- # create JAGS model -->
<!-- model_connection <- textConnection(in_jags$model) -->
<!-- mod <- jags.model(model_connection , -->
<!--                   data = in_jags$data , -->
<!--                   n.chains = 1 , -->
<!--                   n.adapt = 1000) -->
<!-- close(model_connection) -->
<!-- # sample posterior distribution, take every thousandth sample -->
<!-- mod_sim <- coda.samples(model=mod, variable.names=in_jags$variable.names, n.iter=1e4) -->
<!-- mod_csim <- as.mcmc(do.call(rbind, mod_sim)) -->
<!-- post_bcb_pa <- mod_csim[1e3*(1:10),] -->
<!-- tib_plot <- t(post_bcb_pa) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>JAGS | Large sample, noninformative prior:</strong><br><img src="bayesian_figures/indir-JAGS-enf-par-la-ninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-JAGS-enf-par-la-ninf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE, warning=FALSE, results=FALSE} --><!-- # obtain input data for JAGS model --><!-- in_jags <- estim_jags(samp_bcb_la, d, v_prior=v0_bcb_la) --></p>
<!-- # create JAGS model -->
<!-- model_connection <- textConnection(in_jags$model) -->
<!-- mod <- jags.model(model_connection , -->
<!--                   data = in_jags$data , -->
<!--                   n.chains = 1 , -->
<!--                   n.adapt = 1000) -->
<!-- close(model_connection) -->
<!-- # sample posterior distribution, take every thousandth sample -->
<!-- mod_sim <- coda.samples(model=mod, variable.names=in_jags$variable.names, n.iter=1e4) -->
<!-- mod_csim <- as.mcmc(do.call(rbind, mod_sim)) -->
<!-- post_bcb_pa <- mod_csim[1000*(1:10),] -->
<!-- tib_plot <- t(post_bcb_pa) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>JAGS | Large sample, informative prior:</strong><br><img src="bayesian_figures/indir-JAGS-enf-par-la-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-JAGS-enf-par-la-inf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE, warning=FALSE, results=FALSE} --><!-- # obtain input data for JAGS model --><!-- in_jags <- estim_jags(samp_bcb_la, d, v_prior=v0_bcb_la, prior_sample_size=d) --></p>
<!-- # create JAGS model -->
<!-- model_connection <- textConnection(in_jags$model) -->
<!-- mod <- jags.model(model_connection , -->
<!--                   data = in_jags$data , -->
<!--                   n.chains = 1 , -->
<!--                   n.adapt = 1000) -->
<!-- close(model_connection) -->
<!-- # sample posterior distribution, take every thousandth sample -->
<!-- mod_sim <- coda.samples(model=mod, variable.names=in_jags$variable.names, n.iter=1e4) -->
<!-- mod_csim <- as.mcmc(do.call(rbind, mod_sim)) -->
<!-- post_bcb_pa <- mod_csim[1000*(1:10),] -->
<!-- tib_plot <- t(post_bcb_pa) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>Stan | Small sample, noninformative prior:</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">filename &lt;-<span class="st"> "ex_stan_model_bcb.stan"</span>
staninp &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_stan.html">estim_stan</a></span>(samp_bcb_sm, d, dimC, linC,
                      <span class="dt">v_prior=</span>v0_bcb_sm, <span class="dt">filename=</span>filename)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stanfit &lt;-<span class="st"> </span><span class="kw">stan</span>( <span class="dt">file =</span> filename, <span class="dt">data =</span> staninp$data, <span class="dt">chains =</span> <span class="dv">1</span>,
                 <span class="dt">warmup =</span> <span class="dv">1000</span>, <span class="dt">iter =</span> <span class="dv">2000</span>, <span class="dt">cores =</span> <span class="dv">2</span>, <span class="dt">refresh =</span> <span class="dv">1000</span> )
post_bcb_pa &lt;-<span class="st"> </span>rstan::<span class="kw">extract</span>(stanfit)$V[<span class="dv">100</span>*(<span class="dv">1</span>:<span class="dv">10</span>),]</code></pre></div>
<p><img src="bayesian_figures/indir-Stan-enf-par-sm-ninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-Stan-enf-par-sm-ninf, fig.width=7, fig.align="center", echo=FALSE} --><!-- tib_plot <- t(post_bcb_pa) %>% --><!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% --><!--     gather(key,value,2:(N+1)) --><!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + --><!--     geom_line() + theme_bw() + --><!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) --><!-- ``` --></p>
<p><strong>Stan | Small sample, informative prior:</strong><br><img src="bayesian_figures/indir-Stan-enf-par-sm-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-Stan-enf-par-sm-inf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- estim_stan(samp_bcb_sm, d, dimC, linC,prior_sample_size=d, --><!--                       v_prior=v0_bcb_sm) --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --></p>
<!-- post_bcb_pa <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_bcb_pa) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>Stan | Large sample, noninformative prior:</strong><br><img src="bayesian_figures/indir-Stan-enf-par-la-ninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-Stan-enf-par-la-ninf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- estim_stan(samp_bcb_la, d, dimC, linC, --><!--                       v_prior=v0_bcb_la) --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --></p>
<!-- post_bcb_pa <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_bcb_pa) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>Stan | Large sample, informative prior:</strong><br><img src="bayesian_figures/indir-Stan-enf-par-la-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-Stan-enf-par-la-inf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- estim_stan(samp_bcb_la, d, dimC, linC,prior_sample_size=d, --><!--                       v_prior=v0_bcb_la) --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --><!-- # remove Stan input file --><!-- file.remove(filename) --></p>
<!-- post_bcb_pa <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_bcb_pa) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
</div>
<div id="bcb_enf_logc" class="section level3">
<h3 class="hasAnchor">
<a href="#bcb_enf_logc" class="anchor"></a>Enforced log-concavity</h3>
<p>Finally, we enforce log-concavity again using the transformed parameters <span class="math inline">\(S_k=-T\log(V_k)\)</span>. The graphical models, according to the parity of <span class="math inline">\(d\)</span>, thus look as follows.</p>
<p><span class="math inline">\(d\)</span> even:<br><img src="bayes_diagrams/bayes_indirect_logconc_even.png" width="477" style="display: block; margin: auto;"></p>
<p><span class="math inline">\(d\)</span> odd:<br><img src="bayes_diagrams/bayes_indirect_logconc_odd.png" width="408" style="display: block; margin: auto;"></p>
<p>The function <code>estim_stan</code> supports this enforced log-concavity (set the parameter <code>enforce_logconc=TRUE</code>).</p>
<p><strong>Example computations:</strong></p>
<p>We redo the computations one more time, this time using the Stan model that enforces log-concavity.</p>
<p><strong>Small sample, noninformative prior:</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">filename &lt;-<span class="st"> "ex_stan_model_bcb_logc.stan"</span>
staninp &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_stan.html">estim_stan</a></span>(samp_bcb_sm, d, dimC, linC, <span class="dt">enforce_logconc=</span><span class="ot">TRUE</span>,
                      <span class="dt">v_prior=</span>v0_bcb_sm, <span class="dt">filename=</span>filename)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stanfit &lt;-<span class="st"> </span><span class="kw">stan</span>( <span class="dt">file =</span> filename, <span class="dt">data =</span> staninp$data, <span class="dt">chains =</span> <span class="dv">1</span>,
                 <span class="dt">warmup =</span> <span class="dv">1000</span>, <span class="dt">iter =</span> <span class="dv">2000</span>, <span class="dt">cores =</span> <span class="dv">2</span>, <span class="dt">refresh =</span> <span class="dv">1000</span> )
post_bcb_logc &lt;-<span class="st"> </span>rstan::<span class="kw">extract</span>(stanfit)$V[<span class="dv">100</span>*(<span class="dv">1</span>:<span class="dv">10</span>),]</code></pre></div>
<p><img src="bayesian_figures/indir-Stan-enf-logc-sm-ninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-Stan-enf-logc-sm-ninf, fig.width=7, fig.align="center", echo=FALSE} --><!-- tib_plot <- t(post_bcb_logc) %>% --><!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% --><!--     gather(key,value,2:(N+1)) --><!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + --><!--     geom_line() + theme_bw() + --><!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) --><!-- ``` --></p>
<p><strong>Small sample, informative prior:</strong><br><img src="bayesian_figures/indir-Stan-enf-logc-sm-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-Stan-enf-logc-sm-inf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- estim_stan(samp_bcb_sm, d, dimC, linC, enforce_logconc=TRUE, --><!--                       prior="informative", v_prior=v0_bcb_sm) --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --></p>
<!-- post_bcb_logc <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_bcb_logc) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>Large sample, noninformative prior:</strong><br><img src="bayesian_figures/indir-Stan-enf-logc-la-ninf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-Stan-enf-logc-la-ninf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- estim_stan(samp_bcb_la, d, dimC, linC, enforce_logconc=TRUE, --><!--                       v_prior=v0_bcb_la) --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --></p>
<!-- post_bcb_logc <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_bcb_logc) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
<p><strong>Large sample, informative prior:</strong><br><img src="bayesian_figures/indir-Stan-enf-logc-la-inf-1_save.png" width="645" style="display: block; margin: auto;"><!-- ```{r indir-Stan-enf-logc-la-inf, warning=FALSE, fig.width=7, fig.align="center", echo=FALSE} --><!-- # obtain Stan input (model already defined) --><!-- staninp <- estim_stan(samp_bcb_la, d, dimC, linC, enforce_logconc=TRUE, --><!--                       prior="informative", v_prior=v0_bcb_la) --><!-- stanfit <- stan( file = filename, data = staninp$data, chains = 1, --><!--                  warmup = 1000, iter = 2000, cores = 2, refresh = 1000 ) --><!-- # remove Stan input file --><!-- file.remove(filename) --></p>
<!-- post_bcb_logc <- rstan::extract(stanfit)$V[100*(1:10),] -->
<!-- tib_plot <- t(post_bcb_logc) %>% -->
<!--     as_tibble() %>% add_column(k=linC:dimC, .before=1) %>% -->
<!--     gather(key,value,2:(N+1)) -->
<!-- ggplot(tib_plot, aes(x=k, y=(value), color=key)) + -->
<!--     geom_line() + theme_bw() + -->
<!--     theme(legend.position="none", axis.title.x=element_blank(), axis.title.y=element_blank()) -->
<!-- ``` -->
</div>
</div>
<div id="refs" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#refs" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-GCSDVR14">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014. <em>Bayesian Data Analysis</em>. Third. Texts in Statistical Science Series. CRC Press, Boca Raton, FL.</p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#setup">Setup and notation</a></li>
      <li><a href="#prior_dist">Prior distribution</a></li>
      <li><a href="#post_ivol">Posterior for intrinsic volumes data</a></li>
      <li><a href="#biv-chibsq">Posterior for bivariate chi-bar-squared data</a></li>
      <li><a href="#refs">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Dennis Amelunxen, Martin Lotz.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
