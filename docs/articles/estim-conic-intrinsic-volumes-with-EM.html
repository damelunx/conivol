<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Estimating conic intrinsic volumes from bivariate chi-bar-squared data • conivol</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="Estimating conic intrinsic volumes from bivariate chi-bar-squared data">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">conivol</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/bayesian.html">Bayesian estimates for conic intrinsic volumes</a>
    </li>
    <li>
      <a href="../articles/conic-intrinsic-volumes.html">Conic intrinsic volumes and the (bivariate) chi-bar-squared distribution</a>
    </li>
    <li>
      <a href="../articles/estim-conic-intrinsic-volumes-with-EM.html">Estimating conic intrinsic volumes from bivariate chi-bar-squared data</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Estimating conic intrinsic volumes from bivariate chi-bar-squared data</h1>
                        <h4 class="author">Dennis Amelunxen</h4>
            
            <h4 class="date">2018-03-01</h4>
          </div>

    
    
<div class="contents">
<p>This note describes an approach to reconstruct the intrinsic volumes of a closed convex cone from samples of the associated bivariate chi-bar-squared distribution. The approach is based on the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">expectation-maximization algorithm</a> and implemented in <code>estim_em</code>. We assume familiarity with the first of the two other vignettes:</p>
<p><strong>Other vignettes:</strong></p>
<ul>
<li>
<a href="conic-intrinsic-volumes.html">Conic intrinsic volumes and (bivariate) chi-bar-squared distribution</a>: introduces conic intrinsic volumes and (bivariate) chi-bar-squared distributions, as well as the computations involving polyhedral cones,</li>
<li>
<a href="bayesian.html">Bayesian estimates for conic intrinsic volumes</a>: describes the Bayesian approach for reconstructing intrinsic volumes from sampling data, which can either be samples from the intrinsic volumes distribution (in the case of polyhedral cones), or from the bivariate chi-bar-squared distribution, and which can be with or without enforcing log-concavity of the intrinsic volumes.</li>
</ul>
<div id="setup" class="section level2">
<h2 class="hasAnchor">
<a href="#setup" class="anchor"></a>The setup</h2>
<p>We use the following notation:</p>
<ul>
<li>
<span class="math inline">\(C\subseteq\text{R}^d\)</span> denotes a closed convex cone, <span class="math inline">\(C^\circ=\{y\in\text{R}^d\mid \forall x\in C: x^Ty\leq 0\}\)</span> the polar cone, and <span class="math inline">\(\Pi_C\colon\text{R}^d\to C\)</span> denotes the orthogonal projection map, <span class="math display">\[ \Pi_C(z) = \text{argmin}\{\|x-z\|\mid x\in C\} . \]</span> We will assume in the following that <span class="math inline">\(C\)</span> (and thus <span class="math inline">\(C^\circ\)</span>) is not a linear subspace so that the intrinsic volumes with even and with odd indices each add up to <span class="math inline">\(\frac12\)</span>.</li>
<li>
<span class="math inline">\(v = v(C) = (v_0(C),\ldots,v_d(C))\)</span> denotes the vector of intrinsic volumes.</li>
<li>We work with the two main random variables <span class="math display">\[ X=\|\Pi_C(g)\|^2 ,\quad Y=\|\Pi_{C^\circ}(g)\|^2, \]</span> where <span class="math inline">\(g\sim N(0,I_d)\)</span>. So <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are chi-bar-squared distributed with reference cones <span class="math inline">\(C\)</span> and <span class="math inline">\(C^\circ\)</span>, respectively, and the pair <span class="math inline">\((X,Y)\)</span> is distributed according to the bivariate chi-bar-squared distribution with reference cone <span class="math inline">\(C\)</span>.</li>
<li>For later use in the EM algorithm we also define the latent variable <span class="math inline">\(Z\in\{0,1,\ldots,d\}\)</span>, <span class="math inline">\(\text{Prob}\{Z=k\}=v_k\)</span>.</li>
</ul>
<p>Concretely, if <span class="math inline">\(C\)</span> is a <a href="conic-intrinsic-volumes.html#polyh_cones">polyhedral cone</a> then we define <span class="math inline">\(Z\)</span> as the dimension of the face of <span class="math inline">\(C\)</span> such that <span class="math inline">\(\Pi_C(g)\)</span> lies in its relative interior. This is well-defined, as <span class="math inline">\(C\)</span> decomposes into a disjoint union of the relative interiors of its faces. For theoretical purposes we may assume without loss of generality that the underlying cone is polyhedral, as any closed convex cone can be arbitrarily well approximated by polyhedral cones.</p>
<p>Conditioning <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> on <span class="math inline">\(Z\)</span> yields independent random variables <span class="math inline">\(X\mid Z\)</span> and <span class="math inline">\(Y\mid Z\)</span>, which are <span class="math inline">\(\chi^2\)</span>-distributed with <span class="math inline">\(Z\)</span> and <span class="math inline">\(d-Z\)</span> degrees of freedom, respectively. Here and in the following we use the convention that <span class="math inline">\(\chi_0^2\)</span> denotes the Dirac measure supported in <span class="math inline">\(0\)</span>.</p>
<p>We assume to have sampled data: <span class="math inline">\((X_1,Y_1),\ldots,(X_N,Y_N)\)</span> denote iid copies of <span class="math inline">\((X,Y)\)</span> and we assume that they took the sample values <span class="math inline">\((x_1,y_1),\ldots,(x_N,y_N)\)</span>. The general goal is to reconstruct the vector of intrinsic volumes <span class="math inline">\(v=(v_0,\ldots,v_d)\)</span> from this data.</p>
<div id="mod_bivchibarsq" class="section level3">
<h3 class="hasAnchor">
<a href="#mod_bivchibarsq" class="anchor"></a>The modified bivariate chi-bar-squared distribution</h3>
<p>The latent variable <span class="math inline">\(Z\)</span> is not entirely “latent”, or hidden. Indeed, we have the following equivalences <span class="math display">\[ Z=d\iff g\in \text{int}(C) \stackrel{\text{a.s.}}{\iff} Y=0 ,\qquad
    Z=0\iff g\in \text{int}(C^\circ) \stackrel{\text{a.s.}}{\iff} X=0 , \]</span> where a.s. stands for almost surely (with probability one). Moreover, if <span class="math inline">\(Y=0\)</span>, then the value of <span class="math inline">\(X\)</span> is just a draw from the <span class="math inline">\(\chi_d^2\)</span> distribution, and similarly for <span class="math inline">\(X=0\)</span>. What this shows is that those sample data <span class="math inline">\((x_i,y_i)\)</span> with <span class="math inline">\(x_i=0\)</span> or <span class="math inline">\(y_i=0\)</span> should be regarded as noise, and hence should be discarded.</p>
<p>In order to formalize this step we define the following mixed continuous-discrete distribution: for given weight vector <span class="math inline">\(v=(v_0,\ldots,v_d)\)</span>, <span class="math inline">\(v\geq0\)</span>, <span class="math inline">\(\sum_{k=0}^d v_k=1\)</span>, define the distribution <span class="math display">\[ f_v(x,y) = v_d\, \delta(-1,0) + v_0\, \delta(0,-1) +
    \sum_{k=1}^{d-1} v_k f_k(x) f_{d-k}(y) , \]</span> where <span class="math inline">\(\delta\)</span> denotes the (bivariate) Dirac delta, and <span class="math inline">\(f_k(x)\)</span> denotes the density of the chi-squared distribution. Note that for positive <span class="math inline">\(x,y\)</span> this distribution coincides with the distribution of the bivariate chi-bar-squared distribution with weight vector <span class="math inline">\(v\)</span>. The corresponding cumulative distribution function (cdf) is given by <span class="math display">\[ F_v(x,y) = \begin{cases}
      v_d &amp; \text{if } -1\leq x&lt;0\leq y ,
   \\ v_0 &amp; \text{if } -1\leq y&lt;0\leq x ,
   \\ v_0 + v_d + \sum_{k=1}^{d-1} v_k F_k(x)F_{d-k}(y) &amp; \text{if } (x,y)\geq0 ,
   \\ 0 &amp; \text{else} ,
   \end{cases} \]</span> where <span class="math inline">\(F_k(x)\)</span> denotes the cdf of the chi-squared distribution.</p>
<p>The step of “discarding” the sample points <span class="math inline">\((x_i,y_i)\)</span> for which <span class="math inline">\(x_i=0\)</span> or <span class="math inline">\(y_i=0\)</span> then amounts to applying the map the sends <span class="math inline">\((x,0)\)</span> to <span class="math inline">\((-1,0)\)</span>, that sends <span class="math inline">\((0,y)\)</span> to <span class="math inline">\((0,-1)\)</span>, and that keeps points with positive components unchanged. The sample of the bivariate chi-bar-squared distribution then becomes a sample of the modified bivariate chi-bar-squared distribution.</p>
<p>To save notation we assume that the sample points are indexed such that <span class="math inline">\((x_1,y_1),\ldots,(x_{\bar N},y_{\bar N})\)</span> lie in the positive quadrant, <span class="math inline">\(\bar N\leq N\)</span>, and the remaining points lie on the axes. Furthermore, we define the two proportions <span class="math display">\[ p = \frac{\left|\{i\mid y_i=0\}\right|}{N} ,\quad
    q = \frac{\left|\{i\mid x_i=0\}\right|}{N} . \]</span></p>
</div>
</div>
<div id="start_EM" class="section level2">
<h2 class="hasAnchor">
<a href="#start_EM" class="anchor"></a>Choosing a starting point</h2>
As a starting point for the iterative algorithms to reconstruct <span class="math inline">\(v\)</span> we can choose the uniform distribution on <span class="math inline">\(\{0,\ldots,d\}\)</span>, <span class="math inline">\(v^{(0)}= (1,\ldots,1)/(d+1)\)</span>, but we may as well use a more elaborated first guess. The mean and variance of <span class="math inline">\(v\)</span>, more precisely, of the latent variable <span class="math inline">\(Z\)</span>, can be conveniently estimated from the sample data, as
<span class="math display">\[\begin{align*}
   \delta &amp; = \delta(C) = \sum_{k=0}^d k v_k(C) = \text{E}[X] = d-\text{E}[Y] ,
\\ \text{var} &amp; = \text{var}(C) = \sum_{k=0}^d (k-\delta)^2 v_k(C)
\\ &amp; = \text{E}[X^2]-(\delta+1)^2+1 = \text{E}[Y^2]-(d-\delta+1)^2+1 .
\end{align*}\]</span>
<p>See the <a href="../doc/estim-conic-intrinsic-volumes-with-EM.html#bivchibars_cone">vignette on intrinsic volumes</a> for the context of this connection.</p>
<p>The sample data of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> thus yield two natural estimates for both <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\text{var}\)</span>, which we combine as follows:</p>
<p><span class="math display">\[ \left.\begin{array}{c}
   \displaystyle\hat\delta_{\text{prim}} = \frac{1}{N}\sum_{i=1}^N x_i
\\ \displaystyle\hat\delta_{\text{pol}} = d-\frac{1}{N}\sum_{i=1}^N y_i
   \end{array} \right\} \quad\hat\delta := \frac{\hat\delta_{\text{prim}}+\hat\delta_{\text{pol}}}{2} , \]</span></p>
<p><span class="math display">\[ \left.\begin{array}{c}
   \displaystyle\widehat{\text{var}}_{\text{prim}} = \frac{1}{N}\sum_{i=1}^N x_i^2 - (\hat\delta+1)^2+1
\\ \displaystyle\widehat{\text{var}}_{\text{pol}} = \frac{1}{N}\sum_{i=1}^N y_i^2 - (d-\hat\delta+1)^2+1
   \end{array} \right\} \quad \widehat{\text{var}} = \sqrt{ \widehat{\text{var}}_{\text{prim}} \widehat{\text{var}}_{\text{pol}} } . \]</span> These formulas are implemented in <code>estim_statdim_var</code>.</p>
<p>We propose the following ways to choose a starting point for the iterative algorithms to find the intrinsic volumes:</p>
<ol start="0" style="list-style-type: decimal">
<li>
<em>uniform distribution:</em> <span class="math inline">\(v^{(0)}=\frac{1}{d+1}(1,\ldots,1)\)</span>.</li>
<li>
<em>normal distribution:</em> match a normal distribution to the first and second moments of <span class="math inline">\(Z\)</span> and discretize: <span class="math display">\[ v^{(0)}_k = \frac{\text{Prob}\{k-0.5\leq \nu\leq k+0.5\}}{\text{Prob}\{-0.5\leq \nu\leq d+0.5\}} \]</span> where <span class="math inline">\(\nu\sim N(\hat\delta,\widehat{\text{var}})\)</span>.</li>
<li>
<em>circular cones</em>: <span class="math inline">\(d\)</span>-dimensional cones of angle <span class="math inline">\(\alpha\)</span> have an approximate statistical dimension of <span class="math inline">\(d\sin^2\alpha\)</span> and an approximate variance of <span class="math inline">\((d/2-1)\sin^2(2\alpha)\)</span>. We can thus choose <span class="math inline">\(\alpha\)</span> to match either the statistical dimension, or the variance, or take an average of both fits:
<ol style="list-style-type: lower-alpha">
<li>
<em>match first moment</em>: take <span class="math inline">\(\alpha=\text{arcsin}\sqrt{\hat\delta/d}\)</span> and take <span class="math inline">\(v^{(0)}\)</span> to be the intrinsic volumes of a <span class="math inline">\(d\)</span>-dimensional circular cone with an angle of <span class="math inline">\(\alpha\)</span>.</li>
<li>
<em>match second moment</em>: if <span class="math inline">\(\hat\delta&lt;d/2\)</span> then take <span class="math inline">\(\beta=\frac{1}{2}\text{arcsin}\sqrt{\frac{2\widehat{\text{var}}}{d-2}}\)</span> else take <span class="math inline">\(\beta=\frac{\pi}{2}-\frac{1}{2}\text{arcsin}\sqrt{\frac{2\widehat{\text{var}}}{d-2}}\)</span> and take <span class="math inline">\(v^{(0)}\)</span> to be the intrinsic volumes of a <span class="math inline">\(d\)</span>-dimensional circular cone with an angle of <span class="math inline">\(\beta\)</span>.</li>
<li>
<em>average both fits</em>: take <span class="math inline">\(v^{(0)}\)</span> as the geometric mean of the intrinsic volumes obtained in (a) and (b).</li>
</ol>
</li>
</ol>
<p>These formulas are implemented in <code>init_v</code>.</p>
<p><strong>Example computations:</strong> We analyze a randomly chosen ellipsoidal cone:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="dv">12</span>
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(d^<span class="dv">2</span>),d,d)
<span class="kw"><a href="../reference/ellips_semiax.html">ellips_semiax</a></span>(A)
<span class="co">#&gt;  [1] 1.965675989 1.551430100 1.392832384 1.181547315 1.030489491</span>
<span class="co">#&gt;  [6] 0.944529743 0.775232956 0.641000545 0.271741231 0.204502118</span>
<span class="co">#&gt; [11] 0.008808626</span></code></pre></div>
<p>Sample from the corresponding bivariate chi-bar-squared distribution, and use this to estimate statistical dimension and variance and find initial estimates of intrinsic volumes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="fl">1e5</span>
E &lt;-<span class="st"> </span><span class="kw"><a href="../reference/ellips_rbichibarsq.html">ellips_rbichibarsq</a></span>(N,A)
<span class="kw">str</span>(E)
<span class="co">#&gt; List of 2</span>
<span class="co">#&gt;  $ semiax : num [1:11] 1.97 1.55 1.39 1.18 1.03 ...</span>
<span class="co">#&gt;  $ samples: num [1:100000, 1:2] 6.5 1.8 6.45 2.45 8.33 ...</span>

<span class="co"># scatter plot of the sample</span>
<span class="kw">ggplot</span>(<span class="kw">as_tibble</span>(E$samples), <span class="kw">aes</span>(V1,V2)) +<span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">alpha=</span>.<span class="dv">02</span>) +
<span class="st">    </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(),<span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-samp-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mom &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_statdim_var.html">estim_statdim_var</a></span>(d, E$samples); mom
<span class="co">#&gt; $delta</span>
<span class="co">#&gt; [1] 5.009417</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $var</span>
<span class="co">#&gt; [1] 3.680233</span>
v_est &lt;-<span class="st"> </span><span class="kw">list</span>( <span class="dt">mode0 =</span> <span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>(d, <span class="dv">0</span>),
               <span class="dt">mode1 =</span> <span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>(d, <span class="dv">1</span>, mom$delta, mom$var),
               <span class="dt">mode2 =</span> <span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>(d, <span class="dv">2</span>, mom$delta),
               <span class="dt">mode3 =</span> <span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>(d, <span class="dv">3</span>, mom$delta, mom$var),
               <span class="dt">mode4 =</span> <span class="kw"><a href="../reference/init_ivols.html">init_ivols</a></span>(d, <span class="dv">4</span>, mom$delta, mom$var) )
tib_plot &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(v_est) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span><span class="dv">0</span>:d,<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(mode,v,<span class="dv">2</span>:<span class="dv">6</span>)
<span class="kw">ggplot</span>(tib_plot,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>v,<span class="dt">color=</span>mode)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-samp-2.png" width="672"></p>
</div>
<div id="the-likelihood-function" class="section level2">
<h2 class="hasAnchor">
<a href="#the-likelihood-function" class="anchor"></a>The likelihood function</h2>
<p>We derive the likelihood function of the modified bivariate chi-bar-squared distribution, as explained <a href="#mod_bivchibarsq">above</a>. Recall that the proportions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are defined via <span class="math inline">\(p=\frac{\left|\{i\mid y_i=0\}\right|}{N}\)</span> and <span class="math inline">\(q=\frac{\left|\{i\mid x_i=0\}\right|}{N}\)</span>, and are point estimates for <span class="math inline">\(v_d(C)\)</span> and <span class="math inline">\(v_0(C)\)</span>, respectively. The data on which we base our estimation thus has the following form:</p>
<ul>
<li>
<em>general data</em>: dimension <span class="math inline">\(d\)</span> and sample size <span class="math inline">\(N\)</span>
</li>
<li>
<em>estimates of <span class="math inline">\(v_d\)</span> and <span class="math inline">\(v_0\)</span></em>: <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>
</li>
<li>
<em>remaining samples</em>: without loss of generality we use the notation <span class="math inline">\((x_1,y_1),\ldots,(x_{\bar N},y_{\bar N})\)</span> for the remaining sample points, <span class="math inline">\(\bar N\leq N\)</span>, or shortly <span class="math inline">\(\mathbf{x},\mathbf{y}\)</span> for the sample vectors.</li>
</ul>
<p>We obtain the following formula for the likelihood of the given data: <span class="math display">\[ L(v\mid \mathbf{x},\mathbf{y}) = v_d^{Np} v_0^{Nq}
  \prod_{i=1}^{\bar N}\sum_{k=1}^{d-1} v_k f_{ik} , \]</span> where <span class="math inline">\(f_{ik}\)</span> denote the density values <span class="math display">\[ f_{ik} := f_k(x_i)f_{d-k}(y_i) ,\quad k=1,\ldots,d-1,\; i=1,\ldots,\bar N , \]</span> with <span class="math inline">\(f_k(x)\)</span> denoting the density of the chi-squared distribution, <span class="math display">\[ f_k(x)=\frac{1}{2^{k/2}\Gamma(k/2)}x^{k/2-1}e^{-x/2} . \]</span> The function <code>prepare_em</code> evaluates the sample data in this regard; calling this function outside the EM algorithm allows to skip this step on multiple evaluations of the EM algorithm or related functions.</p>
The above likelihood function does not take the latent variable <span class="math inline">\(Z\)</span> into account, which is crucial for the EM algorithm. Assuming that we have sample values <span class="math inline">\(\mathbf{z}=(z_1,\ldots,z_{\bar N})\)</span> for the latent variables, we arrive at the likelihood function
<span class="math display">\[\begin{align*}
   L(v\mid \mathbf{x},\mathbf{y},\mathbf{z}) &amp; = v_d^{Np} v_0^{Nq}
    \prod_{i=1}^{\bar N} v_{z_i} f_{iz_i}
\\ &amp; = v_d^{Np} v_0^{Nq}
    \prod_{i=1}^{\bar N}\prod_{k=1}^{d-1} (v_k f_{ik})^{(z_i=k)} ,
\end{align*}\]</span>
<p>where <span class="math inline">\((z_i=k)=1\)</span> if <span class="math inline">\(z_i=k\)</span> and zero else. For numerical reasons we work with the rescaled log-likelihood functions</p>
<span class="math display">\[\begin{align*}
   \frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y}) &amp; = p \log v_d + q \log v_0
    + \frac{1}{N}\sum_{i=1}^{\bar N} \log\Big(\sum_{k=1}^{d-1} v_k f_{ik}\Big) ,
\\ \frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y},\mathbf{z}) &amp; = p \log v_d
    + q \log v_0 + \frac{1}{N}\sum_{i=1}^{\bar N} \sum_{k=1}^{d-1} (z_i=k)
    \big( \log v_k + \log f_{ik} \big) .
\end{align*}\]</span>
<p>The rescaled log-likelihood function <span class="math inline">\(\frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y})\)</span> is implemented in <code>loglike_ivols</code>.</p>
<p><strong>Example computations:</strong> We continue analyzing the above defined ellipsoidal cone. First, we prepare the data for multiple executions of the log-likelihood function and of the EM algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out_prep &lt;-<span class="st"> </span><span class="kw"><a href="../reference/prepare_em.html">prepare_em</a></span>(d, E$samples)</code></pre></div>
<p>Now evaluate the log-likelihood function in the simple estimate of the intrinsic volumes based on the moments.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/loglike_ivols.html">loglike_ivols</a></span>( v_est$mode0 , out_prep )
<span class="co">#&gt; [1] -5.545065</span>
<span class="kw"><a href="../reference/loglike_ivols.html">loglike_ivols</a></span>( v_est$mode1 , out_prep )
<span class="co">#&gt; [1] -5.24238</span>
<span class="kw"><a href="../reference/loglike_ivols.html">loglike_ivols</a></span>( v_est$mode2 , out_prep )
<span class="co">#&gt; [1] -5.251429</span>
<span class="kw"><a href="../reference/loglike_ivols.html">loglike_ivols</a></span>( v_est$mode3 , out_prep )
<span class="co">#&gt; [1] -5.414345</span>
<span class="kw"><a href="../reference/loglike_ivols.html">loglike_ivols</a></span>( v_est$mode4 , out_prep )
<span class="co">#&gt; [1] -5.287737</span></code></pre></div>
<div id="log-concavity" class="section level3">
<h3 class="hasAnchor">
<a href="#log-concavity" class="anchor"></a>Log-concavity</h3>
<p>The log-concavity inequalities are the inequalities <span class="math inline">\(v_k^2\geq v_{k-1}v_{k+1}\)</span> for <span class="math inline">\(k=1,\ldots,d-1\)</span>, or equivalently, <span class="math display">\[ 2\log v_k - \log v_{k-1} - \log v_{k+1} \geq 0 . \]</span> At this moment, the validity of these inequalities for general closed convex cones is an open conjecture. But for subclasses, like products of circular cones, they are known to hold, and there is some evidence that they hold in general. See the vignette on <a href="conic-intrinsic-volumes.html#inequs">conic intrinsic volumes</a> for a discussion. Assuming log-concavity of the conic intrinsic volumes greatly helps the EM algorithm to converge to a sensible estimate.</p>
<p>There are two natural ways in which log-concavity may be enforced:</p>
<ol style="list-style-type: decimal">
<li>soft enforcement through a penalty term in the log-likelihood functions:
<span class="math display">\[\begin{align*}
 F_\lambda(v) &amp; := \frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y})
+ \sum_{k=1}^{d-1} \lambda_k (2\log v_k - \log v_{k-1} - \log v_{k+1}) ,
  \\ G_\lambda(v,\mathbf{z}) &amp; := \frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y},\mathbf{z})
+ \sum_{k=1}^{d-1} \lambda_k (2\log v_k - \log v_{k-1} - \log v_{k+1}) ,
  \end{align*}\]</span>
where <span class="math inline">\(\lambda_1,\ldots,\lambda_{d-1}\geq0\)</span> and <span class="math inline">\(\lambda_0:=\lambda_d:=0\)</span> (setting all <span class="math inline">\(\lambda_k\)</span> to zero yields the original log-likelihood functions). It should be noted that these “penalty terms” may cancel out, effectively rendering them useless. A true log-concavity penalty would have a form like <span class="math inline">\(\min\{0,2\log v_k - \log v_{k-1} - \log v_{k+1}\}\)</span>; we use the simple (albeit potentially useless) penalty as it allows for easy implementation in the EM step.</li>
<li>projecting the logarithms of the iterates, <span class="math inline">\(u_k=\log v_k\)</span>, <span class="math inline">\(k=0,\ldots,d\)</span>, onto the polyhedral set <span class="math display">\[ \big\{ u\in\text{R}^{d+1}\mid 2 u_k - u_{k-1} - u_{k+1} \geq c_k \big\} . \]</span> for some <span class="math inline">\(c_1,\ldots,c_{d-1} (\geq0)\)</span> (setting all <span class="math inline">\(c_k\)</span> to <span class="math inline">\(-\infty\)</span> eliminates this restriction).</li>
</ol>
<p>Both of these methods are supported in <code>estim_em</code>.</p>
</div>
</div>
<div id="expectation-maximization-em-algorithm" class="section level2">
<h2 class="hasAnchor">
<a href="#expectation-maximization-em-algorithm" class="anchor"></a>Expectation maximization (EM) algorithm</h2>
<p>The expectation maximization (EM) algorithm works by maximizing the conditional expectation of the maximum likelihood method – expectation with respect to the latent variable – given the current iterate for the parameter that is to be found.</p>
For the situation at hand we obtain the conditional probabilities for the latent variable as follows: <span class="math display">\[ p(Z_i=k\mid X_i=x_i, Y_i=y_i) = \frac{p(Z_i=k)p(X_i=x_i,Y_i=y_i\mid Z_i=k)}
    {p(X_i=x_i,Y_i=y_i)} = \frac{v_kf_{ik}}{\sum_{j=1}^{d-1}v_jf_{ij}} . \]</span> Hence, denoting the current estimate of <span class="math inline">\(v\)</span> by <span class="math inline">\(v^{(t)}\)</span>, the next iterate of the EM algorithm is found by maximizing the expression
<span class="math display">\[\begin{align*}
   &amp; \underset{\mathbf{Z}\mid \mathbf{x},\mathbf{y},v^{(t)}}{\text{E}}
    \big[\tfrac{1}{N}\log L(v\mid\mathbf{x},\mathbf{y},\mathbf{Z})\big]
\\ &amp; = p \log v_d + q \log v_0 + \frac{1}{N}\sum_{i=1}^{\bar N} \sum_{k=1}^{d-1}
    \frac{v_k^{(t)}f_{ik}}{\sum_{j=1}^{d-1} v_j^{(t)}f_{ij}} \big( \log v_k +
    \log f_{ik} \big)
\end{align*}\]</span>
<p>as a function in <span class="math inline">\(v\)</span>. Apparently, the final summand can be dropped as it only contributes a constant. Furthermore, replacing the log-likelihood function by the log-concavity penalized version <span class="math inline">\(G_\lambda(v,\mathbf{z})\)</span>, we see that the next iterate <span class="math inline">\(v^{(t+1)}\)</span> is found by maximizing the function <span class="math display">\[ (p-\lambda_{d-1}) \log v_d + (q-\lambda_1) \log v_0 + 
    \sum_{k=1}^{d-1} \Bigg( \frac{1}{N}\sum_{i=1}^{\bar N}
    \frac{v_k^{(t)}f_{ik}}{\sum_{j=1}^{d-1} v_j^{(t)}f_{ij}} + 2\lambda_k
    -\lambda_{k-1}-\lambda_{k+1}\Bigg) \log v_k \]</span> subject to <span class="math inline">\(v_0,\ldots,v_d\geq0\)</span>, <span class="math inline">\(v_0+v_2+\dots=v_1+v_3+\dots=\frac12\)</span>. As long as the coefficients of <span class="math inline">\(\log v_k\)</span> are nonnegative, this is a convex problem and, as a <em>separable convex program</em><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, can be easily solved by MOSEK.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>The log-concavity inequalities <span class="math inline">\(2\log v_k-\log v_{k-1}-\log v_{k+1}\geq c_k\;(\geq0)\)</span> could be modelled as constraints in the above separable optimization problem. But including these will violate the convexity assumptions right away, so the current implementation of the algorithm does not proceed in this way. Instead, after computing the iterate in the normal way, the algorithm will project the vector of the logarithms <span class="math inline">\(u_k=\log v_k\)</span>, <span class="math inline">\(k=0,\ldots,d\)</span>, onto the polyhedral set <span class="math inline">\(\{ u\in\text{R}^{d+1}\mid 2 u_k - u_{k-1} - u_{k+1} \geq c_k \}\)</span> and then rescale the resulting vector so that <span class="math inline">\(v_0^{(t+1)}+v_2^{(t+1)}+\dots=v_1^{(t+1)}+v_3^{(t+1)}+\dots=\frac12\)</span>.</p>
<p><strong>Example computations:</strong> We reconstruct the intrinsic volumes from the sample data. For this we run the EM algorithm with the different starting points for 200 iterations, and display the every twenty-fifth iteration.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">em0 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_em.html">estim_em</a></span>( d, E$samples, <span class="dt">N=</span><span class="dv">200</span>, <span class="dt">v_init=</span>v_est$mode0, <span class="dt">data=</span>out_prep )
em1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_em.html">estim_em</a></span>( d, E$samples, <span class="dt">N=</span><span class="dv">200</span>, <span class="dt">v_init=</span>v_est$mode1, <span class="dt">data=</span>out_prep )
em2 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_em.html">estim_em</a></span>( d, E$samples, <span class="dt">N=</span><span class="dv">200</span>, <span class="dt">v_init=</span>v_est$mode2, <span class="dt">data=</span>out_prep )
em3 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_em.html">estim_em</a></span>( d, E$samples, <span class="dt">N=</span><span class="dv">200</span>, <span class="dt">v_init=</span>v_est$mode3, <span class="dt">data=</span>out_prep )
em4 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_em.html">estim_em</a></span>( d, E$samples, <span class="dt">N=</span><span class="dv">200</span>, <span class="dt">v_init=</span>v_est$mode4, <span class="dt">data=</span>out_prep )

<span class="co"># prepare plots</span>
tib_plot0 &lt;-<span class="st"> </span><span class="kw">as_tibble</span>( <span class="kw">t</span>(em0$iterates[<span class="dv">1+25</span>*(<span class="dv">0</span>:<span class="dv">8</span>), ]) ) %&gt;%
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>))) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span><span class="dv">0</span>:d,<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(step,value,<span class="dv">2</span>:<span class="dv">10</span>)
tib_plot1 &lt;-<span class="st"> </span><span class="kw">as_tibble</span>( <span class="kw">t</span>(em1$iterates[<span class="dv">1+25</span>*(<span class="dv">0</span>:<span class="dv">8</span>), ]) ) %&gt;%
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>))) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span><span class="dv">0</span>:d,<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(step,value,<span class="dv">2</span>:<span class="dv">10</span>)
tib_plot2 &lt;-<span class="st"> </span><span class="kw">as_tibble</span>( <span class="kw">t</span>(em2$iterates[<span class="dv">1+25</span>*(<span class="dv">0</span>:<span class="dv">8</span>), ]) ) %&gt;%
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>))) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span><span class="dv">0</span>:d,<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(step,value,<span class="dv">2</span>:<span class="dv">10</span>)
tib_plot3 &lt;-<span class="st"> </span><span class="kw">as_tibble</span>( <span class="kw">t</span>(em3$iterates[<span class="dv">1+25</span>*(<span class="dv">0</span>:<span class="dv">8</span>), ]) ) %&gt;%
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>))) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span><span class="dv">0</span>:d,<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(step,value,<span class="dv">2</span>:<span class="dv">10</span>)
tib_plot4 &lt;-<span class="st"> </span><span class="kw">as_tibble</span>( <span class="kw">t</span>(em4$iterates[<span class="dv">1+25</span>*(<span class="dv">0</span>:<span class="dv">8</span>), ]) ) %&gt;%
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>))) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span><span class="dv">0</span>:d,<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(step,value,<span class="dv">2</span>:<span class="dv">10</span>)

tib_plot0$step &lt;-<span class="st"> </span><span class="kw">factor</span>(tib_plot0$step, <span class="dt">levels =</span> <span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>)))
tib_plot1$step &lt;-<span class="st"> </span><span class="kw">factor</span>(tib_plot1$step, <span class="dt">levels =</span> <span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>)))
tib_plot2$step &lt;-<span class="st"> </span><span class="kw">factor</span>(tib_plot2$step, <span class="dt">levels =</span> <span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>)))
tib_plot3$step &lt;-<span class="st"> </span><span class="kw">factor</span>(tib_plot3$step, <span class="dt">levels =</span> <span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>)))
tib_plot4$step &lt;-<span class="st"> </span><span class="kw">factor</span>(tib_plot4$step, <span class="dt">levels =</span> <span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>)))

<span class="co"># plot mode 0</span>
<span class="kw">ggplot</span>(tib_plot0,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>value,<span class="dt">color=</span>step)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-em-1.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot mode 1</span>
<span class="kw">ggplot</span>(tib_plot1,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>value,<span class="dt">color=</span>step)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-em-2.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot mode 2</span>
<span class="kw">ggplot</span>(tib_plot2,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>value,<span class="dt">color=</span>step)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-em-3.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot mode 3</span>
<span class="kw">ggplot</span>(tib_plot3,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>value,<span class="dt">color=</span>step)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-em-4.png" width="672"></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot mode 4</span>
<span class="kw">ggplot</span>(tib_plot4,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>value,<span class="dt">color=</span>step)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-em-5.png" width="672"></p>
<p>Comparing the 200th iterates of the different runs shows that they have pretty much converged to the same estimate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># prepare plot</span>
tib_plot_comp &lt;-<span class="st"> </span><span class="kw">tibble</span>(  <span class="dt">mode0=</span>em0$iterates[<span class="dv">201</span>, ],
                          <span class="dt">mode1=</span>em1$iterates[<span class="dv">201</span>, ],
                          <span class="dt">mode2=</span>em2$iterates[<span class="dv">201</span>, ],
                          <span class="dt">mode3=</span>em3$iterates[<span class="dv">201</span>, ],
                          <span class="dt">mode4=</span>em4$iterates[<span class="dv">201</span>, ] ) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span><span class="dv">0</span>:d,<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(mode,value,<span class="dv">2</span>:<span class="dv">6</span>)

<span class="co"># plot 200th iterate of different runs</span>
<span class="kw">ggplot</span>(tib_plot_comp,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>value,<span class="dt">color=</span>mode)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-em-comp-1.png" width="672"></p>
<p>To assess the influence of log-concavity we also have a look at the iterates of the EM algorithm without enforced log-concavity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">em_no_logconc &lt;-<span class="st"> </span><span class="kw"><a href="../reference/estim_em.html">estim_em</a></span>( d, E$samples, <span class="dt">N=</span><span class="dv">200</span>, <span class="dt">v_init=</span>v_est$mode0, <span class="dt">data=</span>out_prep,
                           <span class="dt">no_of_lcc_projections=</span><span class="dv">0</span> )

<span class="co"># prepare plot</span>
tib_plot_no_logconc &lt;-<span class="st"> </span><span class="kw">as_tibble</span>( <span class="kw">t</span>(em_no_logconc$iterates[<span class="dv">1+25</span>*(<span class="dv">0</span>:<span class="dv">8</span>), ]) ) %&gt;%
<span class="st">    `</span><span class="dt">colnames&lt;-</span><span class="st">`</span>(<span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>))) %&gt;%
<span class="st">    </span><span class="kw">add_column</span>(<span class="dt">k=</span><span class="dv">0</span>:d,<span class="dt">.before=</span><span class="dv">1</span>) %&gt;%<span class="st"> </span><span class="kw">gather</span>(step,value,<span class="dv">2</span>:<span class="dv">10</span>)

tib_plot_no_logconc$step &lt;-<span class="st"> </span><span class="kw">factor</span>(tib_plot_no_logconc$step, <span class="dt">levels =</span> <span class="kw">paste0</span>(<span class="st">"s_"</span>,<span class="dv">25</span>*(<span class="dv">0</span>:<span class="dv">8</span>)))

<span class="kw">ggplot</span>(tib_plot_no_logconc,<span class="kw">aes</span>(<span class="dt">x=</span>k,<span class="dt">y=</span>value,<span class="dt">color=</span>step)) +
<span class="st">    </span><span class="kw">geom_line</span>() +<span class="st"> </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</code></pre></div>
<p><img src="estim-conic-intrinsic-volumes-with-EM_figures/ellips-em-no-logconc-1.png" width="672"></p>
<p>It is worth noting that as the algorithm progresses the log-concavity gets increasingly violated leading to bad estimates of the intrinsic volumes. So assuming log-concavity seems crucial for obtaining reasonable estimates of the intrinsic volumes.</p>
<!-- ## References -->
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>For more details see the MOSEK documentation for <a href="http://docs.mosek.com/8.1/rmosek/tutorial-scopt-shared.html">SCopt interface</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>The implementation of this step in <code>estim_em</code> is such that MOSEK is called with the user-defined values for <span class="math inline">\(\lambda\)</span>. These values will be reduced if MOSEK returns an error because of nonconvexity; they might eventually be dropped entirely (in which case the program becomes convex and is solved by MOSEK). It thus could happen that although the penalty terms <span class="math inline">\(\lambda\)</span> have been set to positive values, they are being ignored by (parts of) the algorithm.<a href="#fnref2">↩</a></p></li>
</ol>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#setup">The setup</a></li>
      <li><a href="#start_EM">Choosing a starting point</a></li>
      <li><a href="#the-likelihood-function">The likelihood function</a></li>
      <li><a href="#expectation-maximization-em-algorithm">Expectation maximization (EM) algorithm</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Dennis Amelunxen, Martin Lotz.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
