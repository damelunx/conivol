---
title: "Conic intrinsic volumes and the (bivariate) chi-bar-squared distribution"
author: "Dennis Amelunxen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
# output: rmarkdown::pdf_document
# header-includes:
#   - \hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
link-citations: true
---

This note introduces the conic intrinsic volumes and the associated (bivariate)
chi-bar-squared distributions. The focus is on algorithmic considerations; see
the survey paper [@AL17] and the references therein for more
background on the conic intrinsic volumes.
We will give further pointers to the literature within the text below.


**Other vignettes:**

* [Estimating conic intrinsic volumes via EM algorithm](../doc/estim-conic-intrinsic-volumes-with-EM.html):
    describes the details of the algorithm for finding the intrinsic volumes of closed
    convex cones from samples of the associated bivariate chi-bar-squared distribution,
* [Bayesian estimates for conic intrinsic volumes](../doc/bayesian.html):
    describes the Bayesian approach for reconstructing intrinsic volumes
    from sampling data, which can either be samples from the intrinsic
    volumes distribution (in the case of polyhedral cones), or from the
    bivariate chi-bar-squared distribution, and which can be with or without
    enforcing log-concavity of the intrinsic volumes.

## Closed convex cones

As the name suggests, a closed convex cone $C\subseteq\text{R}^d$ satisfies
the following three properties:  

1. $C$ is **closed**:  
the limit of any convergent sequence is an element of $C$,
  \[ \forall x_1,x_2,\ldots\in C \text{ such that } \lim_{i\to\infty} x_i
    \text{ exists}: \lim_{i\to\infty} x_i\in C \]
2. $C$ is **convex**:  
any convex combination of elements in $C$ lies in $C$,  
  \[ \forall x,y\in C,\; 0\leq \lambda\leq 1 : \lambda x+(1-\lambda) y\in C \]
3. $C$ is a **cone**:  
any positive scaling of an element in $C$ lies in $C$,
  \[ \forall x\in C\;,\lambda>0 : \lambda x\in C \]

Properties 1. and 3. imply that every nonzero closed cone contains the origin,
$0\in C$. Furthermore, if $0\in C$, then properties 2. and 3. taken together
are equivalent to the property
  \[ \forall x,y\in C,\; \lambda,\mu\geq0 : \lambda x+\mu y\in C \]
In other words, a (nonempty) closed convex cone is a closed set, which is
**closed under nonnegative linear combinations**. Compare this to linear
subspaces which can be characterized through their closedness under
(possibly negative) linear combinations.

**Note:**

* Below we will always assume that closed convex cones are nonempty, hence,
they contain at least the origin.
* If $C\subseteq\text{R}^d$ and $D\subseteq\text{R}^e$ are closed
convex cones, then their direct product is a closed convex cone,
$C\times D\subseteq\text{R}^{d+e}$.

### Polar cones

Every closed convex cone $C\subseteq\text{R}^d$ defines a natural counterpart,
its polar cone
  \[ C^\circ := \{y\in\text{R}^d\mid \forall x\in C: x^Ty\leq 0\} . \]
The polar cone is also a closed convex cone, and the polar of the polar cone
is the original cone, which in this context is also called the primal cone,
$(C^\circ)^\circ = C$. It thus makes sense to think about the primal cone and
its polar cone as a primal-polar pair of cones. A cone is called self-dual,
if $C^\circ=-C$.  

Note that $(\text{R}^d)^\circ=\{0\}$. Also, polarity behaves well with direct
products, $(C\times D)^\circ = C^\circ\times D^\circ$.  
The polar cone depends on the ambient space, i.e., if we embed
$C\subseteq\text{R}^d$ in $\text{R}^e$, $e\geq d$, by formally identifying $C$
with $C\times\{0\}\subseteq\text{R}^e$, then the polar of the embedded cone
is given by $(C\times\{0\})^\circ = C^\circ\times\text{R}^{e-d}$ (as opposed
to the polar of $C$ in $\text{R}^d$ embedded in $\text{R}^e$).

If the cone $C$ is transformed by an invertible linear transformation then
its polar cone is transformed by the transpose of the inverse of the
transformation:
  \[ (AC)^\circ = B^T C^\circ ,\qquad \text{if $A$ invertible and } B=A^{-1} . \]

### Dimension and lineality {#dim_lin}

The smallest linear space containing a cone $C$ is given by $C+(-C)$, and
the largest linear space contained in $C$, called the lineality space of $C$,
is given by $C\cap (-C)$. Linear hull and lineality space are related by
polarity, as

  \[ C^\circ+(-C^\circ) = (C\cap (-C))^\bot ,\quad C^\circ\cap(-C^\circ) = (C+(-C))^\bot , \]
where $L^\bot$ denotes the orthogonal complement of a linear space. Every
closed convex cone has the natural orthogonal decomposition
  \[ C = L + \bar C , \]
where $L=C\cap (-C)$ denotes the lineality space, and $\bar C=\Pi_{L^\bot}(C)$
the image of the projection of $C$ onto the orthogonal complement of $L$.

The (linear) dimension of a cone is the dimension of its linear hull, and the
lineality of a cone is the dimension of its lineality space,

  \[ \text{dim}(C) = \text{dim}(C+(-C)) ,\quad \text{lin}(C) = \text{dim}(C\cap(-C)) . \]
Note that by the above polarity statement we have
$\text{dim}(C)=d-\text{lin}(C^\circ)$ and $\text{lin}(C)=d-\text{dim}(C^\circ)$.

### Examples: linear subspaces

A trivial yet important class of examples of closed convex cones are the
linear subspaces. It is easily seen that the polar cone of a linear
subspace is just its orthogonal complement. Clearly, a cone is a linear
space iff $\text{dim}(C)=\text{lin}(C)$.  

The intrinsic volumes and (bivariate) chi-bar-squared distributions
(see [below](#bivchibarsq)) are in this case equivalent to the dimension
of the subspace and the chi-squared distribution
(see [below](#bivchibars_cone) for more precise statements).  

Furthermore, for cones which are not linear subspaces the intrinsic
volumes satisfy an additional linear relation, which is why the algorithms
in `conivol` assume that the underlying closed convex cone is **not**
a linear subspace.

### Examples: polyhedral cones {#polyh_cones}

Polyhedral cones are a straightforward generalization of linear subspaces
and can be described in two ways, which are related via polarity:

1. as nonnegative linear combinations of a finite number of vectors,
  \[ C = \{Az \mid z\in\text{R}^n, z\geq 0\} ,\quad A\in\text{R}^{d\times n} , \]
where the inequality $z\geq0$ is meant componentwise,
2. as the intersection of finitely many closed linear half-spaces,
  \[ C^\circ = \{y\in\text{R}^d \mid A^Ty\leq 0\} . \]

Although the set of polyhedral cones does not exhaust the set of closed
convex cones, it can be shown (and it is intuitively clear) that every
closed convex cone can be arbitrarily well approximated by polyhedral cones.

For later use we recall that a face of a polyhedral cone is the intersection
of a linear hyperplane (linear subspace of codimension one) with the cone,
where the cone has to lie entirely in one of the two closed half-spaces
defined by the hyperplane. Every polyhedral cone has a finite number of
faces, and decomposes into the disjoint union of the relative interiors
of its faces,
  \[ C = \dot{\bigcup_{F\text{ face of } C}} \text{relint}(F) .  \]
See the [appendix](#sampling_polyh) for computations involving the natural
orthogonal decomposition of polyhedral cones.

### Examples: simplicial cones and Weyl chambers of finite reflection groups {#simpl_weyl}

A special class of polyhedral cones is given by so-called simplicial
cones. These are full-dimensional cones with a minimal number 
of edges and facets. Formally, we can describe these cones as positive
orthants transformed by an invertible linear transformation:
  \[ \big\{\text{simplicial cones in }\text{R}^d\big\}
    = \big\{A\text{R}_+^d\mid A\in\text{R}^{d\times d} \text{ invertible} \big\} . \]
In other words, a simplicial cone is generated by the columns of an
invertible matrix. Note that since the positive orthant is self-dual,
$(\text{R}_+^d)^\circ=-\text{R}_+^d$, we easily obtain the polar cone
of a simplicial cone through the inverse matrix,
  \[ \big( A\text{R}_+^d \big)^\circ = -B^T\text{R}_+^d ,\qquad
    \text{if $A$ invertible and } B=A^{-1} . \]

A special class of simplicial cones are the Weyl chambers of finite
reflection groups of which the most important ones are of class 'A',
'BC', and 'D'. These chambers are isometric to the following three cones,
\begin{align*}
   C_A & = \big\{ x\in\text{R}^{d+1} \mid x_1 \leq x_2 \leq \dots \leq x_{d+1} \big\} ,
\\ C_{BC} & = \big\{ x\in\text{R}^d \mid 0\leq x_1 \leq x_2 \leq \dots \leq x_d \big\} ,
\\ C_D & = \big\{ x\in\text{R}^d \mid -x_2\leq x_1 \leq x_2 \leq \dots \leq x_d \big\} .
\end{align*}
Note that the first cone is strictly speaking not simplicial, as it
contains the one-dimensional linear subspace $L=\{\lambda 1_d\mid
\lambda\in\text{R}\}$, where $1_d\in\text{R}^d$ denotes the all-one vector.
However, the projection of $C$ onto the orthogonal complement of $L$ yields
the natural orthogonal decomposition $C_A=L+\bar C_A$, and the projection
$\bar C_A$ is indeed a simplicial cone. Using the matrix description
$\{y\in\text{R}^d \mid A^Ty\leq 0\}$, we see that the above defined
cones correspond to the matrices
\begin{align*}
   A_A & = \begin{pmatrix}
       1
    \\ -1 & 1
    \\    & \ddots & \ddots
    \\    & & -1 & 1
    \\    & & & -1
    \end{pmatrix} ,
   & A_{BC} & = \begin{pmatrix}
       -1 & 1
    \\    & \ddots & \ddots
    \\    & & -1 & 1
    \\    & & & -1
    \end{pmatrix} ,
   \\ A_D & = \begin{pmatrix}
       -1 & 1
    \\ -1 &-1 & 1
    \\    & & -1 & 1
    \\    & & & \ddots & \ddots
    \\    & & & & -1 & 1
    \\    & & & & & -1
    \end{pmatrix} .
\end{align*}
The truly simplicial cone $\bar C_A$ corresponds to the matrix
  \[ \bar A_A = Q^T A_A , \]
where $Q\in \text{R}^{(d+1)\times d}$ is such that its columns form
an orthonormal basis of $L^\bot$.

Taking the negative inverse of the transpose, we see that the polar
cones $\bar C_A^\circ, {C}^\circ_{BC}, {C}^\circ_D$ (the polar cone
of $\bar C_A^\circ$ taken in the ambient space $L^\bot$) correspond
to the following matrices:
\begin{align*}
   \bar A_A^\circ & = Q^T \begin{pmatrix}
       0
    \\ 1 & 0
    \\ 1 & 1 & 0
    \\ \vdots & \dots & \ddots & \ddots
    \\ 1 & \dots & \dots & 1 & 0
    \end{pmatrix} ,
 & {A}^\circ_{BC} & = \begin{pmatrix}
       1
    \\ 1 & 1
    \\ \vdots & \dots & \ddots
    \\ 1 & \dots & \dots & 1
    \end{pmatrix} ,
\\ {A}^\circ_D & = \begin{pmatrix}
       1/2 & -1/2
    \\ 1/2 & 1/2
    \\ 1/2 & 1/2 & 1
    \\ 1/2 & 1/2 & 1 & 1
    \\ \vdots & \vdots & \vdots & \dots & \ddots
    \\ 1/2 & 1/2 & 1 & \dots & \dots & 1
    \end{pmatrix} .
\end{align*}

These matrices can be obtained through the function `weyl_matrix`.

### Examples: circular cones

Circular cones consist of all points that lie below a certain angle
towards a given (half-)line. Choosing this line to be the $d$th
coordinate axis, we define the $d$-dimensional circular cone of
angle $0<\alpha<\frac{\pi}{2}$ via
\begin{align*}
   \text{Circ}_d(\alpha) & := \{x\in\text{R}^d\mid x_d\geq \cos(\alpha)\|x\|\}
\\ & = \Big\{x\in\text{R}^d\mid x_d\geq 0 ,\; x_d^2\geq \sum_{k=1}^{d-1}
    \frac{x_k^2}{\tan(\alpha)^2} \Big\} ,
\end{align*}
where $\|\cdot\|$ denotes the Euclidean norm.  

The circular cones form a class of cones, which is interesting for both
theoretical and practical purposes. For theoretical purposes because
circular cones are known to satisfy certain extreme inequalities (see
[below](#inequs)), and are suspected to satisfy even more; and for
practical purposes because they, and products of them, provide a
reasonably large, yet accessible, class of cones which is easy to
analyze with respect to the intrinsic volumes.

A circular cone is self-dual iff the angle is $\pi/4$. These special
cones are called Lorentz cone or ice-cream cone.
We denote these by $\mathcal{L}^d=\text{Circ}_d(\pi/4)$.

### Examples: ellipsoidal cones {#ellips_cone}

Ellipsoidal cones generalize the class of circular cones;
they can be defined as full-dimensional linear images of Lorentz cones,
  \[ \big\{\text{ellipsoidal cones in }\text{R}^d\big\}
    = \big\{A\mathcal{L}^d\mid A\in\text{R}^{d\times d} \text{ invertible} \big\} . \]
Every $d$-dimensional ellipsoidal cone is isometric to a cone of the form
\begin{align*}
   \mathcal{E}_\alpha & = \text{diag}(\alpha_1,\ldots,\alpha_{d-1},1)\mathcal{L}^d
\\ & = \Big\{ x\in\text{R}^d\mid x_d\geq0,\;  x_d^2\geq\sum_{k=1}^{d-1}\frac{x_k^2}{\alpha_k^2}\Big\}
\end{align*}
with $\alpha_1\geq\dots\geq\alpha_{d-1}>0$, the semiaxes of the cone.
See the [appendix](#proof_ellips) for a proof of that fact.
The proof also shows how to compute the semiaxes: if
$-\lambda_1\leq\dots\leq-\lambda_{d-1}<0$ and $\mu>0$ denote the eigenvalues of
$AJA^T$, where $J=\text{diag}(-1,\ldots,-1,1)$,
then the semiaxes of $A$ are given by $\alpha_k=\sqrt{\lambda_k/\mu}$, $k=1,\ldots,d-1$.
This calculation is implemented in `ellips_semiax`.

Note that the circular cone $\text{Circ}_d(\alpha)$ has all semiaxes
equal to $\tan(\alpha)$. The semiaxes of the polar of $\mathcal{E}_\alpha$ are given by
$\frac{1}{\alpha_{d-1}}\geq\dots\geq\frac{1}{\alpha_1}>0$, as
\begin{align*}
   \mathcal{E}_\alpha^\circ & = (\text{diag}(\alpha_1,\ldots,\alpha_{d-1},1)\mathcal{L}^d)^\circ
  = -\text{diag}(\alpha_1,\ldots,\alpha_{d-1},1)^{-T}\mathcal{L}^d
\\ & = -\text{diag}(1/\alpha_1,\ldots,1/\alpha_{d-1},1)\mathcal{L}^d
   = -\mathcal{E}_{1/\alpha} .
\end{align*}

## Conic intrinsic volumes {#intro_intrvol}

The conic intrinsic volumes of a closed convex cone $C\subseteq\text{R}^d$
form a $(d+1)$-element vector,
  \[ v(C) = (v_0(C),\ldots,v_d(C)) \geq 0 ,\quad \sum_{k=0}^d v_k(C) = 1 . \]
Since this vector lies in the probability simplex, it defines a corresponding
categorical distribution on $\{0,\ldots,d\}$, which we will refer to as the
intrinsic volumes distribution.

If $C$ is a polyhedral cone, then the intrinsic volumes can be
characterized in the following way: letting $g\sim N(0,I_d)$ a Gaussian
vector (coordinates are iid standard normal) and denoting the projection map onto $C$
by $\Pi_C\colon\text{R}^d\to C$, $\Pi_C(z) = \text{argmin}\{\|x-z\|\mid x\in C\}$,
the $k$th intrinsic volume is given as the probability that $\Pi_C(g)$ lies in
the relative interior of a $k$-dimensional face of $C$,
  \[ v_k(C) = \text{Prob}\big\{\Pi_C(g)\in \text{relint}(F), F \text{ face of }C, \dim(F)=k\big\} . \]
This characterization holds exclusively for polyhedral cones.

Note that the above characterization also provides a simple way to sample from the intrinsic volumes
distribution: sample a Gaussian vector, project it onto the cone $C$,
determine the dimension of the face containing the projection in its relative interior.
This procedure is implemented in `polyh_rivols_gen` and `polyh_rivols_ineq`.

If $C=L$ is a linear subspace, then
  \[ v_k(L) = \begin{cases} 1 & \text{if } k=\dim(L) \\ 0 & \text{else} . \end{cases} \]
It can be shown that if $C$ is *not* a linear subspace, then
  \[ v_0(C)+v_2(C)+v_4(C)+\cdots = v_1(C)+v_3(C)+v_5(C)+\cdots = \tfrac{1}{2} . \]
This linear relation is the reason why in the computations in the `conivol`
functions the underlying closed convex cone is generally assumed to be not a linear
subspace.  

The intrinsic volumes of the polar cone are the same as those of the
primal cone, but the index gets reversed,
  \[ v_k(C^\circ) = v_{d-k}(C) . \]
The intrinsic volumes of a product of cones arise as the convolution of
the intrinsic volumes of its components,
  \[ v_k(C\times D) = \sum_{i+j=k} v_i(C) v_j(D) . \]
This convolution is conveniently implemented in `comp_ivols_product`.

From the natural orthogonal decomposition $C = L + \bar C$, where
$L=C\cap (-C)$ and $\bar C=\Pi_{L^\bot}(C)$, one can show that $v_k(C)=0$
if $k<\text{lin}(C)$ or $k>\text{dim}(C)$ and^[The strictly positive
inequalities actually require a proof in the nonpolyhedral case (and
without having log-concavity); one can deduce this from the kinematic
formula by projecting the cone on a uniformly random linear subspace
and looking at the volume of the (relative) boundary of the projection.]
  \[ v_k(C)=v_{k-\text{lin}(C)}(\bar C)>0 \quad\text{for}\quad
    \text{lin}(C)\leq k\leq\text{dim}(C) . \]

### Statistical dimension and variance

The first moment of intrinsic volumes distribution of a cone is called
its statistical dimension,
  \[ \delta(C) = \sum_{k=0}^d k v_k(C) . \]
Note that the statistical dimension coincides with the (linear) dimension
if $C$ is a linear subspace. It also generalizes some other properties,
such as $\delta(C^\circ)=d-\delta(C)$ and  $\delta(C\times D)=\delta(C)+\delta(D)$.

For later use we also define the variance of the cone via
  \[ \text{var}(C) = \sum_{k=0}^d (k-\delta(C))^2 v_k(C) . \]

### Examples: Weyl chambers of finite reflection groups

The intrinsic volumes of the Weyl chambers are easily given through their
generating function:
\begin{align*}
   \sum_{k=0}^d v_k(C_A)\, x^k & = \frac{1}{d!} \prod_{k=1}^d (x+k-1) ,
\\ \sum_{k=0}^d v_k(C_{BC})\, x^k & = \frac{1}{d!\,2^d}\prod_{k=1}^d (x+2k-1) ,
\\ \sum_{k=0}^d v_k(C_D)\, x^k & = \frac{1}{d!\,2^{d-1}}(x+d-1)\prod_{k=1}^{d-1} (x+2k-1) .
\end{align*}
Note that the intrinsic volumes of $C_A$ are the normalized
[Stirling numbers of the first kind](https://en.wikipedia.org/wiki/Stirling_numbers_of_the_first_kind).
These formulas are implemented in `weyl_ivols`.

### Examples: circular cones

The intrinsic volumes of circular cones are given by the following formulas:
  \[ v_k(\text{Circ}_d(\alpha)) = \begin{cases}
       \displaystyle\frac{\Gamma(\frac{d}{2})}{\Gamma(\frac{k+1}{2})\Gamma(\frac{d-k+1}{2})}
            \frac{p^{(k-1)/2} (1-p)^{(d-k-1)/2}}{2} & \text{if } 0<k<d
    \\[1mm] \displaystyle\frac{\Gamma(\frac{d}{2})^2}{\Gamma(\frac{d-1}{2})\Gamma(\frac{d+1}{2})}
            I_p(\tfrac{d}{2},\tfrac{1}{2}) & \text{if } k=d
    \\[1mm] \displaystyle\frac{\Gamma(\frac{d}{2})^2}{\Gamma(\frac{d-1}{2})\Gamma(\frac{d+1}{2})}
            I_{1-p}(\tfrac{d}{2},\tfrac{1}{2}) & \text{if } k=0
    \end{cases} \]
where $p=\sin^2(\alpha)$ and where $I_p(a,b)$ denotes the
[regularized incomplete beta function](https://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function),
that is, the cdf of the corresponding Beta distribution.
These formulas are implemented in `circ_ivols`.

Note that if $d$ is even, then the intrinsic volumes with odd indices form the
(half-)weights of a binomial distribution:
  \[ 2v_{2k+1}(\text{Circ}_d(\alpha)) = \binom{d/2-1}{k} p^k (1-p)^{d/2-1-k} . \]

## (Bivariate) chi-bar-squared distribution {#bivchibarsq}

For $k\geq0$ let $F_k$ denote the cumulative distribution function (cdf)
of the $\chi_k^2$ distribution, with $F_0(t)=1_{\geq0}(t)$.
The chi-bar-squared distribution with weight
vector $v=(v_0,\ldots,v_d)$, where $v\geq0$ and $\sum_{k=0}^dv_k=1$,
can be characterized through its cdf via
  \[ F_v(t) = \sum_{k=0}^d v_k F_k(t) . \]
In terms of random variables, one can think of sampling from the
chi-bar-squared distribution as a two-step procedure:

1. sample an index $k\in\{0,\ldots,d\}$ from the categorical distribution 
defined by the weight vector $v$,
2. sample an element from the $\chi_k^2$ distribution.

The bivariate chi-bar-squared distribution with weight vector $v$
can similarly be characterized through its cdf via
  \[ \Phi_v(s,t) = \sum_{k=0}^d v_k F_k(s) F_{d-k}(t) . \]
In terms of random variables, one can again think of sampling $(x,y)$
from the chi-bar-squared distribution as a two-step procedure:

1. sample an index $k\in\{0,\ldots,d\}$ according to the distribution
defined by the weight vector $v$,
2. sample an element $s$ from the $\chi_k^2$ distribution and,
independently, sample an element $t$ from the $\chi_{d-k}^2$ distribution.

This sampling procedure is useful for the understanding of the EM
algorithm in `estim_em` but for the relation to closed convex
cones the following sampling procedure of the chi-bar-squared
distribution is more intuitive:

1. sample $d$ iid $\chi_1^2$ distributed random variables, say,
$a_1,\ldots,a_d$,
2. sample an index $k\in\{0,\ldots,d\}$ according to the distribution
defined by the weight vector $v$,
3. take $s=a_1+\cdots+a_k$ and $t=a_{k+1}+\cdots+a_d$.


### Relation to closed convex cones {#bivchibars_cone}

The (bivariate) chi-bar-squared distribution corresponding to a closed
convex cone $C$ is of course the one arising from taking the conic
intrinsic volumes as weights; in terms of the cdf,
  \[ F_C(t) = \sum_{k=0}^d v_k(C) F_k(t) ,\qquad \Phi_C(s,t) = \sum_{k=0}^d v_k(C) F_k(s) F_{d-k}(t) . \]
A natural sampling for these distributions (without knowing the intrinsic volumes)
involves the projection
map $\Pi_C$. Denoting by $g\sim N(0,I_d)$ again a Gaussian vector, one can
show that the chi-bar-squared and the bivariate chi-bar-squared
distributions are given by the distributions of the random variables
  \[ \|\Pi_C(g)\|^2 \quad\text{and}\quad \big(\|\Pi_C(g)\|^2,\|\Pi_{C^\circ}(g)\|^2\big) . \]
The fact that these random variables have the (bivariate)
chi-bar-squared distribution with weights given by the conic intrinsic
volumes is one form of the Conic Steiner Theorem, see [@McCT14].

The moments of the continuous random variables $\|\Pi_C(g)\|^2,
\|\Pi_{C^\circ}(g)\|^2$ are closely related with those of the discrete
random variable defined by the intrinsic volumes. In fact, for the
first and second moment we obtain the following:
\begin{align*}
  \delta(C) & = \text{E}\big[\|\Pi_C(g)\|^2\big]
\\ & = d-\text{E}\big[\|\Pi_{C^\circ}(g)\|^2\big] ,
\\ \text{var}(C) & = \text{var}\big(\|\Pi_C(g)\|^2\big)-2\delta(C)
\\ & = \text{var}\big(\|\Pi_{C^\circ}(g)\|^2\big)-2(d-\delta(C)) .
\end{align*}
These formulas underlie the estimation that is implemented in `estim_statdim_var`.
See the [EM algorithm vignette](../doc/estim-conic-intrinsic-volumes-with-EM.html#start_EM)
for more details.

## Inequalities, known and conjectured {#inequs}

Finally, we collect inequalities, known and conjectured, that help our understanding
of the intrinsic volumes, and that have already or might potentially prove useful
in applications. In particular, the log-concavity inequalities are useful in the
estimation of intrinsic volumes from sampling data.

### Inequalities from (half-)tail functionals

The tail and half-tail functionals $t_k$ and $h_k$ are defined via
\begin{align*}
   t_k & = v_k + v_{k+1} + v_{k+2} + \dots ,
 & h_k & = 2(v_k + v_{k+2} + v_{k+4} + \dots) .
\end{align*}
Note that $t_k=(h_k+h_{k+1})/2$, so the following inequalities for $t_k$
follow directly from those for $h_k$.

The above two-functionals are monotonically increasing in both the indices and
in the arguments: let $C,D$ be cones with $C\subseteq D$,
\begin{align*}
   t_k(C) & \leq t_{k+1}(C) , & t_k(C) & \leq t_k(D) ,
 & h_k(C) & \leq h_{k+1}(C) , & h_k(C) & \leq h_k(D) .
\end{align*}

### Isoperimetric inequalities

Broadly speaking, (classical) isoperimetric inequalities compare surface area
and volume of an object with those of a "round" object (in quotes because
isoperimetric inequalities are known not only in Euclidean spaces but in
curved spaces as well). In the context of convex cones one can find the following
formulation: if $C\subseteq\text{R}^d$ is a closed convex cone and if the angle
$\alpha$ is such that $v_{d-1}(C)=v_{d-1}(\text{Circ}_d(\alpha))$ then
  \[ v_d(C)\leq v_d(\text{Circ}_d(\alpha)) . \]
Note that $v_d(C)=t_d(C)=h_d(C)/2$.
Makwoski and Scheuer [-@MS16] added to this inequality the following:
\begin{align*}
   v_{d-2}(C) & \geq v_{d-2}(\text{Circ}_d(\alpha)) ,
 & h_{d-2k+1}(C) & \geq h_{d-2k+1}(\text{Circ}_d(\alpha)) .
\end{align*}

The above inequalities are interesting, but more useful inequalities
would be those comparing, say, the variance (or entropy) of the intrinsic volumes
of a cone with those of the circular cone with the same statistical dimension.
Concretely, we conjecture the following: if $C\subseteq\text{R}^d$ is a closed
convex cone and if the angle $\beta$ is such that
$\delta(C)=\delta(\text{Circ}_d(\beta))=:\delta$ then
$\text{var}(C)\stackrel{?}{\leq} \text{var}(\text{Circ}_d(\beta))$.
Note that by the above relation between the intrinsic volumes distribution and
the chi-bar-squared distribution, we can equivalently state the conjecture as
  \[ \text{E}\big[ \|\Pi_C(g)\|^4 \big] \stackrel{?}{\leq}
    \text{E}\big[ \|\Pi_{\text{Circ}_d(\beta)}(g)\|^4 \big] . \]

### Log-concavity

Log-concavity, as the name suggests, means concavity of the logarithms.
In the context of intrinsic volumes this means $\log v_k(C) \stackrel{?}{\geq}
\frac{\log v_{k-1}(C)+\log v_{k+1}(C)}{2}$, equivalently,
   \[ v_k(C)^2 \stackrel{?}{\geq} v_{k-1}(C) v_{k+1}(C) . \]
These inequalities imply unimodality, and are in general useful for estimating
intrinsic volumes. Unfortunately, these inequalities are so far just conjectured
for general cones; they are true for circular cones, Weyl chambers, and direct
products of cones for which they are known to hold.
In the euclidean case the log-concavity of the euclidean intrinsic volumes
follows from the [Alexandrov-Fenchel inequality](https://en.wikipedia.org/wiki/Mixed_volume),
and log-concavity of conic intrinsic volumes directly implies log-concavity
of euclidean intrinsic volumes.
In the other direction it can be shown that log-concavity of conic intrinsic volumes
hold for dimension $d\leq5$ using the Alexandrov-Fenchel inequality, but for
general dimensions it seems that this inequality is not enough.

## Appendix: Proof of ellipsoidal cone representation {#proof_ellips}

Let $C=A\mathcal L^d$ for some invertible
linear transformation $A$. Consider the eigenvalue decomposition of
the symmetric matrix $AJA^T$ with $J=\text{diag}(-1,\ldots,-1,1)$:
according to [Sylvester's law of inertia](https://en.wikipedia.org/wiki/Singular-value_decomposition)
this matrix has $d-1$ negative and one positive eigenvalues. Let these
be given by $-\lambda_1<\dots<-\lambda_{d-1}<0$ and $\mu>0$,
so that
  \[ AJA^T = Q\text{diag}(-\lambda_1,\ldots,-\lambda_{d-1},\mu)Q^T , \]
for $Q\in\text{R}^d$ orthogonal; taking inverses, we have
  \[ A^{-T}JA^{-1} = Q\text{diag}(-1/\lambda_1,\ldots,-1/\lambda_{d-1},1/\mu)Q^T . \]
Setting $\alpha_k=\sqrt{\lambda_k/\mu}$ for $k=1,\ldots,d-1$, we obtain
\begin{align*}
   \mu J & = A^TQ\text{diag}(-\mu/\lambda_1,\ldots,-\mu/\lambda_{d-1},1)Q^TA
\\ & = A^TQ\text{diag}(\alpha_1^{-1},\ldots,\alpha_{d-1}^{-1},1) J \text{diag}(\alpha_1^{-1},\ldots,\alpha_{d-1}^{-1},1)Q^TA
\end{align*}
According to [@LS75] this implies that either $\text{diag}(\alpha_1^{-1},
\ldots,\alpha_{d-1}^{-1},1)Q^TA\mathcal L^d=\mathcal L^d$ or $\text{diag}(\alpha_1^{-1},\ldots,\alpha_{d-1}^{-1},1)Q^TA\mathcal L^d
=-\mathcal L^d$.
Denoting $\mathcal{E}_\alpha=\text{diag}(\alpha_1,\ldots,\alpha_{d-1},1)\mathcal L^d$,
we see that either $A\mathcal L^d=Q\mathcal{E}_\alpha$ or
$A\mathcal L^d=-Q\mathcal{E}_\alpha$.
In both cases we see that $A\mathcal L^d$ is isometric to $\mathcal{E}_\alpha$.

## References
