<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Dennis Amelunxen" />

<meta name="date" content="2017-08-01" />

<title>Estimating conic intrinsic volumes via EM algorithm</title>






<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Estimating conic intrinsic volumes via EM algorithm</h1>
<h4 class="author"><em>Dennis Amelunxen</em></h4>
<h4 class="date"><em>2017-08-01</em></h4>



<p>This vignette describes the details of the algorithm to find the intrinsic volumes of closed convex cones from samples from the associated bivariate chi-bar-squared distribution. A general knowledge of the theory of conic intrinsic volumes is assumed, see <a href="conic-intrinsic-volumes.html">this vignette</a> for a short introduction.</p>
<div id="the-setup" class="section level2">
<h2>The setup</h2>
<p>We use the following notation:</p>
<ul>
<li><span class="math inline">\(C\subseteq\text{R}^d\)</span> denotes a closed convex cone, <span class="math inline">\(C^\circ\)</span> the polar cone, and <span class="math inline">\(\Pi_C\colon\text{R}^d\to C\)</span> denotes the orthogonal projection map, <span class="math display">\[\Pi_C(z) = \text{argmin}\{\|x-z\|\mid x\in C\}.\]</span> We will assume in the following that both <span class="math inline">\(C\)</span> and <span class="math inline">\(C^\circ\)</span> have nonempty interior, or equivalently, <span class="math inline">\(C^\circ\)</span> and <span class="math inline">\(C\)</span> both do not contain a nonzero linear subspace. These linear algebraic conditions are easily checked/established so there is no serious restriction in assuming this.<br />
In particular, we assume that <span class="math inline">\(C\)</span> is not a linear subspace so that the intrinsic volumes with even and with odd indices each add up to <span class="math inline">\(\frac12\)</span>.</li>
<li><span class="math inline">\(v = v(C) = (v_0(C),\ldots,v_d(C))\)</span> denotes the vector of intrinsic volumes. Under the (primal and polar) nonempty interior assumption we have <span class="math inline">\(v_k&gt;0\)</span> for all <span class="math inline">\(k=0,\ldots,d\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>We work with the two main random variables <span class="math display">\[ X=\|\Pi_C(g)\|^2 ,\quad Y=\|\Pi_{C^Â°}(g)\|^2, \]</span> where <span class="math inline">\(g\sim N(0,I_d)\)</span>. So <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are chi-bar-squared distributed with reference cones <span class="math inline">\(C\)</span> and <span class="math inline">\(C^\circ\)</span>, respectively, and the pair <span class="math inline">\((X,Y)\)</span> distributed according to the bivariate chi-bar-squared distribution with reference cone <span class="math inline">\(C\)</span>.</li>
<li>For later use in the EM algorithm we also define the latent variable <span class="math inline">\(Z\in\{0,1,\ldots,d\}\)</span>, <span class="math inline">\(\text{Prob}\{Z=k\}=v_k\)</span>.</li>
</ul>
<p>Concretely, if <span class="math inline">\(C\)</span> is a polyhedral cone then we define <span class="math inline">\(Z\)</span> as the dimension of the face of <span class="math inline">\(C\)</span> such that <span class="math inline">\(\Pi_C(g)\)</span> lies in its relative interior. This is well-defined, as <span class="math inline">\(C\)</span> decomposes into a disjoint union of the relative interiors of its faces. We may assume without loss of generality that the underlying cone is polyhedral, as any closed convex cone can be arbitrarily well approximated by polyhedral cones.</p>
<p>Conditioning <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> on <span class="math inline">\(Z\)</span> yields independent random variables <span class="math inline">\(X\mid Z\)</span> and <span class="math inline">\(Y\mid Z\)</span>, which are <span class="math inline">\(\chi^2\)</span>-distributed with <span class="math inline">\(Z\)</span> and <span class="math inline">\(d-Z\)</span> degrees of freedom, respectively. Here and in the following we use the convention that <span class="math inline">\(\chi_0^2\)</span> denotes the Dirac measure supported in <span class="math inline">\(0\)</span>.</p>
<p>We assume to have sampled data: <span class="math inline">\((X_1,Y_1),\ldots,(X_N,Y_N)\)</span> denote iid copies of <span class="math inline">\((X,Y)\)</span> and we assume that they took the sample values: <span class="math inline">\((x_1,y_1),\ldots,(x_N,y_N)\)</span>. The general goal is to reconstruct <span class="math inline">\(v\)</span> from these sample values.</p>
</div>
<div id="choosing-a-starting-point" class="section level2">
<h2>Choosing a starting point</h2>
<p>We need to find a starting point for the iterative algorithms to reconstruct <span class="math inline">\(v\)</span>. This can of course be the uniform distribution on <span class="math inline">\(\{0,\ldots,d\}\)</span>, but we may as well use a more elaborated first guess. The first and second moment of <span class="math inline">\(v\)</span>, more precisely, of the latent variable <span class="math inline">\(Z\)</span>, can be conveniently estimated from the sample data, as <span class="math display">\[ \delta := \sum_{k=0}^d k v_k(C) = \text{E}[X] = d-\text{E}[Y] , \]</span> and <span class="math display">\[ \text{var} := \sum_{k=0}^d (k-\delta)^2 v_k(C) = \text{E}[X^2]-(\delta+1)^2+1 = \text{E}[Y^2]-(d-\delta+1)^2+1 . \]</span> The sample data of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> thus yield two natural estimates for both <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\text{var}\)</span>, which we combine as follows: <span class="math display">\[ \left.\begin{array}{c}
   \displaystyle\hat\delta_{\text{prim}} = \frac{1}{N}\sum_{i=1}^N x_i
\\ \displaystyle\hat\delta_{\text{pol}} = d-\frac{1}{N}\sum_{i=1}^N y_i
   \end{array} \right\} \quad\hat\delta := \frac{\hat\delta_{\text{prim}}+\hat\delta_{\text{pol}}}{2} , \]</span> <span class="math display">\[ \left.\begin{array}{c}
   \displaystyle\widehat{\text{var}}_{\text{prim}} = \frac{1}{N}\sum_{i=1}^N x_i^2 - (\hat\delta+1)^2+1
\\ \displaystyle\widehat{\text{var}}_{\text{pol}} = \frac{1}{N}\sum_{i=1}^N y_i^2 - (d-\hat\delta+1)^2+1
   \end{array} \right\} \quad \widehat{\text{var}} = \sqrt{ \widehat{\text{var}}_{\text{prim}} \widehat{\text{var}}_{\text{pol}} } . \]</span> These formulas are implemented in <code>conivol::estimate_statdim_var</code>.</p>
<p>We propose the following ways to choose a starting point for the iterative algorithms to find the intrinsic volumes:</p>
<ol start="0" style="list-style-type: decimal">
<li><em>uniform distribution:</em> <span class="math inline">\(v^{(0)}=\frac{1}{d+1}(1,\ldots,1)\)</span>.</li>
<li><em>normal distribution:</em> match a normal distribution to the first and second moments of <span class="math inline">\(Z\)</span> and discretize: <span class="math display">\[ v^{(0)}_k = \frac{\text{Prob}\{k-0.5\leq \nu\leq k+0.5\}}{\text{Prob}\{-0.5\leq \nu\leq d+0.5\}} \]</span> where <span class="math inline">\(\nu\sim N(\hat\delta,\widehat{\text{var}})\)</span>.</li>
<li><em>circular cones</em>: <span class="math inline">\(d\)</span>-dimensional cones of angle <span class="math inline">\(\alpha\)</span> have an approximate statistical dimension of <span class="math inline">\(d\sin^2\alpha\)</span> and an approximate variance of <span class="math inline">\((d/2-1)\sin^2(2\alpha)\)</span>. We can thus choose <span class="math inline">\(\alpha\)</span> to match either the statistical dimension, or the variance, or take an average of both fits:
<ol style="list-style-type: lower-alpha">
<li><em>match first moment</em>: take <span class="math inline">\(\alpha=\text{arcsin}\sqrt{\hat\delta/d}\)</span> and take <span class="math inline">\(v^{(0)}\)</span> to be the intrinsic volumes of a <span class="math inline">\(d\)</span>-dimensional circlar cone with an angle of <span class="math inline">\(\alpha\)</span>.</li>
<li><em>match second moment</em>: if <span class="math inline">\(\hat\delta&lt;d/2\)</span> then take <span class="math inline">\(\beta=\frac{1}{2}\text{arcsin}\sqrt{\frac{2\widehat{\text{var}}}{d-2}}\)</span> else take <span class="math inline">\(\beta=\frac{\pi}{2}-\frac{1}{2}\text{arcsin}\sqrt{\frac{2\widehat{\text{var}}}{d-2}}\)</span> and take <span class="math inline">\(v^{(0)}\)</span> to be the intrinsic volumes of a <span class="math inline">\(d\)</span>-dimensional circlar cone with an angle of <span class="math inline">\(\beta\)</span>.</li>
<li><em>average both fits</em>: take <span class="math inline">\(v^{(0)}\)</span> as the geometric mean of the intrinsic volumes obtained in (a) and (b).</li>
</ol></li>
</ol>
<p>These formulas are implemented in <code>conivol::init_v</code>.</p>
</div>
<div id="the-likelihood-function" class="section level2">
<h2>The likelihood function</h2>
<p>The likelihood function is straightforward to derive, but one has to be cautious about the âedge casesâ, i.e., the cases where <span class="math inline">\(g\in C\cup C^\circ\)</span>. Notice that this event is visible in the sample data, as <span class="math display">\[ g\in C \iff Y=0 ,\qquad g\in C^\circ \iff X=0 . \]</span> Moreover, if <span class="math inline">\(Y=0\)</span>, then the value of <span class="math inline">\(X\)</span> is just a draw from the <span class="math inline">\(\chi_d^2\)</span> distribution, and similarly for <span class="math inline">\(X=0\)</span>. This shows that those sample values <span class="math inline">\((x_i,y_i)\)</span> with <span class="math inline">\(x_i=0\)</span> or <span class="math inline">\(y_i=0\)</span> are just noise and should be discarded. The non-noisy part is of course the <em>proportion</em> of points with <span class="math inline">\(Y=0\)</span> and <span class="math inline">\(X=0\)</span>, which form point estimates for <span class="math inline">\(v_d(C)\)</span> and <span class="math inline">\(v_0(C)\)</span>, respectively. The data on which we base our estimation thus has the following form:</p>
<ul>
<li><em>general data</em>: dimension <span class="math inline">\(d\)</span> and sample size <span class="math inline">\(N\)</span></li>
<li><em>estimate of <span class="math inline">\(v_d\)</span></em>: <span class="math inline">\(p = |\{i\mid y_i=0\}|/N\)</span></li>
<li><em>estimate of <span class="math inline">\(v_0\)</span></em>: <span class="math inline">\(q = |\{i\mid x_i=0\}|/N\)</span></li>
<li><em>remaining samples</em>: without loss of generality we may use the notation <span class="math inline">\((x_1,y_1),\ldots,(x_{\bar N},y_{\bar N})\)</span> for the remaining sample points, <span class="math inline">\(\bar N\leq N\)</span>, or shortly <span class="math inline">\(\mathbf{x},\mathbf{y}\)</span> for the sample vectors.</li>
</ul>
<p>In fact, it turns out that the sample data enters the computation only through the density values <span class="math display">\[ f_{ik} := f_k(x_i)f_{d-k}(y_i) ,\quad k=1,\ldots,d-1,\; i=1,\ldots,\bar N , \]</span> where <span class="math inline">\(f_k(t)\)</span> denotes the density of the chi-squared distribution, <span class="math display">\[ f_k(t)=\frac{1}{2^{k/2}\Gamma(k/2)}t^{k/2-1}e^{-t/2} . \]</span> The function <code>conivol::prepare_data</code> evaluates the sample data in this regard; calling this function outside the EM algorithm allows to skip this step on multiple evaluations of the EM algorithm or related functions.</p>
<p>We obtain the following formula for the likelihood of the given data: <span class="math display">\[ L(v\mid \mathbf{x},\mathbf{y}) = v_0^{|\{i\mid x_i=0\}|} v_d^{|\{i\mid y_i=0\}|} \prod_{i=1}^{\bar N}\sum_{k=1}^{d-1} v_k f_{ik} . \]</span> This likelihood function does not take the latent variable <span class="math inline">\(Z\)</span> into account, which is crucial for the EM algorithm. Assuming that we have sample values <span class="math inline">\(\mathbf{z}=(z_1,\ldots,z_{\bar N})\)</span> for the latent variables, we arrive at the likelihood function <span class="math display">\[ L(v\mid \mathbf{x},\mathbf{y},\mathbf{z}) = v_0^{|\{i\mid x_i=0\}|} v_d^{|\{i\mid y_i=0\}|} \prod_{i=1}^{\bar N}\prod_{k=1}^{d-1} (v_k f_{ik})^{(z_i=k)} , \]</span> where <span class="math inline">\((z_i=k)=1\)</span> if <span class="math inline">\(z_i=k\)</span> and zero else.</p>
<p>For numerical reasons we work with the rescaled log-likelihood functions <span class="math display">\[\begin{align*}
\frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y}) &amp; = q \log v_0 + p \log v_d + \frac{1}{N}\sum_{i=1}^{\bar N} \log\Big(\sum_{k=1}^{d-1} v_k f_{ik}\Big) ,
\\ \frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y},\mathbf{z}) &amp; = q \log v_0 + p \log v_d + \frac{1}{N}\sum_{i=1}^{\bar N} \sum_{k=1}^{d-1} (z_i=k) \big( \log v_k + \log f_{ik} \big) .
\end{align*}\]</span> The rescaled log-likelihood function <span class="math inline">\(\frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y})\)</span> is implemented in <code>conivol::comp_loglike</code>.</p>
<div id="log-concavity" class="section level3">
<h3>Log-concavity</h3>
<p>The log-concavity inequalities are the inequalities <span class="math inline">\(v_k^2\geq v_{k-1}v_{k+1}\)</span> for <span class="math inline">\(k=1,\ldots,d-1\)</span>, or equivalently, <span class="math display">\[ 2\log v_k - \log v_{k-1} - \log v_{k+1} \geq 0 . \]</span> At this moment, the validity of these inequalities is an open conjecture for general closed convex cones. But for subclasses, like products of circular cones, they are known to hold, and there is some evidence that they hold in general.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> Assuming log-concavity of the conic intrinsic volumes greatly helps the EM algorithm to converge to a sensible estimate.</p>
<p>There are two natural ways in which log-concavity may be exploited:</p>
<ol style="list-style-type: decimal">
<li>soft enforcement through a penalty term in the log-likelihood functions: <span class="math display">\[ F_\lambda(v) := \frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y}) + \sum_{k=1}^{d-1} \lambda_k (2\log v_k - \log v_{k-1} - \log v_{k+1})\]</span> and <span class="math display">\[ G_\lambda(v,\mathbf{z}) := \frac{1}{N} \log L(v\mid \mathbf{x},\mathbf{y},\mathbf{z}) + \sum_{k=1}^{d-1} \lambda_k (2\log v_k - \log v_{k-1} - \log v_{k+1}) , \]</span> where <span class="math inline">\(\lambda_1,\ldots,\lambda_{d-1}\geq0\)</span> and <span class="math inline">\(\lambda_0:=\lambda_d:=0\)</span> (setting all <span class="math inline">\(\lambda_k\)</span> to zero yields the original log-likelihood functions).<br />
It should be noted that these âpenalty termsâ may cancel out, effectively rendering them useless. A true log-concavity penalty would have a form like <span class="math inline">\(\min\{0,2\log v_k - \log v_{k-1} - \log v_{k+1}\}\)</span>; we use the simple (albeit potentially useless) penalty as it allows for easy implementation in the EM step.</li>
<li>projecting the logarithms of the iterates onto the polyhedral set <span class="math display">\[ \big\{ u\in\text{R}^{d+1}\mid 2 u_k - u_{k-1} - u_{k+1} \geq c_k \big\} . \]</span> for some <span class="math inline">\(c_1,\ldots,c_{d-1} (\geq0)\)</span> (setting all <span class="math inline">\(c_k\)</span> to <span class="math inline">\(-\infty\)</span> eliminates this restriction).</li>
</ol>
<p>Both of these methods are supported in <code>conivol::find_ivols_EM</code>.</p>
</div>
</div>
<div id="expectation-maximization-em-algorithm" class="section level2">
<h2>Expectation maximization (EM) algorithm</h2>
<p>The expectation maximization (EM) algorithm works by maximizing the conditional expectation with respect to the latent variable of the maximum likelihood method, given the current iterate for the parameter that is to be found. In the situation under review, we seek to maximize the function <span class="math display">\[\begin{align}
   &amp; \underset{\mathbf{Z}\mid \mathbf{x},\mathbf{y},v^{(t)}}{\text{E}} \big[\tfrac{1}{N}\log L(v\mid\mathbf{x},\mathbf{y},\mathbf{Z})\big]
\\ &amp; = q \log v_0 + p \log v_d + \frac{1}{N}\sum_{i=1}^{\bar N} \sum_{k=1}^{d-1} \frac{v_k^{(t)}f_{ik}}{\sum_{j=1}^{d-1} v_j^{(t)}f_{ij}} \big( \log v_k + \log f_k(x_i) + \log f_{d-k}(y_i) \big)
\end{align}\]</span> as a function in <span class="math inline">\(v\)</span>. Apparently, the final summands can be dropped as they only contribute as constants. Furthermore, replacing the log-likelihood function by <span class="math inline">\(G_\lambda(v,\mathbf{z})\)</span> to take the log-concavity inequalities into account, we see that the next iterate <span class="math inline">\(v^{(t+1)}\)</span> is found by solving the optimization problem <span class="math display">\[ \text{maximize}\quad (q-\lambda_1) \log v_0 + (p-\lambda_{d+1}) \log v_d + \sum_{k=1}^{d-1} \frac{1}{N}\sum_{i=1}^{\bar N} \Bigg( \frac{v_k^{(t)}f_{ik}}{\sum_{j=1}^{d-1} v_j^{(t)}f_{ij}} + 2\lambda_k-\lambda_{k-1}-\lambda_{k+1}\Bigg) \log v_k \]</span> subject to <span class="math inline">\(v_0,\ldots,v_d\geq0\)</span>, <span class="math inline">\(v_0+v_2+\dots=v_1+v_3+\dots=\frac12\)</span>. As long as the coefficients of <span class="math inline">\(\log v_k\)</span> are nonnegative, this is a convex problem and, as a <em>separable convex program</em>, can be easily solved by MOSEK.</p>
<p>As a side note, the implementation of this method in <code>conivol::find_ivols_EM</code> is such that MOSEK is called with the user-defined values for <span class="math inline">\(\lambda\)</span>, but then reduced if the program turns out to be nonconvex so that MOSEK will return an error, and eventually dropped (in which case the program becomes convex and is solved by MOSEK). It thus could happen that although the penalty terms <span class="math inline">\(\lambda\)</span> have positive values, they are being ignored by the algorithm.</p>
</div>
<div id="enforcing-log-concavity" class="section level2">
<h2>Enforcing log-concavity</h2>
<p>The log-concavity inequalities <span class="math inline">\(2\log v_k-\log v_{k-1}-\log v_{k+1}\geq c_k\;(\geq0)\)</span> could be modelled as constraints in the above separable optimization problem. But including these will violate the convexity assumptions right away, so the current implementation of the algorithm does not proceed in this way. Instead, after computing the iterate in the normal way, the algorithm will project the vector of the logarithms onto the polyhedral set <span class="math inline">\(\{ u\in\text{R}^{d+1}\mid 2 u_k - u_{k-1} - u_{k+1} \geq c_k \}\)</span> and then rescale the resulting vector so that <span class="math inline">\(v_0^{(t+1)}+v_2^{(t+1)}+\dots=v_1^{(t+1)}+v_3^{(t+1)}+\dots=\frac12\)</span>.</p>
</div>
<div id="a-moderate-sized-example" class="section level2">
<h2>A moderate-sized example</h2>
<p>(TBD) (Martinâs example)</p>
</div>
<div id="appendix-a-socp-formulation-of-projection-maps-and-mosek-implementation" class="section level2">
<h2>Appendix A: SOCP formulation of projection maps and MOSEK implementation</h2>
<p>The problem of finding the projection of a point <span class="math inline">\(z\in\text{R}^d\)</span> onto a polyhedral cone <span class="math inline">\(\{Au\mid u\geq0\}\)</span>, <span class="math inline">\(A\in\text{R}^{d\times n}\)</span>, also known as <em>nonnegative least squares</em>, can be formulated as a second order cone program (SOCP): <span class="math display">\[\begin{align}
   &amp; \underset{u\geq0}{\text{argmin}}\, \|Au-z\|_2
\\ \iff &amp; \text{minimize } \tfrac12 \|Au\|^2 - z^T A u \quad\text{subject to}\quad u\geq0
\\ \iff &amp; \text{minimize } (-A^Tz)^Tu + r \quad\text{subject to}\quad u\geq0, Ax=w, s=1, \|w\|^2\leq 2rs .
\end{align}\]</span> This last formulation can be solved by MOSEK, using the rotated quadratic cone constraint (see the <a href="http://rmosek.r-forge.r-project.org/">Rmosek userguide</a>).</p>
<p>This is the approach taken in the function <code>conivol::rbichibarsq_polyh</code>. (STILL NEED TO COMPLETE DIMENSION REDUCTION THERE..)</p>
The projection of a point <span class="math inline">\(z\in\text{R}^n\)</span> onto the polar cone, and more generally, the projection onto the polyhedral set <span class="math inline">\(\{y\mid A^Ty\leq c\}\)</span>, <span class="math inline">\(A\in\text{R}^{d\times n}\)</span>, can be formulated as SOCP in a similar way:
<span class="math display">\[\begin{align*}
   &amp; \underset{A^Ty\leq c}{\text{argmin}}\, \|y-z\|_2
\\ \iff &amp; \text{minimize } \tfrac12\|y\|^2 - z^Ty \quad\text{subject to}\quad A^Ty\leq c
\\ \iff &amp; \text{minimize } (-z)^Ty+r \quad\text{subject to}\quad u\geq 0, A^Ty+u=c, s=1, \|y\|^2\leq 2rs
\end{align*}\]</span>
<p>This last formulation is solved with MOSEK, using the rotated quadratic cone constraint, to enforce the log-concavity constraints in the EM algorithm.</p>
<div id="appendix-b-gradient-and-hessian" class="section level3">
<h3>Appendix B: Gradient and Hessian</h3>
<p>Gradient descent and Newtonâs method both seem to be inappropriate for the task of reconstructing the intrinsic volumes from sample data as described above. However, analyzing the gradient and the Hessian might still be instructive to decide whether the maximum likelihood estimate has been reached.</p>
<p>We compute the gradient and Hessian in the affine-linear subspace <span class="math display">\[ V:=\big\{v\in\text{R}^{d+1}\mid v_0 + v_2 + \dots = v_1 + v_3 + \dots = \tfrac12\big\} . \]</span> For this we change coordinates in the following way: let <span class="math inline">\(k_0,k_1\in\{1,\ldots,d-1\}\)</span>, <span class="math inline">\(k_0\)</span> even, <span class="math inline">\(k_1\)</span> odd, and parametrize the neighborhood of the vector <span class="math inline">\(v\)</span> through <span class="math display">\[ v + \sum_{k\text{ even}} a_k (e_k-e_{k_0}) + \sum_{k\text{ odd}} a_k (e_k-e_{k_1}) . \]</span> Changing coordinates in <span class="math inline">\(F_\lambda\)</span> by defining <span class="math display">\[ \tilde F_\lambda(a) := F_\lambda \Big( v + \sum_{k\text{ even}} a_k (e_k-e_{k_0}) + \sum_{k\text{ odd}} a_k (e_k-e_{k_1})\Big) , \]</span> one obtains the following gradient in zero: <span class="math display">\[\begin{align}
  \nabla \tilde F_\lambda(0) &amp; = -\frac{1}{N} \sum_{i=1}^{\bar N} \frac{ f_{ik_0}\mathbf1_{\text{even}} + f_{ik_1}\mathbf1_{\text{odd}} - f_i}{ \sum_{k=1}^{d-1} v_kf_{ik} } + \text{diag}(v)^{-1} \begin{pmatrix} q-\lambda_1 \\ 2\lambda-\lambda_{-1}-\lambda_{+1} \\ p-\lambda_{d-1} \end{pmatrix}
\\[2mm] &amp; \quad - \frac{2\lambda_{k_0}-\lambda_{k_0-1}-\lambda_{k_0+1}}{v_{k_0}} \mathbf1_{\text{even}} - \frac{2\lambda_{k_1}-\lambda_{k_1-1}-\lambda_{k_1+1}}{v_{k_1}} \mathbf1_{\text{odd}} ,
\end{align}\]</span> where</p>
<ul>
<li><span class="math inline">\(f_i = (0,f_{i1},\ldots,f_{i,d-1},0)^T\)</span>,</li>
<li><span class="math inline">\(\mathbf1_{\text{even}} = (1, 0, 1, 0, 1, \ldots)^T\)</span>, <span class="math inline">\(\mathbf1_{\text{odd}} = (0, 1, 0, 1, 0, 1, \ldots)^T\)</span>,</li>
<li><span class="math inline">\(\lambda=(\lambda_1,\ldots,\lambda_{d-1})^T\)</span>, <span class="math inline">\(\lambda_{-1}=(1,\lambda_1,\ldots,\lambda_{d-2})^T\)</span>, <span class="math inline">\(\lambda_{+1}=(\lambda_2,\ldots,\lambda_{d-1},1)^T\)</span></li>
</ul>
<p>Note that <span class="math inline">\(\tilde F_\lambda\)</span> is constant in <span class="math inline">\(a_{k_0}\)</span> and <span class="math inline">\(a_{k_1}\)</span>, and the gradient is correspondingly zero in these points; the gradient of <span class="math inline">\(F_\lambda\)</span> projected to the affine subspace <span class="math inline">\(V\)</span> is thus (up to rescaling) given by <span class="math display">\[ \bar\nabla F_\lambda(v)_k = \begin{cases}
                          \nabla\tilde F_\lambda(0)_k &amp; \text{if } k\not\in\{k_0.k_1\}
                       \\ -\sum_{j\text{ even}} \nabla\tilde F_\lambda(0)_j &amp; \text{if } k=k_0
                       \\ -\sum_{j\text{ odd}} \nabla\tilde F_\lambda(0)_j &amp; \text{if } k=k_1
                       \end{cases} \]</span> The Hessian of <span class="math inline">\(\tilde F_\lambda\)</span> in zero is given by <span class="math display">\[\begin{align}
  H\tilde F_\lambda(0) &amp; = -\frac{1}{N} \sum_{i=1}^{\bar N} \frac{ \big(f_{ik_0}\mathbf1_{\text{even}} + f_{ik_1}\mathbf1_{\text{odd}} - f_i \big) \big(f_{ik_0}\mathbf1_{\text{even}} + f_{ik_1}\mathbf1_{\text{odd}} - f_i \big)^T}{ \big(\sum_{k=1}^{d-1} v_kf_{ik}\big)^2 }
\\ &amp; \qquad - \text{diag}(v)^{-2}\text{diag} \begin{pmatrix} q-\lambda_1 \\ 2\lambda_j-\lambda_{j-1}-\lambda_{j+1} \\ p-\lambda_{d-1} \end{pmatrix} - \frac{2\lambda_{k_0}-\lambda_{k_0-1}-\lambda_{k_0+1}}{v_{k_0}^2} \mathbf1_{\text{even}}\mathbf1_{\text{even}}^T
\\ &amp; \qquad - \frac{2\lambda_{k_1}-\lambda_{k_1-1}-\lambda_{k_1+1}}{v_{k_1}^2} \mathbf1_{\text{odd}}\mathbf1_{\text{odd}}^T
\end{align}\]</span></p>
<!-- These formulas are implemented in `conivol::find_ivols_gradient` and `conivol::find_ivols_Hessian`. -->
<!-- ## Finding a good example with circular cones -->
<!-- ```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE} -->
<!-- ### loading some packages -->
<!-- library(conivol) -->
<!-- library(tidyverse) -->
<!-- library(Rmosek) -->
<!-- ``` -->
<!-- Ok, so the main function is of course `bichibarsq_find_weights`. Note that a simple way of estimating the intrinsic volumes is to estimate the statistical dimension and the variance, and then fit (and discretize) a normal distribution. This works very well for some cones (for example the nonnegative orthant), so we should find an example where this simple strategy does not work well. Another strategy is to find the circular cone that fits the statistical dimension. We should also avoid examples where this strategy gives a too good starting point. -->
<!-- Let's try a product of two cones such that the statistical dimension is comparably large: -->
<!--   $$C=\text{Circ}_{d_1}(\alpha_1)\times \text{Circ}_{d_2}(\alpha_2)$$ -->
<!-- with $d_1=5, d_2=8, \alpha_1=\text{arcsin}(\sqrt{0.9}), \alpha_2=\text{arcsin}(\sqrt{0.8})$: -->
<!-- ```{r} -->
<!-- D <- c(5,8) -->
<!-- alpha <- c( asin(sqrt(0.9)) , asin(sqrt(0.8))) -->
<!-- v <- circ_ivol(D, alpha, product = TRUE) -->
<!-- ``` -->
<!-- We compute the statistical dimension and variance and fit-and-discretize a normal distribution: -->
<!-- ```{r} -->
<!-- d <- sum(D) -->
<!-- delta <- sum(v*(0:d)) -->
<!-- var <- sum(v*(0:d)^2) - delta^2 -->
<!-- vn <- sapply( 0:d,  -->
<!--      function(k){ pnorm((k+0.5-delta)/sqrt(var)) - pnorm((k-0.5-delta)/sqrt(var)) }) -->
<!-- vn <- vn/sum(vn) -->
<!-- ``` -->
<!-- We find the circular cone, which matches the statistical dimension: -->
<!-- ```{r} -->
<!-- search_alpha <- pi/2*(1:999)/1000 -->
<!-- ivols <- circ_ivol(rep(d,length(search_alpha)),search_alpha) -->
<!-- i_fit <- which.min( ( sapply(ivols, function(v){sum(v*(0:d))}) - delta )^2 ) -->
<!-- alpha_fit <- search_alpha[i_fit] -->
<!-- vc <- circ_ivol(d, alpha_fit) -->
<!-- ``` -->
<!-- Let's have a look at the corresponding plots (normal and log): -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- tib_plot     <- gather(tibble(k=0:d,exact=v,     est_norm=vn,     est_circ=vc),    type,value,2:4) -->
<!-- tib_plot_log <- gather(tibble(k=0:d,exact=log(v),est_norm=log(vn),est_circ=log(vc)),type,value,2:4) -->
<!-- ggplot(tib_plot, aes(x=k, y=value, color=type)) + -->
<!--     geom_line() + -->
<!--     scale_colour_manual(values=c("red","blue","black")) + -->
<!--     theme(legend.position="bottom", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) -->
<!-- ggplot(tib_plot_log, aes(x=k, y=value, color=type)) + -->
<!--     geom_line() + -->
<!--     scale_colour_manual(values=c("red","blue","black")) + -->
<!--     theme(legend.position="bottom", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) -->
<!-- ``` -->
<!-- Ok, that example seems reasonable. We will choose this example for the next computations. -->
<!-- ## Estimating the conic weights from projection data -->
<!-- First, let's sample from the bivariate chi-bar-squared distribution. Internally, this will be done by sampling with the exact values of the intrinsic volumes, because the conic intrinsic volumes of products of circular cones are easy to calculate and because this seems to be the fastest way to sample from this distribution. But the idea is of course that these samples can be obtained by computing the squared norms of the projections of random Gaussian vectors onto the cone and its polar. -->
<!-- ```{r, fig.align='center', fig.width=7, fig.height=6, eval=TRUE} -->
<!-- n <- 10^5 -->
<!-- set.seed(1234) -->
<!-- m_samp <- rbichibarsq_circ(n,D,alpha) -->
<!-- ggplot(as_tibble(m_samp), aes(V1,V2)) + geom_point(alpha=.02) + -->
<!--     theme(axis.title.x=element_blank(),axis.title.y=element_blank()) -->
<!-- ``` -->
<!-- We compute the first thousand iterates of the EM algorithm, using three different starting points: uniform, discretized normal (from estimated statistical dimension and variance), circular cone (fitting the estimated statistical dimension).  -->
<!-- ```{r, eval=FALSE} -->
<!-- EM_iterates_mode0 <- bichibarsq_find_weights( m_samp, d, N=1000, mode=0) -->
<!-- EM_iterates_mode1 <- bichibarsq_find_weights( m_samp, d, N=1000, mode=1) -->
<!-- EM_iterates_mode2 <- bichibarsq_find_weights( m_samp, d, N=1000, mode=2) -->
<!-- ``` -->
<!-- Each computation (of thousand iterates) takes about ten minutes, so the resulting data has been included in the package. See the corresponding descriptions via `?EM_iterates_mode0`, etc.   -->
<!-- Here are the first few iterates (normal and log-plots); the true values are plotted in black: -->
<!-- 0. uniform distribution as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- tib_iter <- as_tibble( t(EM_iterates_mode0$iterates[1:8, ]) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot <- gather(tib_iter,step,value,2:dim(tib_iter)[2]) -->
<!-- tib_exact <- tibble( k=0:d, exact=circ_ivol(D,alpha,TRUE)) -->
<!-- ggplot(tib_plot,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- tib_iter_log <- as_tibble( t(log(EM_iterates_mode0$iterates[1:8, ])) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot_log <- gather(tib_iter_log,step,value,2:dim(tib_iter_log)[2]) -->
<!-- tib_exact_log <- tibble( k=0:d, exact=log(circ_ivol(D,alpha,TRUE))) -->
<!-- ggplot(tib_plot_log,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact_log,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- ``` -->
<!-- 1. discretized normal distribution as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- tib_iter <- as_tibble( t(EM_iterates_mode1$iterates[1:8, ]) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot <- gather(tib_iter,step,value,2:dim(tib_iter)[2]) -->
<!-- tib_exact <- tibble( k=0:d, exact=circ_ivol(D,alpha,TRUE)) -->
<!-- ggplot(tib_plot,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- tib_iter_log <- as_tibble( t(log(EM_iterates_mode1$iterates[1:8, ])) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot_log <- gather(tib_iter_log,step,value,2:dim(tib_iter_log)[2]) -->
<!-- tib_exact_log <- tibble( k=0:d, exact=log(circ_ivol(D,alpha,TRUE))) -->
<!-- ggplot(tib_plot_log,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact_log,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- ``` -->
<!-- 2. circular cone with same statistical dimension as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- tib_iter <- as_tibble( t(EM_iterates_mode2$iterates[1:8, ]) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot <- gather(tib_iter,step,value,2:dim(tib_iter)[2]) -->
<!-- tib_exact <- tibble( k=0:d, exact=circ_ivol(D,alpha,TRUE)) -->
<!-- ggplot(tib_plot,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- tib_iter_log <- as_tibble( t(log(EM_iterates_mode2$iterates[1:8, ])) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot_log <- gather(tib_iter_log,step,value,2:dim(tib_iter_log)[2]) -->
<!-- tib_exact_log <- tibble( k=0:d, exact=log(circ_ivol(D,alpha,TRUE))) -->
<!-- ggplot(tib_plot_log,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact_log,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- ``` -->
<!-- Well, that looks promising. Let's have a look at the iterates further out, say, in steps of thousands: -->
<!-- 0. uniform distribution as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- tib_iter <- as_tibble( t(EM_iterates_mode0$iterates[c(10,20,100,500,1000,2000,4000,10000), ]) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot <- gather(tib_iter,step,value,2:dim(tib_iter)[2]) -->
<!-- tib_exact <- tibble( k=0:d, exact=circ_ivol(D,alpha,TRUE)) -->
<!-- ggplot(tib_plot,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- tib_iter_log <- as_tibble( t(log(EM_iterates_mode0$iterates[c(10,20,100,500,1000,2000,4000,10000), ])) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot_log <- gather(tib_iter_log,step,value,2:dim(tib_iter_log)[2]) -->
<!-- tib_exact_log <- tibble( k=0:d, exact=log(circ_ivol(D,alpha,TRUE))) -->
<!-- ggplot(tib_plot_log,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact_log,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- ``` -->
<!-- 1. discretized normal distribution as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- tib_iter <- as_tibble( t(EM_iterates_mode1$iterates[c(10,20,100,500,1000,2000,4000,10000), ]) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot <- gather(tib_iter,step,value,2:dim(tib_iter)[2]) -->
<!-- tib_exact <- tibble( k=0:d, exact=circ_ivol(D,alpha,TRUE)) -->
<!-- ggplot(tib_plot,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- tib_iter_log <- as_tibble( t(log(EM_iterates_mode1$iterates[c(10,20,100,500,1000,2000,4000,10000), ])) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot_log <- gather(tib_iter_log,step,value,2:dim(tib_iter_log)[2]) -->
<!-- tib_exact_log <- tibble( k=0:d, exact=log(circ_ivol(D,alpha,TRUE))) -->
<!-- ggplot(tib_plot_log,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact_log,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- ``` -->
<!-- 2. circular cone with same statistical dimension as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- tib_iter <- as_tibble( t(EM_iterates_mode2$iterates[c(10,20,100,500,1000,2000,4000,10000), ]) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot <- gather(tib_iter,step,value,2:dim(tib_iter)[2]) -->
<!-- tib_exact <- tibble( k=0:d, exact=circ_ivol(D,alpha,TRUE)) -->
<!-- ggplot(tib_plot,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- tib_iter_log <- as_tibble( t(log(EM_iterates_mode2$iterates[c(10,20,100,500,1000,2000,4000,10000), ])) ) %>% -->
<!--                 add_column(k=0:d,.before=1) -->
<!-- tib_plot_log <- gather(tib_iter_log,step,value,2:dim(tib_iter_log)[2]) -->
<!-- tib_exact_log <- tibble( k=0:d, exact=log(circ_ivol(D,alpha,TRUE))) -->
<!-- ggplot(tib_plot_log,aes(x=k,y=value,color=step)) + -->
<!--     geom_line() + -->
<!--     geom_line(data=tib_exact_log,aes(x=k,y=exact),colour="black") +  -->
<!--     theme(legend.position="none", -->
<!--           axis.title.x=element_blank(), -->
<!--           axis.title.y=element_blank()) + -->
<!--     scale_colour_brewer() -->
<!-- ``` -->
<!-- <!-- -->
<!-- Let's have a look at the normalized (logarithmic) progression, that is, we normalize each intrinsic volume by the true value (without those one would of course take the last iterate); leaving out the first ten iterates: -->
<!-- 0. uniform distribution as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- #N_plot <- 100 -->
<!-- tib_prog <- as_tibble( sweep( EM_iterates_mode0$iterates[101:1001, ], MARGIN=2, circ_ivol(D,alpha,TRUE), "/") ) %>% -->
<!--                 add_column(iterate=100:1000, .before=1) -->
<!-- tib_prog_plot <- gather(tib_prog, vol, value, 2:dim(tib_prog)[2]) -->
<!-- ggplot(tib_prog_plot, aes(x=iterate, y=value, color=vol)) + -->
<!--     geom_line() + -->
<!--     scale_colour_brewer(palette = "Set3") -->
<!-- ``` -->
<!-- 1. discretized normal distribution as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- ``` -->
<!-- 2. circular cone with same statistical dimension as starting point -->
<!-- ```{r, echo=FALSE, fig.align='center', fig.width=7, eval=TRUE} -->
<!-- ``` -->
<!-- -->
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This actually requires a proof in the nonpolyhedral case (and without having log-concavity); one can deduce this from the kinematic formula by projecting the cone on a uniformly random linear subspace and looking at the volume of the (relative) boundary of the projection.<a href="#fnref1">â©</a></p></li>
<li id="fn2"><p>The euclidean intrinsic volumes satisfy the log-concavity inequalities by the Alexandrov-Fenchel inequality for the mixed volumes. In small dimensions (<span class="math inline">\(\leq10\)</span>) this partly implies log-concavity for the conic intrinsic volumes. But in fact the implication goes the other way: log-concavity of conic intrinsic volumes easily implies log-concavity of euclidean intrinsic volumes.<a href="#fnref2">â©</a></p></li>
</ol>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
