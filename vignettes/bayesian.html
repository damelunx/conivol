<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Dennis Amelunxen" />

<meta name="date" content="2017-09-12" />

<title>Bayesian estimates for conic intrinsic volumes</title>






<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Bayesian estimates for conic intrinsic volumes</h1>
<h4 class="author"><em>Dennis Amelunxen</em></h4>
<h4 class="date"><em>2017-09-12</em></h4>



<p>In this vignette we derive Bayesian estimates of the conic intrinsic volumes: given a prior distribution on the intrinsic volumes, for which we will give two choices corresponding to an informative and a non-informative prior, we derive a sampling procedure for the posterior distribution corresponding to sample data of the corresponding bivariate chi-bar-squared distribution. More precisely, we will use sample data from a slightly modified distribution, which was also used in the EM algorithm, described in <a href="estim-conic-intrinsic-volumes-with-EM.html">this vignette</a>. See <a href="conic-intrinsic-volumes.html">this vignette</a> for a short introduction to conic intrinsic volumes.</p>
<div id="mod_bivchibarsq" class="section level2">
<h2>The modified bivariate chi-bar-squared distribution</h2>
<p>Recall from <a href="estim-conic-intrinsic-volumes-with-EM.html#setup">this vignette</a> that we assumed to have sampled data of the following form: <span class="math inline">\((X_1,Y_1),\ldots,(X_N,Y_N)\)</span> denote iid copies of <span class="math inline">\((X,Y)\)</span>, which follow the chi-bar-squared distribution of some closed convex cone, which is not a linear subspace, and we assume that these copies took the sample values <span class="math inline">\((x_1,y_1),\ldots,(x_N,y_N)\)</span>. The latent variable <span class="math inline">\(Z\)</span> and the corresponding iid copies <span class="math inline">\(Z_1,\ldots,Z_N\)</span> are not completely latent, as the values <span class="math inline">\(0\)</span> and <span class="math inline">\(d\)</span> are visible in the data by the events <span class="math inline">\(x_i=0\)</span> and <span class="math inline">\(y_i=0\)</span>, respectively. We thus defined the two proportions <span class="math display">\[ p = \frac{\left|\{i\mid y_i=0\}\right|}{N} ,\quad q = \frac{\left|\{i\mid x_i=0\}\right|}{N} , \]</span> and dropped those sample values <span class="math inline">\((x_i,y_i)\)</span> with <span class="math inline">\(x_i=0\)</span> or <span class="math inline">\(y_i=0\)</span>. Without loss of generality, we used the notation <span class="math inline">\((x_1,y_1),\ldots,(x_{\bar N},y_{\bar N})\)</span> for the remaining sample points, <span class="math inline">\(\bar N\leq N\)</span>.</p>
<p>We can formalize these steps by considering the following mixed continuous-discrete distribution: for given weight vector <span class="math inline">\(v=(v_0,\ldots,v_d)\)</span>, <span class="math inline">\(v\geq0\)</span>, <span class="math inline">\(\sum_{k=0}^d v_k=1\)</span>, consider the distribution <span class="math display">\[ f_v(x,y) = v_d\, \delta(-1,0) + v_0\, \delta(0,-1) + \sum_{k=1}^{d-1} v_k f_k(x) f_{d-k}(y) , \]</span> where <span class="math inline">\(\delta\)</span> denotes the (bivariate) Dirac delta, and <span class="math inline">\(f_k(x)\)</span> denotes the density of the chi-squared distribution with <span class="math inline">\(k\)</span> degrees of freedom. The original sample data is then chosen from this distribution, while the sample points remaining after dropping the sample values with <span class="math inline">\(x_i=0\)</span> or <span class="math inline">\(y_i=0\)</span> can be interpreted as iid sample data from the continuous distribution <span class="math display">\[ \bar f_v(x,y) = \sum_{k=1}^{d-1} \frac{v_k}{1-v_0-v_d} f_k(x) f_{d-k}(y) , \]</span> with corresponding conditional latent variables <span class="math inline">\(\bar Z\in\{1,\ldots,d-1\}\)</span> and corresponding copies <span class="math inline">\(\bar Z_1,\ldots,\bar Z_{\bar N}\)</span>.</p>
<!-- The cumulative distribution function (cdf) is thus given by -->
<!--   \[ F_v(x,y) = \begin{cases} -->
<!--       v_d & \text{if } -1\leq x<0\leq y , -->
<!--    \\ v_0 & \text{if } -1\leq y<0\leq x , -->
<!--    \\ v_0 + v_d + \sum_{k=1}^{d-1} v_k F_k(x)F_{d-k}(y) & \text{if } (x,y)\geq0 , -->
<!--    \\ 0 & \text{else} , -->
<!--    \end{cases} \] -->
<!-- where $F_k(x)$ denotes the cdf of the chi-squared distribution. We will assume this distribution in what follows below. -->
</div>
<div id="bayesian-approach" class="section level2">
<h2>Bayesian approach</h2>
<p>In a full Bayesian approach we do not consider the intrinsic volumes as parameters but rather as random themselves. So we change notation and introduce the random variable <span class="math inline">\(V=(V_0,\ldots,V_d)^T\)</span> that takes values in the probability simplex <span class="math display">\[ \Delta^d = \big\{v\in\text{R}^{d+1}\mid 0\leq v_k\leq 1 \text{ for all }k, \|v\|_1=1\big\} , \]</span> almost surely, where <span class="math inline">\(\|v\|_1=|v_0|+\dots+|v_d|\)</span> denotes the <span class="math inline">\(\ell_1\)</span>-norm.</p>
<p>The intrinsic volumes of cones which are not linear subspaces satisfy the additional equation <span class="math inline">\(V_0+V_2+V_4+\dots=V_1+V_3+V_5+\dots=\frac{1}{2}\)</span>, which is why we also use the two random variables <span class="math inline">\(V^e=2(V_0,V_2,V_4,\ldots)^T\)</span> and <span class="math inline">\(V^o=2(V_1,V_3,V_5,\ldots)^T\)</span> both taking values in a probability simplex (of possibly different dimension) with the correlation <span class="math display">\[ V = \tfrac12 
% \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; \dots \\ 0 &amp; 0 &amp; 0 &amp; \dots  \\ 0 &amp; 1 &amp; 0 &amp; \dots \\ 0 &amp; 0 &amp; 0 &amp; \dots \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots \end{pmatrix}
(e_0, e_2, e_4,\ldots) V^e  + \tfrac12
% \begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; \dots \\ 1 &amp; 0 &amp; 0 &amp; \dots  \\ 0 &amp; 0 &amp; 0 &amp; \dots \\ 0 &amp; 1 &amp; 0 &amp; \dots \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots \end{pmatrix}
(e_1, e_3, e_5,\ldots) V^o , \]</span> where <span class="math inline">\(e_i\in\text{R}^{d+1}\)</span> denotes the <span class="math inline">\(i\)</span>th canonical basis vector (indices starting at zero). Notationwise, we define <span class="math inline">\(d^e=\lfloor \frac{d}{2}\rfloor\)</span> and <span class="math inline">\(d^o=\lceil \frac{d}{2}\rceil-1\)</span> so that <span class="math inline">\(V^e\in \Delta^{d^e}\)</span> and <span class="math inline">\(V^o\in \Delta^{d^o}\)</span>.</p>
<p>In the following we will explain the issues first in terms of the random vector <span class="math inline">\(V\)</span> before introducing a refinement that takes the split in even and odd indices into account.</p>
</div>
<div id="prior-distribution" class="section level2">
<h2>Prior distribution</h2>
We use the Dirichlet distribution as a prior, <span class="math inline">\(V\sim \text{Dirichlet}(a)\)</span> with <span class="math inline">\(a=(a_0,\ldots,a_d)\)</span> and <span class="math inline">\(a_k&gt;0\)</span> for all <span class="math inline">\(k\)</span>. The density of <span class="math inline">\(V\)</span> is given by
<span class="math display">\[\begin{align*}
   p(V=v) &amp; = \frac{\Gamma(\|a\|_1)}{\Gamma(a_0)\dots\Gamma(a_d)}\, v_0^{a_0-1}\dots v_d^{a_d-1} \; \big(\propto v_0^{a_0-1}\dots v_d^{a_d-1}\big)
\end{align*}\]</span>
<p>for <span class="math inline">\(v\in\Delta^d\)</span>, and zero else. The expectation of <span class="math inline">\(V\)</span> is given by <span class="math display">\[ \text{E}[V] = \frac{1}{\|a\|_1} (a_0,\ldots,a_d) , \]</span> and the <em>prior sample size</em>, cf. <span class="citation">(Gelman et al. 2014)</span>, is given by <span class="math inline">\(\|a\|_1\)</span>. Note that the <span class="math inline">\(k\)</span>th marginal distribution is concave iff <span class="math inline">\(a_k\geq1\)</span>; if <span class="math inline">\(a_k&gt;1\)</span> for all <span class="math inline">\(k\)</span>, then the mode of the <span class="math inline">\(k\)</span>th marginal distribution is given by <span class="math display">\[ \text{mode}(V_k) = \frac{a_k-1}{\|a-1\|_1} . \]</span> There are two natural choices for the parameters of the prior distribution, one noninformative, the other (slightly) informative; both using the starting point of the EM algorithm <span class="math inline">\(v^{(0)}\)</span>, see <a href="estim-conic-intrinsic-volumes-with-EM.html#start_EM">this vignette</a>:</p>
<ol style="list-style-type: decimal">
<li><em>noninformative:</em> <span class="math inline">\(a_k = v^{(0)}_k\)</span> for <span class="math inline">\(k=0,\ldots,d\)</span>. In this case we have matched the first moment: <span class="math inline">\(\text{E}[V]=v^{(0)}\)</span>. The marginal distributions are convex and widely spread; the prior sample size is <span class="math inline">\(\|a\|_1=1\)</span>. Note that in this case we must have <span class="math inline">\(v^{(0)}_k&gt;0\)</span> for all <span class="math inline">\(k\)</span>, cp. the discussion in <a href="conic-intrinsic-volumes.html#intro_intrvol">this vignette</a>.</li>
<li><em>informative:</em> <span class="math inline">\(a_k = 1 + v^{(0)}_k\)</span> for <span class="math inline">\(k=0,\ldots,d\)</span>. In this case we have matched the modes: <span class="math inline">\(\text{mode}(V_k)=v^{(0)}_k\)</span>. The marginal distributions are concave; the prior sample size is <span class="math inline">\(\|a\|_1=d+2\)</span>.</li>
</ol>
<div id="even-and-odd-indices" class="section level3">
<h3>Even and odd indices</h3>
<p>By the same reasoning we arrive at the following prior distributions for <span class="math inline">\(V^e\)</span> and <span class="math inline">\(V^o\)</span>: <span class="math display">\[ V^e\sim \text{Dirichlet}(b) ,\quad V^o\sim \text{Dirichlet}(c) , \]</span> with the constants <span class="math inline">\(b=(b_0,b_2,b_4,\ldots)\)</span> and <span class="math inline">\(c=(c_1,c_3,c_5,\ldots)\)</span> chosen either in the noninformative or in the informative way:</p>
<ol style="list-style-type: decimal">
<li><em>noninformative:</em> <span class="math inline">\(b_j = 2v^{(0)}_j\)</span>, <span class="math inline">\(c_k = 2v^{(0)}_k\)</span>, for <span class="math inline">\(j\)</span> even and <span class="math inline">\(k\)</span> odd,</li>
<li><em>informative:</em> <span class="math inline">\(b_j = 1 + 2v^{(0)}_j\)</span>, <span class="math inline">\(c_k = 1 + 2v^{(0)}_k\)</span>, for <span class="math inline">\(j\)</span> even and <span class="math inline">\(k\)</span> odd.</li>
</ol>
</div>
</div>
<div id="posterior-distribution" class="section level2">
<h2>Posterior distribution</h2>
<p>The posterior distribution for <span class="math inline">\(V\)</span>, or <span class="math inline">\(V^e\)</span> and <span class="math inline">\(V^o\)</span>, respectively, is easily derived through Bayes’ Theorem. What is a bit more complicated is the question how to sample from this distribution, which is crucial for obtaining, for example, credible regions or morginal credible intervals. For this we will describe how to extend the distribution on the probability simplex (or product of probability simplices in the even/odd index decomposition) to a distribution on the positive orthant, so that we can employ a standard Gibbs sampling procedure to sample from the posterior distribution.</p>
<p>As before we will first describe this for the random vector <span class="math inline">\(V\)</span> before describing the necessary adaptations for the even/odd index decomposition.</p>
<div id="distr_simplex" class="section level3">
<h3>Distribution on probability simplex</h3>
We derive the posterior distribution by using the latent variable <span class="math inline">\(Z\)</span>. Note first that the posterior distribution of <span class="math inline">\(V\)</span> given direct samples of the latent variable is again Dirichlet distributed:
<span class="math display">\[\begin{align*}
   p(V=v\mid \mathbf Z=\mathbf z) &amp; = \frac{p(\mathbf Z=\mathbf z\mid V=v)\, p(V=v)}{p(\mathbf Z=\mathbf z)} \propto \prod_{k=0}^d v_k^{a_k+|\{i\colon z_i=k\}|-1}
\\ &amp; = \text{Dirichlet}\big(a_0+\big|\{i\colon z_i=0\}\big|,a_1+\big|\{i\colon z_i=1\}\big|,\ldots,a_d+\big|\{i\colon z_i=d\}\big|\big) .
\end{align*}\]</span>
<p>We do not have direct samples of the latent variables, except for those values for which <span class="math inline">\(Z\in\{0,d\}\)</span>. For the remaining sample points, which we assume without loss of generality to have the indices <span class="math inline">\(1,\ldots,\bar N\)</span>, we do not have the values of the latent variable, so we will have to consider all possible constellations for these.</p>
Recall that given the value of the latent variable, the distribution of the sample is independent of the intrinsic volumes. As in <a href="estim-conic-intrinsic-volumes-with-EM.html">this vignette</a>, we use bold letters to denote the vectors collecting the sample points, and we use the notation <span class="math display">\[ f_{ik} = f_k(x_i)f_{d-k}(y_i) \quad\text{for $1\leq i\leq \bar N$ and $0&lt;k&lt;d$} , \]</span> where <span class="math inline">\(f_k\)</span> denotes the density of <span class="math inline">\(\chi_k^2\)</span>. We obtain the density of the posterior distribution as follows:
<span class="math display">\[\begin{align*}
   &amp; p(V=v\mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y) = \sum_{\bar{\mathbf z}\in\{1,\ldots,d-1\}^{\bar N}} p(V=v, \mathbf Z=\mathbf z\mid \mathbf X=\mathbf x,\mathbf Y=\mathbf y)
\\ &amp; \qquad = \sum_{\bar{\mathbf z}\in\{1,\ldots,d-1\}^{\bar N}} \frac{p(\mathbf X=\mathbf x,\mathbf Y=\mathbf y\mid \mathbf Z=\mathbf z)\, p(\mathbf Z=\mathbf z\mid V=v)\, p(V=v)}{p(\mathbf X=\mathbf x,\mathbf Y=\mathbf y)}
\\ &amp; \qquad = \frac{p(V=v)\; v_0^{qN} v_d^{pN}}{p(\mathbf X=\mathbf x,\mathbf Y=\mathbf y)} \sum_{\bar{\mathbf z}\in\{1,\ldots,d-1\}^{\bar N}} \prod_{i=1}^{\bar N} v_{z_i}\,f_{z_i}(x_i)\,f_{d-z_i}(y_i)
\\ &amp; \qquad = \frac{p(V=v)\;v_0^{qN} v_d^{pN}}{p(\mathbf X=\mathbf x,\mathbf Y=\mathbf y)} \; \prod_{i=1}^{\bar N} \sum_{k=1}^{d-1} v_k\,f_{ik}
\\ &amp; \qquad \propto v_0^{a_0+qN-1} v_d^{a_d+pN-1} \Big( \prod_{k=1}^{d-1} v_k^{a_k-1} \Big) \; \prod_{i=1}^{\bar N} \sum_{k=1}^{d-1} v_k\,f_{ik} .
\end{align*}\]</span>
In order to avoid case distinctions corresponding to the first and last indices, we extend the notation for <span class="math inline">\(f_{ik}\)</span>:
<span class="math display">\[\begin{align*}
   \text{for } i\leq \bar N &amp; : \qquad f_{i0} = f_{id} = 0 ,
\\[1mm] \text{for } \bar N&lt;i\leq N &amp; : \qquad f_{ik} = \begin{cases}
                 1 &amp; \text{if $k=0$ and $x_i=0$}
              \\ 1 &amp; \text{if $k=d$ and $y_i=0$}
              \\ 0 &amp; \text{else} .
              \end{cases}
\end{align*}\]</span>
With this extended notation we can write the posterior distribution as follows:
<span class="math display">\[\begin{align*}
   &amp; p(V=v\mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y) \propto \Big(\prod_{k=0}^d v_k^{a_k-1} \Big) \; \prod_{i=1}^N \sum_{k=0}^d v_k\,f_{ik} .
\end{align*}\]</span>
<p>Note that this is only for notational convenience; in the <a href="#implementation">implementation</a> of the resulting formulas we will of course break this down again.</p>
</div>
<div id="extended-distribution-on-positive-orthant" class="section level3">
<h3>Extended distribution on positive orthant</h3>
<p>In order to be able to employ a standard Gibbs sampling procedure to obtain samples from the posterior distribution, we extend the lower-dimensional distribution on the probability simplex to a full-dimensional distribution on the positive orthant. For this we define the random variable <span class="math inline">\(W\in\text{R}^{d+1}\)</span> via <span class="math display">\[ W = R\cdot V = \frac{R}{\sqrt{d+1}}\sqrt{d+1} V , \]</span> where <span class="math inline">\(R\sim \chi_{d+1}\)</span> independent of <span class="math inline">\(V\)</span>. By construction, renormalizing <span class="math inline">\(W\)</span> yields <span class="math inline">\(V\)</span>, <span class="math display">\[ \frac{W}{\|W\|_1}=V , \]</span> so any sampling of <span class="math inline">\(W\)</span> can easily be converted into a sampling of <span class="math inline">\(V\)</span>.</p>
What is missing at this point is the density of <span class="math inline">\(W\)</span>. This can be obtained by considering the parametrization of the positive orthant given by <span class="math display">\[ \Delta_d\times \text{R}_+\to\text{R}_+^{d+1} ,\quad (v,t)\mapsto t\sqrt{d+1} x , \]</span> where we introduce the scaling factor <span class="math inline">\(\sqrt{d+1}\)</span> because the probability simplex only contains the scaled diagonal <span class="math inline">\(\frac{1}{d+1}1_{d+1}\)</span>, which has length <span class="math inline">\(\frac1{\sqrt{d+1}}\)</span>. The normal Jacobian in <span class="math inline">\(w=(w_0,\ldots,w_d)\)</span>, that is, the absolute value of the determinant of the Jacobian in <span class="math inline">\(w\)</span>, of the inverse map is given by <span class="math inline">\(\|w\|_1^{-d}\)</span>; see <a href="#proof_normal_jacobian">below</a> for a proof. Hence, the density of <span class="math inline">\(W\)</span> can be expressed in the form <span class="math display">\[ p(W=w) = \frac{p(R=\|w\|_1)}{\|w\|_1^d}\; p\Big(V=\frac{w}{\|w\|_1}\Big) . \]</span> For the posterior distribution we thus obtain <span class="math display">\[ p(W=w\mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y) = \frac{p(W=w)}{p(\mathbf X=\mathbf x,\mathbf Y=\mathbf y)} \prod_{i=1}^N \sum_{k=0}^d f_{ik} \frac{w_k}{\|w\|_1} . \]</span> The prior distribution of the extended random variable is given by
<span class="math display">\[\begin{align*}
   p(W=w) &amp; = \frac{p(R=\|w\|_1)}{\|w\|_1^d}\, \frac{\Gamma(\|a\|_1)}{\Gamma(a_0)\dots\Gamma(a_d)}\, \frac{w_0^{a_0-1}\dots w_d^{a_d-1}}{\|w\|_1^{\|a\|_1-d-1}}
\\ &amp; = \frac{\frac{\Gamma(\|a\|_1)}{\Gamma(a_0)\dots\Gamma(a_d)}}{2^{(d-1)/2}\Gamma((d+1)/2)} \frac{w_0^{a_0-1}\dots w_d^{a_d-1}}{\|w\|_1^{\|a\|_1-d-1}\exp(\|w\|_1^2/2)} .
\end{align*}\]</span>
<p>Ignoring the normalizing constant, we thus have <span class="math display">\[ p(W=w\mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y) \propto \frac{1}{\exp(\|w\|_1^2/2)} \Big(\prod_{k=0}^d \Big(\frac{w_k}{\|w\|_1}\Big)^{a_k-1} \Big) \prod_{i=1}^N \sum_{k=0}^d f_{ik}\frac{w_k}{\|w\|_1} . \]</span></p>
</div>
<div id="change-to-logarithmic-scale" class="section level3">
<h3>Change to logarithmic scale</h3>
<p>For the sampling it makes more sense, numerically, to consider the logarithm of the random variables. Taking into account the normal Jacobian that results from the change of variable, we obtain the posterior density of the random variable <span class="math inline">\(\log W\)</span> as follows: <span class="math display">\[ p(\log W=\log w\mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y) \propto \frac{\prod_{k=0}^d w_k}{\exp(\|w\|_1^2/2)} \Big(\prod_{k=0}^d \Big(\frac{w_k}{\|w\|_1}\Big)^{a_k-1} \Big) \prod_{i=1}^N \sum_{k=0}^d f_{ik}\frac{w_k}{\|w\|_1} . \]</span> <!-- For the two choices of constants for the prior distribution we obtain --> <!-- \begin{align*} --> <!--    & \text{non-informative choice:} --> <!-- \\ & \qquad p(\log W=\log w\mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y) \propto \frac{w_0^{v^{(0)}_0+qN} w_d^{v^{(0)}_d+pN} \prod_{k=1}^{d-1} w_k^{v^{(0)}_k}}{\|w\|_1^{N-d}\exp(\|w\|_1^2/2)} \prod_{i=1}^{\bar N} \sum_{k=1}^{d-1} f_{ik}\, w_k , --> <!-- \\[2mm] & \text{informative choice:} --> <!-- \\ & \qquad p(\log W=\log w\mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y) \propto \frac{w_0^{v^{(0)}_0+qN} w_d^{v^{(0)}_d+pN} \prod_{k=1}^{d-1} w_k^{v^{(0)}_k}}{\|w\|_1^{N+1}\exp(\|w\|_1^2/2)} \prod_{i=1}^{\bar N} \sum_{k=1}^{d-1} f_{ik}\, w_k . --> <!-- \end{align*} --></p>
</div>
<div id="even-and-odd-indices-1" class="section level3">
<h3>Even and odd indices</h3>
As for the decomposition of the vectors into even and odd indices, we note that the posterior distribution on the probability simplex is trivially obtained from the observation <span class="math inline">\(V=v\iff (V^e=v^e \text{ and } V^o=v^o)\)</span>, where <span class="math inline">\(v=(v_0,v_1,\ldots,v_d)\)</span>, <span class="math inline">\(v^e=2(v_0,v_2,v_4,\ldots)\)</span>, <span class="math inline">\(v^o=2(v_1,v_3,v_5,\ldots)\)</span>,
<span class="math display">\[\begin{align*}
   &amp; p(V^e=v^e, V^o=v^o\mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y) = \frac{p(V^e=v^e)\; p(V^o=v^o)}{p(\mathbf X=\mathbf x,\mathbf Y=\mathbf y)} \; \prod_{i=1}^N \sum_{k=0}^d v_k\,f_{ik} .
\end{align*}\]</span>
Analogous to the above extension of <span class="math inline">\(V\)</span> to <span class="math inline">\(W\)</span>, we extend <span class="math inline">\(V^e\)</span> and <span class="math inline">\(V^o\)</span> to <span class="math inline">\(T\)</span> and <span class="math inline">\(U\)</span>, respectively,<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> <span class="math display">\[ T = R^e\cdot V^e ,\quad U = R^o\cdot V^o , \]</span> where <span class="math inline">\(R^e\sim \chi_{d^e+1}\)</span> and <span class="math inline">\(R^o\sim \chi_{d^o+1}\)</span> independent of each other and of <span class="math inline">\(V^e\)</span> and <span class="math inline">\(V^o\)</span>. We use the index notation <span class="math inline">\(T=(T_0,T_2,T_4,\ldots)\in\text{R}^{d^e+1}\)</span> and <span class="math inline">\(U=(U_1,U_3,U_5,\ldots)\in\text{R}^{d^o+1}\)</span>. As above, we obtain
<span class="math display">\[\begin{align*}
   p(T=t, U=u) &amp; = \frac{p(R^e=\|t\|_1)}{\|t\|_1^{d^e}}\; \frac{p(R^o=\|u\|_1)}{\|u\|_1^{d^o}}\; p\Big(V^e=\frac{t}{\|t\|_1}, V^o=\frac{u}{\|u\|_1}\Big) ,
\end{align*}\]</span>
so the extended posterior distribution, directly converted to the distribution of the logarithms, is given by
<span class="math display">\[\begin{align*}
   p(\log T=\log t, \log U=\log u &amp; \mid \mathbf X=\mathbf x, \mathbf Y=\mathbf y)
\\ &amp; \propto \frac{t_0t_2t_4\dots}{\exp(\|t\|_1^2/2)} \Big( \frac{t_0}{\|t\|_1} \Big)^{b_0-1} \Big( \frac{t_2}{\|t\|_1} \Big)^{b_2-1} \Big( \frac{t_4}{\|t\|_1} \Big)^{b_4-1} \dots
\\ &amp; \qquad \cdot \frac{u_1u_3u_5\dots}{\exp(\|u\|_1^2/2)} \Big( \frac{u_1}{\|u\|_1} \Big)^{c_1-1} \Big( \frac{u_3}{\|u\|_1} \Big)^{c_3-1} \Big( \frac{u_5}{\|u\|_1} \Big)^{c_5-1} \dots 
\\ &amp; \qquad \cdot \prod_{i=1}^N \Big( \frac{\sum_{k\text{ even}}f_{ik}\,t_k}{2\|t\|_1} + \frac{\sum_{k\text{ odd}}f_{ik}\,u_k}{2\|u\|_1}\Big) .
\end{align*}\]</span>
<!-- For the two choices of constants for the prior distribution we obtain -->
<!-- \begin{align*} -->
<!--    & \text{non-informative choice:} -->
<!-- \\ & \qquad \frac{t_0^{qN + 2v^{(0)}_0}t_2^{2v^{(0)}_2}t_4^{2v^{(0)}_4}\dots}{\|t\|_1^{qN-d^e} \exp(\|t\|_1^2/2)}\, \frac{u_1^{2v^{(0)}_1}u_3^{2v^{(0)}_3}u_5^{2v^{(0)}_5}\dots}{\|u\|_1^{-d^o} \exp(\|u\|_1^2/2)} \cdot \begin{cases} -->
<!--        t_d^{pN} \|t\|_1^{-pN} & \text{if $d$ even} -->
<!--     \\[1mm] u_d^{pN} \|u\|_1^{-pN} & \text{if $d$ odd} -->
<!--     \end{cases} -->
<!-- \\ & \qquad \cdot \prod_{i=1}^{\bar N} \Big( \frac{\sum_{k\text{ even}}f_{ik}\,t_k}{2\|t\|_1} + \frac{\sum_{k\text{ odd}}f_{ik}\,u_k}{2\|u\|_1}\Big) , -->
<!-- \\[3mm] & \text{informative choice:} -->
<!-- \\ & \qquad \frac{t_0^{qN + 1+2v^{(0)}_0}t_2^{1+2v^{(0)}_2}t_4^{1+2v^{(0)}_4}\dots}{\|t\|_1^{qN+1} \exp(\|t\|_1^2/2)}\, \frac{u_1^{1+2v^{(0)}_1}u_3^{1+2v^{(0)}_3}u_5^{1+2v^{(0)}_5}\dots}{\|u\|_1 \exp(\|u\|_1^2/2)} \cdot \begin{cases} -->
<!--        t_d^{pN} \|t\|_1^{-pN} & \text{if $d$ even} -->
<!--     \\[1mm] u_d^{pN} \|u\|_1^{-pN} & \text{if $d$ odd} -->
<!--     \end{cases} -->
<!-- \\ & \qquad \cdot \prod_{i=1}^{\bar N} \Big( \frac{\sum_{k\text{ even}}f_{ik}\,t_k}{2\|t\|_1} + \frac{\sum_{k\text{ odd}}f_{ik}\,u_k}{2\|u\|_1}\Big) . -->
<!-- \end{align*} -->
</div>
</div>
<div id="gibbs-sampling" class="section level2">
<h2>Gibbs sampling</h2>
<p>For the extended distribution we can employ a standard Gibbs sampling method, see for example <span class="citation">(Gelman et al. 2014, Ch. 11)</span> or <span class="citation">(Hastie, Tibshirani, and Friedman 2009, Sec. 8.6)</span>.</p>
<p>The simple idea of this algorithm is to go from one sample point to the next by going over all coordinates (in a random order that is chosen before each updating round) and replace each, while holding the remaining coordinates fixed, with a sample from the corresponding conditional distribution.</p>
Assuming that the current sample is <span class="math inline">\(w=(w_0,\ldots,w_d)\)</span>, we have, up to normalizing constant, the following conditional posterior distribution for the <span class="math inline">\(k\)</span>th coordinate of <span class="math inline">\(\log W\)</span>:
<span class="math display">\[\begin{align*}
   p(\log W_k=r &amp; \mid W_{-k}=w_{-k}, \mathbf X=\mathbf x, \mathbf Y=\mathbf y)
\\ &amp; \propto \frac{e^{a_kr} \prod_{i=1}^N \big(e^r f_{ik} + \sum_{j\neq k} f_{ij}\, w_j\big)}{(e^r+\|w_{-k}\|_1)^{\|a\|_1+N-d-1}\exp((e^r+\|w_{-k}\|_1)^2/2)} =: F(r) .
\end{align*}\]</span>
<p>We sample from <span class="math inline">\(\log W_k\)</span>, in theory, through the following procedure:</p>
<ol style="list-style-type: decimal">
<li>compute the integral <span class="math inline">\(\int_{-\infty}^\infty F(r)\,dr\)</span> to get the normalizing constant,</li>
<li>sample <span class="math inline">\(u\in(0,1)\)</span> from the uniform distribution,</li>
<li>find the sample <span class="math inline">\(r_w\)</span> by solving the equation <span class="math inline">\(\int_{-\infty}^{r_w} F(r)\,dr=u\int_{-\infty}^\infty F(r)\,dr\)</span> or, equivalently, <span class="math inline">\(\int_{r_w}^\infty F(r)\,dr=(1-u)\int_{-\infty}^\infty F(r)\,dr\)</span>.</li>
</ol>
<div id="cut-off-points" class="section level3">
<h3>Cut-off points</h3>
In order to make the above procedure feasible, we must find sensible cut-off points <span class="math inline">\(r_-&lt;r_+\)</span> so that, broadly speaking, solving the equation <span class="math inline">\(\int_{r_-}^{r_w} F(r)\,dr= u\int_{r_-}^{r_+} F(r)\,dr\)</span> or <span class="math inline">\(\int_{r_w}^{r_+} F(r)\,dr=(1-u)\int_{r_-}^{r_+} F(r)\,dr\)</span> yields a point close enough to the solution of the original equations. More precisely, assuming that <span class="math display">\[ \int_{-\infty}^{r_-} F(r)\,dr \leq \varepsilon_- ,\quad \int_{r_+}^\infty F(r)\,dr \leq \varepsilon_+ , \]</span> we have
<span class="math display">\[\begin{align*}
   \int_{r_-}^{r^\ast} F(r)\,dr &amp; \leq u \int_{r_-}^{r_+} F(r)\,dr - \varepsilon_-
%\\ &amp;
\;\Rightarrow\; \int_{-\infty}^{r^\ast} F(r)\,dr \leq u \int_{-\infty}^\infty F(r)\,dr 
% &amp;
\;\Rightarrow\; r^\ast \leq r^w .
% \\ \int_{r_\ast}^{r^+} F(r)\,dr &amp; \leq (1-u) \int_{r_-}^{r_+} F(r)\,dr - \varepsilon_+
% \\ &amp; \;\Rightarrow\; \int_{r^\ast}^\infty F(r)\,dr \leq (1-u) \int_{-\infty}^\infty F(r)\,dr &amp; \;\Rightarrow\; r^\ast \geq r^w ,
% \\ \int_{r_-}^{r^\ast} F(r)\,dr &amp; \geq u \int_{r_-}^{r_+} F(r)\,dr + u(\varepsilon_-+\varepsilon_+)
% \\ &amp; \;\Rightarrow\; \int_{-\infty}^{r^\ast} F(r)\,dr \geq u \int_{-\infty}^\infty F(r)\,dr &amp; \;\Rightarrow\; r^\ast \geq r^w ,
% \\ \int_{r^\ast}^{r_+} F(r)\,dr &amp; \geq (1-u) \int_{r_-}^{r_+} F(r)\,dr + (1-u)(\varepsilon_-+\varepsilon_+)
% \\ &amp; \;\Rightarrow\; \int_{r^\ast}^\infty F(r)\,dr \geq (1-u) \int_{-\infty}^\infty F(r)\,dr &amp; \;\Rightarrow\; r^\ast \leq r^w .
\end{align*}\]</span>
Similarly,
<span class="math display">\[\begin{align*}
   \int_{r^\ast}^{r_+} F(r)\,dr \geq (1-u) \int_{r_-}^{r_+} F(r)\,dr + (1-u)(\varepsilon_-+\varepsilon_+) &amp; \;\Rightarrow\; r^\ast \leq r^w ,
\\ \int_{r_\ast}^{r^+} F(r)\,dr \leq (1-u) \int_{r_-}^{r_+} F(r)\,dr - \varepsilon_+ &amp; \;\Rightarrow\; r^\ast \geq r^w ,
\\ \int_{r_-}^{r^\ast} F(r)\,dr \geq u \int_{r_-}^{r_+} F(r)\,dr + u(\varepsilon_-+\varepsilon_+) &amp; \;\Rightarrow\; r^\ast \geq r^w .
\end{align*}\]</span>
It can be shown, see <a href="#proof_cutoff">below</a> for a proof, that in the non-informative case we have the following valid formulas for <span class="math inline">\(r_-\)</span> and <span class="math inline">\(r_+\)</span>:
<span class="math display">\[\begin{align*}
   r_- &amp; = \frac{\max\Big\{0,\; \log \varepsilon_- + \log v^{(0)}_k + \frac{\|w_{-k}\|_1^2}{2} - d\log(1+\|w_{-k}\|_1) - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\} \Big\}}{v^{(0)}_k} ,
\\ r_+ &amp; = \min\Bigg\{ 0, \frac{\frac{1}{2} \log\Big( 2\log\Gamma^{-1}\Big( \frac{d+1}{2}, \log\varepsilon_+ - \frac{d-1}{2} \log 2 - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\}\Big) \Big) }{\log \|w_{-k}\|_1} \Bigg\} ,
\end{align*}\]</span>
where <span class="math inline">\(\Gamma(\cdot,\cdot)\)</span> denotes the <a href="https://en.wikipedia.org/wiki/Incomplete_gamma_function">incomplete gamma function</a>. In the informative case we have the following valid cut-off points:
<span class="math display">\[\begin{align*}
   r_- &amp; = \frac{\max\Big\{0,\; \log \varepsilon_- + \log(1+v^{(0)}_k) + \frac{\|w_{-k}\|_1^2}{2} + \log\|w_{-k}\|_1 - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\} \Big\}}{1+v^{(0)}_k} ,
\\ r_+ &amp; = \min\Bigg\{ 0, \frac{\frac{1}{2}\log\Big( 2\log\Gamma^{-1}\Big( \frac{1}{2},\; \log\varepsilon_+ + \frac{\log 2}{2} - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\}\Big) \Big)}{\log \|w_{-k}\|_1} \Bigg\} .
\end{align*}\]</span>
<p>As for the question on how to choose <span class="math inline">\(\varepsilon_-\)</span> and <span class="math inline">\(\varepsilon_+\)</span> so that the corresponding upper and lower interval limits are sufficiently close, we can do this iteratively by starting with <span class="math inline">\(\varepsilon_-=\varepsilon_+=1\)</span> and successively halfing these values until a pre-specified threshold for the interval length has been met; the estimate for <span class="math inline">\(r^w\)</span> can then be chosen as the midpoint of that interval.</p>
</div>
<div id="even-and-odd-indices-2" class="section level3">
<h3>Even and odd indices</h3>
For even <span class="math inline">\(k\)</span> we have
<span class="math display">\[\begin{align*}
   p(\log T_k=r &amp; \mid T_{-k}=t_{-k}, U=u, \mathbf X=\mathbf x, \mathbf Y=\mathbf y)
\\ &amp; \propto \frac{e^{b_kr} \prod_{i=1}^N \big( \big(e^r f_{ik} + \sum_{j\neq k \text{ even}} f_{ij}\, t_j\big)/(e^r+\|t_{-k}\|_1) + \sum_{j\text{ odd}} f_{ij}\, u_j/\|u\|_1 \big)}{(e^r+\|t_{-k}\|_1)^{\|b\|_1-d^e-1}\exp((e^r+\|t_{-k}\|_1)^2/2)} =: G(r) ,
\end{align*}\]</span>
for odd <span class="math inline">\(k\)</span> we have
<span class="math display">\[\begin{align*}
   p(\log U_k=r &amp; \mid U_{-k}=u_{-k}, T=t, \mathbf X=\mathbf x, \mathbf Y=\mathbf y)
\\ &amp; \propto \frac{e^{c_kr} \prod_{i=1}^N \big( \sum_{j\text{ even}} f_{ij}\, t_j/\|t\|_1 + \big( e^r f_{ik} + \sum_{j\neq k \text{ odd}} f_{ij}\, u_j\big)/(e^r+\|u_{-k}\|_1) \big)}{(e^r+\|u_{-k}\|_1)^{\|c\|_1-d^o-1}\exp((e^r+\|u_{-k}\|_1)^2/2)} =: H(r) .
\end{align*}\]</span>
As above, we need to specify cut-off points, for <span class="math inline">\(k\)</span> even, <span class="math display">\[ \int_{-\infty}^{r_-} G(r)\,dr \leq \varepsilon_- ,\quad \int_{r_+}^\infty G(r)\,dr \leq \varepsilon_+ . \]</span> Similarly as above, it can be shown that in the non-informative case we have the following valid formulas for <span class="math inline">\(r_-\)</span> and <span class="math inline">\(r_+\)</span>:
<span class="math display">\[\begin{align*}
   r_- &amp; = \frac{\max\Big\{0,\; \log \varepsilon_- + \log v^{(0)}_k + \frac{\|t_{-k}\|_1^2}{2} - d^e\log(1+\|t_{-k}\|_1) - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\} \Big\}}{v^{(0)}_k} ,
\\ r_+ &amp; = \min\Bigg\{ 0, \frac{\frac{1}{2} \log\Big( 2\log\Gamma^{-1}\Big( \frac{d^e+1}{2}, \log\varepsilon_+ - \frac{d^e-1}{2} \log 2 - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\}\Big) \Big) }{\log \|t_{-k}\|_1} \Bigg\} .
\end{align*}\]</span>
In the informative case we have the following valid cut-off points:
<span class="math display">\[\begin{align*}
   r_- &amp; = \frac{\max\Big\{0,\; \log \varepsilon_- + \log(1+v^{(0)}_k) + \frac{\|t_{-k}\|_1^2}{2} + \log\|t_{-k}\|_1 - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\} \Big\}}{1+v^{(0)}_k} ,
\\ r_+ &amp; = \min\Bigg\{ 0, \frac{\frac{1}{2}\log\Big( 2\log\Gamma^{-1}\Big( \frac{1}{2},\; \log\varepsilon_+ + \frac{\log 2}{2} - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\}\Big) \Big)}{\log \|t_{-k}\|_1} \Bigg\} .
\end{align*}\]</span>
For <span class="math inline">\(k\)</span> odd we need to find cut-offs so that, <span class="math display">\[ \int_{-\infty}^{r_-} H(r)\,dr \leq \varepsilon_- ,\quad \int_{r_+}^\infty H(r)\,dr \leq \varepsilon_+ . \]</span> In the non-informative case we have the following valid formulas for <span class="math inline">\(r_-\)</span> and <span class="math inline">\(r_+\)</span>:
<span class="math display">\[\begin{align*}
   r_- &amp; = \frac{\max\Big\{0,\; \log \varepsilon_- + \log v^{(0)}_k + \frac{\|u_{-k}\|_1^2}{2} - d^o\log(1+\|u_{-k}\|_1) - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\} \Big\}}{v^{(0)}_k} ,
\\ r_+ &amp; = \min\Bigg\{ 0, \frac{\frac{1}{2} \log\Big( 2\log\Gamma^{-1}\Big( \frac{d^o+1}{2}, \log\varepsilon_+ - \frac{d^o-1}{2} \log 2 - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\}\Big) \Big) }{\log \|u_{-k}\|_1} \Bigg\} .
\end{align*}\]</span>
In the informative case we have the following valid cut-off points:
<span class="math display">\[\begin{align*}
   r_- &amp; = \frac{\max\Big\{0,\; \log \varepsilon_- + \log(1+v^{(0)}_k) + \frac{\|u_{-k}\|_1^2}{2} + \log\|u_{-k}\|_1 - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\} \Big\}}{1+v^{(0)}_k} ,
\\ r_+ &amp; = \min\Bigg\{ 0, \frac{\frac{1}{2}\log\Big( 2\log\Gamma^{-1}\Big( \frac{1}{2},\; \log\varepsilon_+ + \frac{\log 2}{2} - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\}\Big) \Big)}{\log \|u_{-k}\|_1} \Bigg\} .
\end{align*}\]</span>
</div>
</div>
<div id="implementation" class="section level2">
<h2>Implementation</h2>
As mentioned <a href="#distr_simplex">above</a>, the extension of the notation <span class="math inline">\(f_{ik}\)</span> to include the edge cases was solely for notational convenience; for the implementation this is of course broken down:
<span class="math display">\[\begin{align*}
   G(r) &amp; = \frac{e^{b_kr} \prod_{i=1}^{\bar N} \big( \big(e^r f_{ik} + \sum_{j\neq k \text{ even}} f_{ij}\, t_j\big)/(e^r+\|t_{-k}\|_1) + \sum_{j\text{ odd}} f_{ij}\, u_j/\|u\|_1 \big)}{(e^r+\|t_{-k}\|_1)^{\|b\|_1-d^e-1}\exp((e^r+\|t_{-k}\|_1)^2/2)}
\\[1mm] &amp; \qquad \cdot \begin{cases}
       \displaystyle \frac{e^{rpN}\, t_d^{qN}}{(e^r+\|t_{-0}\|_1)^{N-\bar N}} &amp; \text{if $k=0$, $d$ even}
    \\[1mm] \displaystyle \frac{e^{rpN}\, u_d^{qN}}{(e^r+\|t_{-0}\|_1)^{pN}\, \|u\|_1^{qN}} &amp; \text{if $k=0$, $d$ odd}
    \\[1mm] \displaystyle \frac{t_0^{pN}\, t_d^{qN}}{(e^r+\|t_{-0}\|_1)^{N-\bar N}} &amp; \text{if $0&lt;k&lt;d$, $d$ even}
    \\[1mm] \displaystyle \frac{t_0^{pN}\, u_d^{qN}}{(e^r+\|t_{-0}\|_1)^{pN}\, \|u\|_1^{qN}} &amp; \text{if $0&lt;k&lt;d$, $d$ odd}
    \\[1mm] \displaystyle \frac{t_0^{pN}\, e^{rqN}}{(e^r+\|t_{-0}\|_1)^{N-\bar N}} &amp; \text{if $k=d$ even}
    \\[1mm] \displaystyle \frac{t_0^{pN}\, e^{rqN}}{(e^r+\|t_{-0}\|_1)^{pN}\, \|u\|_1^{qN}} &amp; \text{if $k=d$ odd} .
    \end{cases}
\end{align*}\]</span>
<p>Similar case distinction hold for <span class="math inline">\(H\)</span>, and correspondingly for the cutoff points that have to be computed to realize the sampling of the conditional distributions.</p>
<div id="example" class="section level3">
<h3>Example</h3>
<p>TBD</p>
</div>
</div>
<div id="proof_normal_jacobian" class="section level2">
<h2>Appendix A: proof of the parameter change formula</h2>
Let <span class="math inline">\(B\in\text{R}^{(d+1)\times d}\)</span> such that its columns form an orthogonal basis of the hyperplane defined by the main diagonal <span class="math inline">\(1_{d+1}=(1,\ldots,1)\in\text{R}^{d+1}\)</span>, that is, <span class="math display">\[ B^TB=I_d ,\quad B^T1_{d+1}=0 ,\quad \text{or equivalently,}\quad \big(B , \tfrac{1}{\sqrt{d+1}}1_{d+1}\big)\in O(d+1) . \]</span> We denote the transformed probability simplex by <span class="math inline">\(\tilde\Delta^d=B^T\Delta^d\subset\text{R}^d\)</span>, so that <span class="math display">\[ \Delta^d = \tfrac{1}{d+1}1_{d+1} + B\tilde\Delta^d , \]</span> and consider the parametrization <span class="math display">\[ \psi\colon\tilde\Delta^d\times \text{R}_+\to \text{R}_+^{d+1} ,\quad \psi(\mu,t) = t\sqrt{d+1}(\tfrac{1}{d+1}1_{d+1} + B\mu) . \]</span> The inverse of this parametrization is given by <span class="math display">\[ \psi^{-1}(w) = \Big( \frac{1}{w_0+\dots+w_d}B^Tw ,\; \frac{w_0+\dots+w_d}{\sqrt{d+1}}\Big) , \]</span> and its Jacobian matrix is given by <span class="math display">\[ D\psi^{-1}(w) = \begin{pmatrix} B^T \frac{1}{(w_0+\dots+w_d)^2}\big( (w_0+\dots+w_d) I_{d+1} - w1_{d+1}^T\big) \\ \frac{1}{\sqrt{d+1}}1_{d+1}^T \end{pmatrix} . \]</span> We compute the normal determinant of the Jacobian matrix as follows:
<span class="math display">\[\begin{align*}
   \big|\det D\psi^{-1}(w)\big| &amp; = \left|\det \big( D\psi^{-1}(w) \big(B , \tfrac{1}{\sqrt{d+1}}1_{d+1}\big)\big) \right|
\\ &amp; = \left|\det \begin{pmatrix}
B^T \frac{1}{(w_0+\dots+w_d)^2}\big( (w_0+\dots+w_d) I_{d+1} - w1_{d+1}^T\big) B &amp; *
\\ 0 &amp; 1
\end{pmatrix} \right|
\\ &amp; = \left|\det \big( \tfrac{1}{(w_0+\dots+w_d)^2}\big( (w_0+\dots+w_d) I_d \big) \big) \right|
\\ &amp; = (w_0+\dots+w_d)^{-d} .
\end{align*}\]</span>
</div>
<div id="proof_cutoff" class="section level2">
<h2>Appendix B: proof of the validity of the cut-off points</h2>
For the upper bound of the left tail we assume <span class="math inline">\(r_-\leq0\)</span>, so that in the non-informative case
<span class="math display">\[\begin{align*}
   \int_{-\infty}^{r_-} F(r)\,dr &amp; = \int_{-\infty}^{r_-} \frac{(e^r+\|w_{-k}\|_1)^d\, e^{r v^{(0)}_k}}{\exp((e^r+\|w_{-k}\|_1)^2/2)} \prod_{i=1}^N \frac{e^r f_{ik} + \sum_{j\neq k} w_j\,f_{ij}}{e^r+\|w_{-k}\|_1} \,dr
\\ &amp; \leq \Big( \prod_{i=1}^N \max\{f_{ij}\mid 0\leq j\leq d\} \Big) \int_{-\infty}^{r_-} \frac{(1+\|w_{-k}\|_1)^d\, e^{r v^{(0)}_k}}{\exp(\|w_{-k}\|_1^2/2)} \,dr
\\ &amp; = \Big( \prod_{i=1}^N \max\{f_{ij}\mid 0\leq j\leq d\} \Big) \frac{(1+\|w_{-k}\|_1)^d}{\exp(\|w_{-k}\|_1^2/2)} \frac{e^{v^{(0)}_k r_-}}{v^{(0)}_k} = \varepsilon_- .
\end{align*}\]</span>
Solving for <span class="math inline">\(r_-\)</span> yields
<span class="math display">\[\begin{align*}
   r_- = \frac{\max\Big\{0,\; \log \varepsilon_- + \log v^{(0)}_k + \frac{\|w_{-k}\|_1^2}{2} - d\log(1+\|w_{-k}\|_1) - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\} \Big\}}{v^{(0)}_k} .
\end{align*}\]</span>
Similarly, we obtain in the informative case <!-- \begin{align*} --> <!--    \int_{-\infty}^{r_-} F(r)\,dr & = \int_{-\infty}^{r_-} \frac{e^{r(1+v^{(0)}_k)}}{(e^r+\|w_{-k}\|_1)\,\exp((e^r+\|w_{-k}\|_1)^2/2)} \prod_{i=1}^N \frac{e^r f_{ik} + \sum_{j\neq k} w_j\,f_{ij}}{e^r+\|w_{-k}\|_1} \,dr --> <!-- \\ & \leq \Big( \prod_{i=1}^N \max\{f_{ij}\mid 0\leq j\leq d\} \Big) \int_{-\infty}^{r_-} \frac{e^{r(1+v^{(0)}_k)}}{\|w_{-k}\|_1\, \exp(\|w_{-k}\|_1^2/2)} \,dr --> <!-- \\ & = \Big( \prod_{i=1}^N \max\{f_{ij}\mid 0\leq j\leq d\} \Big) \frac{1}{\|w_{-k}\|_1\, \exp(\|w_{-k}\|_1^2/2)} \frac{e^{r_-}}{1+v^{(0)}_k} = \varepsilon_- . --> <!-- \end{align*} --> <!-- Solving for $r_-$ yields -->
<span class="math display">\[\begin{align*}
   r_- = \frac{\max\Big\{0,\; \log \varepsilon_- + \log(1+v^{(0)}_k) + \frac{\|w_{-k}\|_1^2}{2} + \log\|w_{-k}\|_1 - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\} \Big\}}{1+v^{(0)}_k} .
\end{align*}\]</span>
For the upper bound of the right tail we denote <span class="math inline">\(s=e^r,s_+=e^{r_+}\)</span>, so that
<span class="math display">\[\begin{align*}
   \int_{r_+}^\infty F(r)\,dr &amp; = \int_{s_+}^\infty \frac{F(\log s)}{s}\,ds
% \\ &amp;
= \int_{s_+}^\infty \frac{(s+\|w_{-k}\|_1)^{d+1-\|a\|_1}\, s^{a_k-1}}{\exp((s+\|w_{-k}\|_1)^2/2)} \prod_{i=1}^N \frac{s f_{ik} + \sum_{j\neq k} w_j\,f_{ij}}{s+\|w_{-k}\|_1} \,ds .
\end{align*}\]</span>
In the non-informative case we obtain, assuming <span class="math inline">\(r_+\geq0\)</span>,
<span class="math display">\[\begin{align*}
   \int_{r_+}^\infty F(r)\,dr &amp; = \int_{s_+}^\infty \frac{(s+\|w_{-k}\|_1)^d}{s^{1-v^{(0)}_k}\, \exp((s+\|w_{-k}\|_1)^2/2)} \prod_{i=1}^N \frac{s f_{ik} + \sum_{j\neq k} w_j\,f_{ij}}{s+\|w_{-k}\|_1} \,ds
\\ &amp; \leq \Big( \prod_{i=1}^N \max\{f_{ij}\mid 0\leq j\leq d\} \Big) \int_{s_+ +\|w_{-k}\|_1}^\infty s^d\, e^{-s^2/2} \,ds
\\ &amp; = \Big( \prod_{i=1}^N \max\{f_{ij}\mid 0\leq j\leq d\} \Big) 2^{(d-1)/2} \Gamma\Big( \frac{d+1}{2}, \frac{(s_+ +\|w_{-k}\|_1)^2}{2}\Big) = \varepsilon_+ ,
\end{align*}\]</span>
where <span class="math inline">\(\Gamma(\cdot,\cdot)\)</span> denotes the <a href="https://en.wikipedia.org/wiki/Incomplete_gamma_function">incomplete gamma function</a>. Solving for <span class="math inline">\(r_+=\log s_+\)</span> yields
<span class="math display">\[\begin{align*}
    r_+ = \min\Bigg\{ 0, \frac{\frac{1}{2} \log\Big( 2\log\Gamma^{-1}\Big( \frac{d+1}{2}, \log\varepsilon_+ - \frac{d-1}{2} \log 2 - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\}\Big) \Big) }{\log \|w_{-k}\|_1} \Bigg\} .
\end{align*}\]</span>
Similarly, we obtain in the informative case <!-- \begin{align*} --> <!--    \int_{r_+}^\infty F(r)\,dr & = \int_{s_+}^\infty \frac{s^{v^{(0)}_k}}{(s+\|w_{-k}\|_1)\, \exp((s+\|w_{-k}\|_1)^2/2)} \prod_{i=1}^N \frac{s f_{ik} + \sum_{j\neq k} w_j\,f_{ij}}{s+\|w_{-k}\|_1} \,ds --> <!-- \\ & \leq \Big( \prod_{i=1}^N \max\{f_{ij}\mid 0\leq j\leq d\} \Big) \int_{s_+ +\|w_{-k}\|_1}^\infty e^{-s^2/2} \,ds --> <!-- \\ & = \Big( \prod_{i=1}^N \max\{f_{ij}\mid 0\leq j\leq d\} \Big) \frac{1}{\sqrt{2}} \Gamma\Big( \frac{1}{2}, \frac{(s_+ +\|w_{-k}\|_1)^2}{2}\Big) = \varepsilon_+ , --> <!-- \end{align*} --> <!-- where $\Gamma(\cdot,\cdot)$ denotes the [incomplete gamma function](https://en.wikipedia.org/wiki/Incomplete_gamma_function). Solving for $r_+=\log s_+$ yields -->
<span class="math display">\[\begin{align*}
    r_+ = \min\Bigg\{ 0, \frac{\frac{1}{2}\log\Big( 2\log\Gamma^{-1}\Big( \frac{1}{2},\; \log\varepsilon_+ + \frac{\log 2}{2} - \sum_{i=1}^N \log\max\{f_{ij}\mid 0\leq j\leq d\}\Big) \Big)}{\log \|w_{-k}\|_1} \Bigg\} .
\end{align*}\]</span>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-GCSDVR14">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014. <em>Bayesian Data Analysis</em>. Third. Texts in Statistical Science Series. CRC Press, Boca Raton, FL.</p>
</div>
<div id="ref-HTF09">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning</em>. Second. Springer Series in Statistics. Springer, New York. doi:<a href="https://doi.org/10.1007/978-0-387-84858-7">10.1007/978-0-387-84858-7</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>We use the letters <span class="math inline">\(T\)</span> and <span class="math inline">\(U\)</span> out of lack of better choices; using superscripts makes subsequent formulas unnecessarily complicated.<a href="#fnref1">↩</a></p></li>
</ol>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
