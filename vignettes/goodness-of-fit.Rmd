---
title: "Goodness of fit for bivariate chi-bar-squared distribution"
author: "Dennis Amelunxen"
date: "`r Sys.Date()`"
# output: rmarkdown::html_vignette
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

**After a second thought, this goodness of fit test seems to be impractical and not very powerful. Instead we should pursue Bayesian estimates.**

In this vignette we analyze more rigorously the distribution used in the EM algorithm, described [here](estim-conic-intrinsic-volumes-with-EM.html), and analyze the goodness of fit of the resulting estimate of the conic intrinsic volumes with respect to the sample data. See [this vignette](conic-intrinsic-volumes.html) for a short introduction to conic intrinsic volumes.

## The modified bivariate chi-bar-squared distribution {#mod_bivchibarsq}

[Recall](estim-conic-intrinsic-volumes-with-EM.html#setup) that we assumed to have sampled data: $(X_1,Y_1),\ldots,(X_N,Y_N)$ denote iid copies of $(X,Y)$ and we assume that they took the sample values $(x_1,y_1),\ldots,(x_N,y_N)$. Furthermore, we defined the two proportions
$$ p = \frac{\left|\{i\mid y_i=0\}\right|}{N} ,\quad q = \frac{\left|\{i\mid x_i=0\}\right|}{N} , $$
and dropped those sample values $(x_i,y_i)$ with $x_i=0$ or $y_i=0$. Without loss of generality, we used the notation $(x_1,y_1),\ldots,(x_{\bar N},y_{\bar N})$ for the remaining sample points, $\bar N\leq N$.

We can formalize these steps by considering the following mixed continuous-discrete distribution: for given weight vector $v=(v_0,\ldots,v_d)$, $v\geq0$, $\sum_{k=0}^d v_k=1$, consider the distribution
$$ f_v(x,y) = v_d\, \delta(-1,0) + v_0\, \delta(0,-1) + \sum_{k=1}^{d-1} v_k f_k(x) f_{d-k}(y) , $$
where $\delta$ denotes the (bivariate) Dirac delta, and $f_k(x)$ denotes the density of the chi-squared distribution.

The cumulative distribution function (cdf) is thus given by
$$ F_v(x,y) = \begin{cases}
      v_d & \text{if } -1\leq x<0\leq y ,
   \\ v_0 & \text{if } -1\leq y<0\leq x ,
   \\ v_0 + v_d + \sum_{k=1}^{d-1} v_k F_k(x)F_{d-k}(y) & \text{if } (x,y)\geq0 ,
   \\ 0 & \text{else} ,
   \end{cases} $$
where $F_k(x)$ denotes the cdf of the chi-squared distribution.

### The empirical distribution function

In the above described formal setting of the modified bivariate chi-bar-squared distribution we can describe the sample data as having $pN$ times drawn the point $(0,-1)$, $qN$ times drawn the point $(-1, 0)$, and $\bar N$ times drawn samples in the first quadrant, $(x_1,y_1),\ldots,(x_{\bar N},y_{\bar N})$.

The empirical distribution function is thus given by
$$ \hat F_N(x,y) = \begin{cases}
      p & \text{if } -1\leq x<0\leq y ,
   \\ q & \text{if } -1\leq y<0\leq x ,
   \\ p + q + \frac{1}{N}\sum_{i=1}^{\bar N} 1_{(x_i,y_i)\leq(x,y)} & \text{if } (x,y)\geq0 ,
   \\ 0 & \text{else} .
   \end{cases} $$

## Assessing the goodness of fit

To assess the goodness of fit we decompose the mixed coninuous-discrete distribution into a discrete distribution corresponding to the edge values $v_0, v_d$, and an absolutely continuous distribution corresponding to the bulk values $v_k$, $k=1,\ldots,d-1$. More precisely, the chi-squared test measures the goodness of fit of the observed probabilities $(q,p,1-q-p)$ with the theoretical probabilities $(v_0,v_d,1-v_0-v_d)$, whereas for the absolutely continuous part of the distribution we compare
$$ G_v(x,y) \sum_{k=1}^{d-1} \frac{v_k}{1-v_0-v_d} f_k(x) f_{d-k}(y) \quad\text{and}\quad \hat G_{\bar N}(x,y) = \frac{1}{\bar N}\sum_{i=1}^{\bar N} 1_{(x_i,y_i)\leq(x,y)} . $$

### Pearson's chi squared test for the edge values

We compute the chi-squared statistic
$$ \chi^2 = N\Big( \frac{(q-v_0)^2}{v_0} + \frac{(p-v_d)^2}{v_d} + \frac{(q-v_0+p-v_d)^2}{1-v_0-v_d} \Big) . $$
The p-values returned with this statistic correspond to two degrees of freedom.

### Bivariate Kolmogorov-Smirnov statistic

For univariate distributions the Kolmogorov-Smirnov statistic of a distribution with respect to a given sample is defined as the maximum absolute difference of the hypothesized cdf $F(T)$ and the empirical cdf $F_N(x)$,
$$ \sup_{x\in \text{R}} \left|\hat F_N(x)-F(x)\right| . $$
The straightforward bivariate generalization,
$$ \sup_{(x,y)\in \text{R}^2} \left|\hat F_N(x,y)-F(x,y)\right| , $$
is not distribution-free. This is why [@JPZ97] define the bivariate^[The authors define this statistic in the general multivariate case; we restrict the discussion to the bivariate case to simplify notation.] Kolmogorov-Smirnov statistic in the following way: denoting the distribution by $f(x,y)$ and using the transformations $(x,y)\mapsto(a^{(1)},b^{(1)})$ and $(x,y)\mapsto(a^{(2)},b^{(2)})$,
\begin{align*}
   a^{(1)} & = \int_{-\infty}^x\int_{-\infty}^\infty f(s,t)\,dt\,ds , & b^{(1)} & = \frac{\int_{-\infty}^y f(x,t)\,dt}{\int_{-\infty}^\infty f(x,t)\,dt}
\\[1mm] a^{(2)} & = \int_{-\infty}^y\int_{-\infty}^\infty f(s,t)\,ds\,dt , & b^{(2)} & = \frac{\int_{-\infty}^x f(s,y)\,ds}{\int_{-\infty}^\infty f(s,y)\,ds}
\end{align*}
the distribution of both transformed pairs, according to a theorem by [@R52], is the uniform distribution on $[0,1]^2$. Denoting the transformed sample points by $(a^{(j)}_1,b^{(j)}_1),\ldots,(a^{(j)}_N,b^{(j)}_N)$, $j=1,2$, the bivariate Kolmogorov-Smirnov statistic is defined by
$$ D^{\text{KS}} = \max_{j\in\{1,2\}, (a,b)\in[0,1]^2}\left| \frac{1}{N}\sum_{i=1}^N 1_{(a_i^{(j)},b_i^{(j)})\leq(a,b)} - ab \right| . $$

Apart from the Kolmogorov-Smirnov statistic, many other statistics for assessing the goodness of fit of univariate distributions or multivariate normal distributions have been proposed. The specific case of multivariate non-normal distributions is less well-established, but a multivariate version of the Cram&eacute;r-von Mises statistic can be found in [@CL09].

### Goodness of fit for the bulk values

We compute the transformed samples for the absolutely continuous distribution $G_v$ as follows:
\begin{align*}
   a^{(1)} & = \sum_{k=1}^{d-1} \frac{v_k}{1-v_0-v_d} F_k(x) ,
 & b^{(1)} & = \frac{\sum_{k=1}^{d-1} v_k f_k(x)F_{d-k}(y)}{\sum_{k=1}^{d-1} v_kf_k(x)} ,
\\ a^{(2)} & = \sum_{k=1}^{d-1} \frac{v_k}{1-v_0-v_d} F_{d-k}(y) ,
 & b^{(2)} & = \frac{\sum_{k=1}^{d-1} v_k F_k(x)f_{d-k}(y)}{\sum_{k=1}^{d-1} v_k f_{d-k}(y)} .
\end{align*}
So we assume to have the transformed sample points $\big(a^{(1)}_1, b^{(1)}_1\big),\ldots,\big(a^{(1)}_{\bar N}, b^{(1)}_{\bar N}\big)$ and $\big(a^{(2)}_1, b^{(2)}_1\big),\ldots,\big(a^{(2)}_{\bar N}, b^{(2)}_{\bar N}\big)$. We define the functions
$$ D^{(j)}(a,b) = \frac{1}{\bar N}\sum_{i=1}^{\bar N} 1_{(a,b)\leq (a^{(j)}_i,b^{(j)}_i)} - ab ,\quad j=1,2 . $$
[@JPZ97] give the following algorithm for computing the bivariate Kolmogorov-Smirnov statistic $D^{\text{KS}}$:

* For $j=1,2$ compute $D_i^{(j)}$, $i=1,\ldots,5$:  
(we drop the superscript $(j)$ in the following to ease readability)
\begin{align*}
   D_1 & = \max\{D(a_i,b_i)\mid 1\leq i\leq \bar N\} ,
\\ D_2 & = \max\{D(a_\ell,b_i)\mid 1\leq i,\ell\leq \bar N, a_\ell>a_i, b_\ell<b_i \} ,
\\ D_3 & = \tfrac{2}{\bar N} - \min\{D(a_\ell,b_i)\mid 1\leq i,\ell\leq \bar N, a_\ell>a_i, b_\ell<b_i \} ,
\\ D_4 & = \tfrac{1}{\bar N} - \min\{D(1,b_i)\mid 1\leq i\leq \bar N \} ,
\\ D_5 & = \tfrac{1}{\bar N} - \min\{D(a_i,1)\mid 1\leq i\leq \bar N \} .
\end{align*}
* The Kolmogorov-Smirnov statistic is given by
$$ D^{\text{KS}} = \max\big\{ D^{(j)}_i\mid i\in\{1,\ldots,5\}, j\in\{1,2\}\big\} . $$

## References















